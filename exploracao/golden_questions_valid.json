[
    {
        "context": "Hierarchical Semantic Correspondence Networks\nfor Video Paragraph Grounding\nChaolei Tan1\nZihang Lin1\nJian-Fang Hu1,2,3*\nWei-Shi Zheng1,2,3\nJianhuang Lai1,2,3\n1School of Computer Science and Engineering, Sun Yat-sen University, China\n2Guangdong Province Key Laboratory of Information Security Technology, China\n3Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\n{tanchlei,linzh59}@mail2.sysu.edu.cn, hujf5@mail.sysu.edu.cn,\nwszheng@ieee.org, stsljh@mail.sysu.edu.cn\nAbstract\nVideo Paragraph Grounding (VPG) is an essential yet\nchallenging task in vision-language understanding, which\naims to jointly localize multiple events from an untrimmed\nvideo with a paragraph query description. One of the crit-\nical challenges in addressing this problem is to compre-\nhend the complex semantic relations between visual and\ntextual modalities.\nPrevious methods focus on modeling\nthe contextual information between the video and text from\na single-level perspective (i.e., the sentence level), ignor-\ning rich visual-textual correspondence relations at different\nsemantic levels, e.g., the video-word and video-paragraph\ncorrespondence. To this end, we propose a novel Hierar-\nchical Semantic Correspondence Network (HSCNet), which\nexplores multi-level visual-textual correspondence by learn-\ning hierarchical semantic alignment and utilizes dense su-\npervision by grounding diverse levels of queries. Specifi-\ncally, we develop a hierarchical encoder that encodes the\nmulti-modal inputs into semantics-aligned representations\nat different levels. To exploit the hierarchical semantic cor-\nrespondence learned in the encoder for multi-level supervi-\nsion, we further design a hierarchical decoder that progres-\nsively performs finer grounding for lower-level queries con-\nditioned on higher-level semantics. Extensive experiments\ndemonstrate the effectiveness of HSCNet and our method\nsignificantly outstrips the state-of-the-arts on two challeng-\ning benchmarks, i.e., ActivityNet-Captions and TACoS.\n",
        "question": {
            "statement": "What is the main challenge in Video Paragraph Grounding (VPG) tasks?",
            "options": [
                "localizing events from trimmed videos",
                "developing hierarchical encoder-decoder architectures",
                "understanding single-level contextual information",
                "comprehending complex semantic relations between visual and textual modalities"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Preserving Linear Separability in Continual Learning\nby Backward Feature Projection\nQiao Gu\nUniversity of Toronto\nqgu@cs.toronto.edu\nDongsub Shim\nLG AI Research\ndongsub.shim@lgresearch.ai\nFlorian Shkurti\nUniversity of Toronto\nflorian@cs.toronto.edu\nAbstract\nCatastrophic forgetting has been a major challenge in\ncontinual learning, where the model needs to learn new\ntasks with limited or no access to data from previously seen\ntasks. To tackle this challenge, methods based on knowl-\nedge distillation in feature space have been proposed and\nshown to reduce forgetting [16,17,25]. However, most fea-\nture distillation methods directly constrain the new features\nto match the old ones, overlooking the need for plasticity.\nTo achieve a better stability-plasticity trade-off, we propose\nBackward Feature Projection (BFP), a method for contin-\nual learning that allows the new features to change up to\na learnable linear transformation of the old features. BFP\npreserves the linear separability of the old classes while al-\nlowing the emergence of new feature directions to accom-\nmodate new classes. BFP can be integrated with existing\nexperience replay methods and boost performance by a sig-\nnificant margin. We also demonstrate that BFP helps learn\na better representation space, in which linear separability is\nwell preserved during continual learning and linear prob-\ning achieves high classification accuracy.\n",
        "question": {
            "statement": "What is the main limitation of traditional knowledge distillation methods in feature space for continual learning?",
            "options": [
                "They cannot integrate with experience replay methods",
                "They directly constrain the new features to match the old ones, limiting plasticity.",
                "They are unable to accommodate new classes",
                "They require access to all previous task data"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "5",
                "0"
            ]
        },
        "difference": 5,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Ground-Truth Free Meta-Learning for Deep Compressive Sampling\nXinran Qin1,2\nYuhui Quan1,2* Tongyao Pang3\nHui Ji3\n1School of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China\n2 Pazhou Lab, Guangzhou 510335, China\n3 Department of Mathematics, National University of Singapore, 119076, Singapore\ncsqinxinran@gmail.com, csyhquan@scut.edu.cn, matpt@nus.edu.sg, matjh@nus.edu.sg †\nAbstract\nCompressive sampling (CS) is an efﬁcient technique for\nimaging.\nThis paper proposes a ground-truth (GT) free\nmeta-learning method for CS, which leverages both ex-\nternal and internal deep learning for unsupervised high-\nquality image reconstruction. The proposed method ﬁrst\ntrains a deep neural network (NN) via external meta-\nlearning using only CS measurements, and then efﬁciently\nadapts the trained model to a test sample for exploit-\ning sample-speciﬁc internal characteristic for performance\ngain. The meta-learning and model adaptation are built on\nan improved Stein’s unbiased risk estimator (iSURE) that\nprovides efﬁcient computation and effective guidance for\naccurate prediction in the range space of the adjoint of the\nmeasurement matrix. To improve the learning and adap-\ntion on the null space of the measurement matrix, a modi-\nﬁed model-agnostic meta-learning scheme and a null-space\nconsistency loss are proposed. In addition, a bias tuning\nscheme for unrolling NNs is introduced for further acceler-\nation of model adaption. Experimental results have demon-\nstrated that the proposed GT-free method performs well and\ncan even compete with supervised methods.\n",
        "question": {
            "statement": "What is the main advantage of using compressive sampling in imaging?",
            "options": [
                "It reduces the amount of data required while preserving image quality",
                "It increases the resolution of the image",
                "It requires human annotation for each image",
                "It is only suitable for black and white images"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "9",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Boosting Accuracy and Robustness of Student Models via Adaptive Adversarial\nDistillation\nBo Huang1,2, Mingyang Chen1,2, Yi Wang3, Junda Lu4, Minhao Cheng2, Wei Wang1,2,*\n1The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China\n2The Hong Kong University of Science and Technology, Hong Kong SAR, China\n3Dongguan University of Technology, Dongguan, China\n4Macquarie University, Sydney, Australia\n{bhuangas, mchenbt}@connect.ust.hk; wangyi@dgut.edu.cn;\njunda.lu@mq.edu.au;{minhaocheng, weiwcs}@ust.hk\nAbstract\nDistilled student models in teacher-student architectures\nare widely considered for computational-effective deploy-\nment in real-time applications and edge devices. However,\nthere is a higher risk of student models to encounter ad-\nversarial attacks at the edge. Popular enhancing schemes\nsuch as adversarial training have limited performance on\ncompressed networks. Thus, recent studies concern about\nadversarial distillation (AD) that aims to inherit not only\nprediction accuracy but also adversarial robustness of a ro-\nbust teacher model under the paradigm of robust optimiza-\ntion. In the min-max framework of AD, existing AD methods\ngenerally use fixed supervision information from the teacher\nmodel to guide the inner optimization for knowledge distil-\nlation which often leads to an overcorrection towards model\nsmoothness. In this paper, we propose an adaptive adver-\nsarial distillation (AdaAD) that involves the teacher model\nin the knowledge optimization process in a way interacting\nwith the student model to adaptively search for the inner\nresults. Comparing with state-of-the-art methods, the pro-\nposed AdaAD can significantly boost both the prediction\naccuracy and adversarial robustness of student models in\nmost scenarios. In particular, the ResNet-18 model trained\nby AdaAD achieves top-rank performance (54.23% robust\naccuracy) on RobustBench under AutoAttack.\n",
        "question": {
            "statement": "What is the primary goal of adversarial distillation in deep learning?",
            "options": [
                "to increase the complexity of teacher models",
                "to enhance the interpretability of neural networks",
                "to improve the robustness of student models against adversarial attacks",
                "to reduce the size of student models"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Co-Salient Object Detection with Uncertainty-aware Group Exchange-Masking\nYang Wu1\nHuihui Song1∗\nBo Liu2\nKaihua Zhang1\nDong Liu3\n1B-DAT and CICAEET, Nanjing University of Information Science and Technology, Nanjing, China\n2Walmart Global Tech, Sunnyvale, CA, 94086, USA\n3Netflix Inc, Los Gatos, CA, 95032, USA\nsonghuihui@nuist.edu.cn\nAbstract\nThe traditional definition of co-salient object detection\n(CoSOD) task is to segment the common salient objects in\na group of relevant images. Existing CoSOD models by-\ndefault adopt the group consensus assumption. This brings\nabout model robustness defect under the condition of ir-\nrelevant images in the testing image group, which hinders\nthe use of CoSOD models in real-world applications. To\naddress this issue, this paper presents a group exchange-\nmasking (GEM) strategy for robust CoSOD model learn-\ning. With two group of image containing different types of\nsalient object as input, the GEM first selects a set of images\nfrom each group by the proposed learning based strategy,\nthen these images are exchanged. The proposed feature ex-\ntraction module considers both the uncertainty caused by\nthe irrelevant images and group consensus in the remain-\ning relevant images. We design a latent variable genera-\ntor branch which is made of conditional variational autoen-\ncoder to generate uncertainly-based global stochastic fea-\ntures. A CoSOD transformer branch is devised to capture\nthe correlation-based local features that contain the group\nconsistency information. At last, the output of two branches\nare concatenated and fed into a transformer-based decoder,\nproducing robust co-saliency prediction. Extensive evalua-\ntions on co-saliency detection with and without irrelevant\nimages demonstrate the superiority of our method over a\nvariety of state-of-the-art methods.\n",
        "question": {
            "statement": "What is a limitation of traditional co-salient object detection models?",
            "options": [
                "They assume all images in a group are relevant",
                "They require human annotation for training",
                "They can only be applied to single images",
                "They are unable to detect salient objects at all"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "MSeg3D: Multi-modal 3D Semantic Segmentation for Autonomous Driving\nJiale Li1\nHang Dai2∗\nHao Han3\nYong Ding3∗\n1College of Information Science and Electronic Engineering, Zhejiang University\n2School of Computing Science, University of Glasgow\n3School of Micro-Nano Electronics, Zhejiang University\n∗Corresponding authors{Hang.Dai@glasgow.ac.uk, dingyong09@zju.edu.cn}.\nAbstract\nLiDAR and camera are two modalities available for\n3D semantic segmentation in autonomous driving.\nThe\npopular LiDAR-only methods severely suffer from inferior\nsegmentation on small and distant objects due to insufficient\nlaser points, while the robust multi-modal solution is\nunder-explored, where we investigate three crucial inherent\ndifficulties:\nmodality heterogeneity, limited sensor field\nof view intersection, and multi-modal data augmentation.\nWe propose a multi-modal 3D semantic segmentation\nmodel (MSeg3D) with joint intra-modal feature extraction\nand inter-modal feature fusion to mitigate the modality\nheterogeneity. The multi-modal fusion in MSeg3D consists\nof geometry-based feature fusion GF-Phase, cross-modal\nfeature completion, and semantic-based feature fusion\nSF-Phase on all visible points.\nThe multi-modal data\naugmentation is reinvigorated by applying asymmetric\ntransformations on LiDAR point cloud and multi-camera\nimages individually, which benefits the model training\nwith diversified augmentation transformations.\nMSeg3D\nachieves state-of-the-art results on nuScenes, Waymo, and\nSemanticKITTI datasets. Under the malfunctioning multi-\ncamera input and the multi-frame point clouds input,\nMSeg3D still shows robustness and improves the LiDAR-\nonly baseline. Our code is publicly available at https:\n//github.com/jialeli1/lidarseg3d.\n",
        "question": {
            "statement": "What is a limitation of LiDAR-only methods for 3D semantic segmentation in autonomous driving?",
            "options": [
                "Insufficient laser points on close objects",
                "Difficulty in handling multiple frames",
                "High computational requirements",
                "Inferior segmentation on small and distant objects"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "NeFII: Inverse Rendering for Reflectance Decomposition with\nNear-Field Indirect Illumination\nHaoqian Wu1, Zhipeng Hu1,2, Lincheng Li1*\n, Yongqiang Zhang1, Changjie Fan1, Xin Yu3\n1 NetEase Fuxi AI Lab 2 Zhejiang University 3 The University of Queensland\n{wuhaoqian, zphu, lilincheng, zhangyongqiang02, fanchangjie}@corp.netease.com\nxin.yu@uq.edu.au\nAbstract\nInverse rendering methods aim to estimate geometry,\nmaterials and illumination from multi-view RGB images. In\norder to achieve better decomposition, recent approaches\nattempt to model indirect illuminations reflected from dif-\nferent materials via Spherical Gaussians (SG), which, how-\never, tends to blur the high-frequency reflection details. In\nthis paper, we propose an end-to-end inverse rendering\npipeline that decomposes materials and illumination from\nmulti-view images, while considering near-field indirect il-\nlumination. In a nutshell, we introduce the Monte Carlo\nsampling based path tracing and cache the indirect illumi-\nnation as neural radiance, enabling a physics-faithful and\neasy-to-optimize inverse rendering method. To enhance ef-\nficiency and practicality, we leverage SG to represent the\nsmooth environment illuminations and apply importance\nsampling techniques. To supervise indirect illuminations\nfrom unobserved directions, we develop a novel radiance\nconsistency constraint between implicit neural radiance\nand path tracing results of unobserved rays along with the\njoint optimization of materials and illuminations, thus sig-\nnificantly improving the decomposition performance. Ex-\ntensive experiments demonstrate that our method outper-\nforms the state-of-the-art on multiple synthetic and real\ndatasets, especially in terms of inter-reflection decomposi-\ntion.\n",
        "question": {
            "statement": "What is a common limitation of using Spherical Gaussians (SG) to model indirect illuminations in inverse rendering?",
            "options": [
                "Insufficient memory usage",
                "Inability to handle complex geometries",
                "Blurring of high-frequency reflection details",
                "Increased computational time"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Parametric Implicit Face Representation for Audio-Driven Facial Reenactment\nRicong Huang 1\nPeiwen Lai1\nYipeng Qin2\nGuanbin Li1*\n1School of Computer Science and Engineering, Sun Yat-sen University\n2Cardiff University\n{huangrc3, laipw5}@mail2.sysu.edu.cn, qiny16@cardiff.ac.uk, liguanbin@mail.sysu.edu.cn\nAbstract\nAudio-driven facial reenactment is a crucial technique\nthat has a range of applications in film-making, virtual\navatars and video conferences. Existing works either em-\nploy explicit intermediate face representations (e.g., 2D fa-\ncial landmarks or 3D face models) or implicit ones (e.g.,\nNeural Radiance Fields), thus suffering from the trade-offs\nbetween interpretability and expressive power, hence be-\ntween controllability and quality of the results. In this work,\nwe break these trade-offs with our novel parametric implicit\nface representation and propose a novel audio-driven fa-\ncial reenactment framework that is both controllable and\ncan generate high-quality talking heads. Specifically, our\nparametric implicit representation parameterizes the im-\nplicit representation with interpretable parameters of 3D\nface models, thereby taking the best of both explicit and im-\nplicit methods. In addition, we propose several new tech-\nniques to improve the three components of our framework,\nincluding i) incorporating contextual information into the\naudio-to-expression parameters encoding; ii) using condi-\ntional image synthesis to parameterize the implicit repre-\nsentation and implementing it with an innovative tri-plane\nstructure for efficient learning; iii) formulating facial reen-\nactment as a conditional image inpainting problem and\nproposing a novel data augmentation technique to improve\nmodel generalizability. Extensive experiments demonstrate\nthat our method can generate more realistic results than\nprevious methods with greater fidelity to the identities and\ntalking styles of speakers.\n",
        "question": {
            "statement": "What is a major limitation of existing audio-driven facial reenactment techniques?",
            "options": [
                "They require a trade-off between interpretability and expressive power",
                "They are unable to preserve speaker identity",
                "They can only be used for video conferencing",
                "They are limited to 2D facial landmarks"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "PD-Quant: Post-Training Quantization Based on\nPrediction Difference Metric\nJiawei Liu1,⋆,⋄\nLin Niu1,⋆,⋄\nZhihang Yuan2,†\nDawei Yang2\nXinggang Wang1\nWenyu Liu1,†\n1 School of EIC, Huazhong University of Science & Technology\n2 Houmo AI\n{jiaweiliu, linniu}@hust.edu.cn\nhahnyuan@gmail.com\ndawei.yang@houmo.ai\n{xgwang, liuwy}@hust.edu.cn\nAbstract\nPost-training quantization (PTQ) is a neural network\ncompression technique that converts a full-precision model\ninto a quantized model using lower-precision data types.\nAlthough it can help reduce the size and computational cost\nof deep neural networks, it can also introduce quantiza-\ntion noise and reduce prediction accuracy, especially in ex-\ntremely low-bit settings. How to determine the appropriate\nquantization parameters (e.g., scaling factors and round-\ning of weights) is the main problem facing now. Existing\nmethods attempt to determine these parameters by minimize\nthe distance between features before and after quantization,\nbut such an approach only considers local information and\nmay not result in the most optimal quantization parameters.\nWe analyze this issue and propose PD-Quant, a method\nthat addresses this limitation by considering global infor-\nmation. It determines the quantization parameters by us-\ning the information of differences between network predic-\ntion before and after quantization. In addition, PD-Quant\ncan alleviate the overfitting problem in PTQ caused by the\nsmall number of calibration sets by adjusting the distribu-\ntion of activations. Experiments show that PD-Quant leads\nto better quantization parameters and improves the predic-\ntion accuracy of quantized models, especially in low-bit\nsettings. For example, PD-Quant pushes the accuracy of\nResNet-18 up to 53.14% and RegNetX-600MF up to 40.67%\nin weight 2-bit activation 2-bit. The code is released at\nhttps://github.com/hustvl/PD-Quant.\n",
        "question": {
            "statement": "What is the primary goal of post-training quantization techniques for deep neural networks?",
            "options": [
                "Enhance the robustness of deep neural networks to adversarial attacks",
                "Reduce the size and computational cost of deep neural networks",
                "Improve the interpretability of deep neural networks",
                "Increase the complexity of deep neural networks"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Phone2Proc: Bringing Robust Robots Into Our Chaotic World\nMatt Deitke∗†ψ, Rose Hendrix∗†, Ali Farhadiψ,\nKiana Ehsani†, Aniruddha Kembhavi†ψ\n†PRIOR @ Allen Institute for AI, ψUniversity of Washington, Seattle\nhttps://phone2proc.allenai.org/\nFigure 1. Successfully deploying agents trained in simulation to the real world has generally proved fraught - we present PHONE2PROC, a\nsimple approach that uses a cellphone to scan an environment and procedurally generate targeted training scene variations of that location,\nwhose usage results in successful and robust agents in the real environment.\nAbstract\nTraining embodied agents in simulation has become\nmainstream for the embodied AI community.\nHowever,\nthese agents often struggle when deployed in the physical\nworld due to their inability to generalize to real-world envi-\nronments. In this paper, we present Phone2Proc, a method\nthat uses a 10-minute phone scan and conditional proce-\ndural generation to create a distribution of training scenes\nthat are semantically similar to the target environment. The\ngenerated scenes are conditioned on the wall layout and\narrangement of large objects from the scan, while also\nsampling lighting, clutter, surface textures, and instances\nof smaller objects with randomized placement and materi-\nals. Leveraging just a simple RGB camera, training with\nPhone2Proc shows massive improvements from 34.7% to\n70.7% success rate in sim-to-real ObjectNav performance\n∗Equal contribution.\nacross a test suite of over 200 trials in diverse real-world\nenvironments, including homes, offices, and RoboTHOR.\nFurthermore, Phone2Proc’s diverse distribution of gener-\nated scenes makes agents remarkably robust to changes in\nthe real world, such as human movement, object rearrange-\nment, lighting changes, or clutter.\n",
        "question": {
            "statement": "What is a common challenge faced by embodied agents trained in simulation when they are deployed in the real world?",
            "options": [
                "Their inability to generalize to real-world environments",
                "Their need for continuous internet connection",
                "Their lack of physical capabilities",
                "Their high computational requirements"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "You Do Not Need Additional Priors or Regularizers in Retinex-based Low-light\nImage Enhancement\nHuiyuan Fu1 , Wenkai Zheng1, Xiangyu Meng1, Xin Wang2, Chuanming Wang1, Huadong Ma1\n1Beijing University of Posts and Telecommunications, 2Stony Brook University\n{fhy, ciki, clearlove7, wcm, mhd}@bupt.edu.cn, x.wang@stonybrook.edu\nAbstract\nImages captured in low-light conditions often suffer from\nsignificant quality degradation. Recent works have built a\nlarge variety of deep Retinex-based networks to enhance\nlow-light images. The Retinex-based methods require de-\ncomposing the image into reflectance and illumination com-\nponents, which is a highly ill-posed problem and there is\nno available ground truth. Previous works addressed this\nproblem by imposing some additional priors or regulariz-\ners. However, finding an effective prior or regularizer that\ncan be applied in various scenes is challenging, and the\nperformance of the model suffers from too many additional\nconstraints. We propose a contrastive learning method and\na self-knowledge distillation method for Retinex decompo-\nsition that allow training our Retinex-based model with-\nout elaborate hand-crafted regularization functions. Rather\nthan estimating reflectance and illuminance images and rep-\nresenting the final images as their element-wise products as\nin previous works, our regularizer-free Retinex decomposi-\ntion and synthesis network (RFR) extracts reflectance and\nilluminance features and synthesizes them end-to-end. In\naddition, we propose a loss function for contrastive learning\nand a progressive learning strategy for self-knowledge dis-\ntillation. Extensive experimental results demonstrate that\nour proposed methods can achieve superior performance\ncompared with state-of-the-art approaches.\n",
        "question": {
            "statement": "What is a common challenge when using Retinex-based networks for low-light image enhancement?",
            "options": [
                "Dealing with over-exposure in bright areas",
                "Achieving real-time processing speed",
                "Finding an effective prior or regularizer that can be applied in various scenes",
                "Handling noise reduction in dark areas"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Distribution Shift Inversion for Out-of-Distribution Prediction\nRunpeng Yu\nSonghua Liu\nXingyi Yang\nXinchao Wang:\nNational University of Singapore\n{r.yu,songhua.liu,xyang}@u.nus.edu\nxinchao@nus.edu.sg\nFigure 1. Transformed OoD samples from PACS dataset. Odd rows show the original OoD images, and even rows show their transformation\nresults to the source distribution, obtained by the proposed DSI. Please zoom in for better visualization.\nAbstract\nMachine learning society has witnessed the emergence\nof a myriad of Out-of-Distribution (OoD) algorithms, which\naddress the distribution shift between the training and the\ntesting distribution by searching for a unified predictor or\ninvariant feature representation. However, the task of di-\nrectly mitigating the distribution shift in the unseen testing\nset is rarely investigated, due to the unavailability of the\ntesting distribution during the training phase and thus the\nimpossibility of training a distribution translator mapping\nbetween the training and testing distribution. In this pa-\nper, we explore how to bypass the requirement of testing\ndistribution for distribution translator training and make\nthe distribution translation useful for OoD prediction. We\npropose a portable Distribution Shift Inversion (DSI) algo-\nrithm, in which, before being fed into the prediction model,\nthe OoD testing samples are first linearly combined with ad-\nditional Gaussian noise and then transferred back towards\nthe training distribution using a diffusion model trained only\non the source distribution. Theoretical analysis reveals the\nfeasibility of our method. Experimental results, on both\nmultiple-domain generalization datasets and single-domain\ngeneralization datasets, show that our method provides a\ngeneral performance gain when plugged into a wide range\nof commonly used OoD algorithms. Our code is available at\nhttps://github.com/yu-rp/Distribution-Shift-Iverson.\n",
        "question": {
            "statement": "What is the main challenge in directly addressing the distribution shift between the training and testing distributions?",
            "options": [
                "The unavailability of the testing distribution during the training phase",
                "The complexity of the training data",
                "The lack of robustness in the prediction model",
                "The high dimensionality of the feature space"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures\nGal Metzer*\nElad Richardson*\nOr Patashnik\nRaja Giryes\nDaniel Cohen-Or\nTel Aviv University\nLatent-NeRF\nSketch-Shape\nLatent-Paint\n“A stack of\npancakes covered\nin maple syrup”\n“A highly detailed\nsandcastle”\n“A German\nShepherd”\n“A ﬁsh with\nleopard spots”\nFigure 1. Our three text-guided models: a purely text-guided Latent-NeRF, Latent-NeRF with Sketch-Shape guidance for more exact\ncontrol over the generated shape, and Latent-Paint for texture generation for explicit shapes. The top row represents the models’ inputs.\nAbstract\nText-guided image generation has progressed rapidly in\nrecent years, inspiring major breakthroughs in text-guided\nshape generation. Recently, it has been shown that using\nscore distillation, one can successfully text-guide a NeRF\nmodel to generate a 3D object. We adapt the score distilla-\ntion to the publicly available, and computationally efﬁcient,\nLatent Diffusion Models, which apply the entire diffusion\nprocess in a compact latent space of a pretrained autoen-\ncoder. As NeRFs operate in image space, a na¨\nıve solution\nfor guiding them with latent score distillation would require\nencoding to the latent space at each guidance step. Instead,\nwe propose to bring the NeRF to the latent space, result-\ning in a Latent-NeRF. Analyzing our Latent-NeRF, we show\nthat while Text-to-3D models can generate impressive re-\nsults, they are inherently unconstrained and may lack the\nability to guide or enforce a speciﬁc 3D structure. To as-\nsist and direct the 3D generation, we propose to guide our\nLatent-NeRF using a Sketch-Shape: an abstract geometry\nthat deﬁnes the coarse structure of the desired object. Then,\nwe present means to integrate such a constraint directly into\na Latent-NeRF. This unique combination of text and shape\nguidance allows for increased control over the generation\nprocess. We also show that latent score distillation can be\nsuccessfully applied directly on 3D meshes. This allows\nfor generating high-quality textures on a given geometry.\nOur experiments validate the power of our different forms\nof guidance and the efﬁciency of using latent rendering.\n",
        "question": {
            "statement": "What is the main advantage of using a Latent-NeRF model compared to traditional NeRF models when it comes to text-guided 3D shape generation?",
            "options": [
                "It is better suited for generating realistic textures",
                "It provides higher quality results due to its ability to handle complex geometries",
                "It eliminates the need for score distillation",
                "It allows for more efficient computation by operating in a compact latent space"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Quality-aware Pre-trained Models for Blind Image Quality Assessment\nKai Zhao†, Kun Yuan†, Ming Sun, Mading Li and Xing Wen\nKuaishou Technology\n{zhaokai05,yuankun03,sunming03,limading,wenxing}@kuaishou.com\nAbstract\nBlind image quality assessment (BIQA) aims to auto-\nmatically evaluate the perceived quality of a single image,\nwhose performance has been improved by deep learning-\nbased methods in recent years. However, the paucity of la-\nbeled data somewhat restrains deep learning-based BIQA\nmethods from unleashing their full potential. In this pa-\nper, we propose to solve the problem by a pretext task\ncustomized for BIQA in a self-supervised learning manner,\nwhich enables learning representations from orders of mag-\nnitude more data. To constrain the learning process, we\npropose a quality-aware contrastive loss based on a simple\nassumption: the quality of patches from a distorted image\nshould be similar, but vary from patches from the same im-\nage with different degradations and patches from different\nimages. Further, we improve the existing degradation pro-\ncess and form a degradation space with the size of roughly\n2 × 107. After pre-trained on ImageNet using our method,\nmodels are more sensitive to image quality and perform sig-\nnificantly better on downstream BIQA tasks. Experimental\nresults show that our method obtains remarkable improve-\nments on popular BIQA datasets.\n",
        "question": {
            "statement": "What is a major challenge faced by deep learning-based blind image quality assessment (BIQA) methods?",
            "options": [
                "Insufficient computing power",
                "Inability to handle high-resolution images",
                "Lack of diversity in image types",
                "Limited availability of labeled data"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Re-IQA: Unsupervised Learning for Image Quality Assessment in the Wild\nAvinab Saha∗\nSandeep Mishra∗\nAlan C. Bovik\nLaboratory of Image and Video Engineering, The University of Texas at Austin\nAbstract\nAutomatic Perceptual Image Quality Assessment is a\nchallenging problem that impacts billions of internet, and\nsocial media users daily. To advance research in this field,\nwe propose a Mixture of Experts approach to train two sep-\narate encoders to learn high-level content and low-level im-\nage quality features in an unsupervised setting. The unique\nnovelty of our approach is its ability to generate low-level\nrepresentations of image quality that are complementary to\nhigh-level features representing image content. We refer to\nthe framework used to train the two encoders as Re-IQA.\nFor Image Quality Assessment in the Wild, we deploy the\ncomplementary low and high-level image representations\nobtained from the Re-IQA framework to train a linear re-\ngression model, which is used to map the image representa-\ntions to the ground truth quality scores, refer Figure 1. Our\nmethod achieves state-of-the-art performance on multiple\nlarge-scale image quality assessment databases containing\nboth real and synthetic distortions, demonstrating how deep\nneural networks can be trained in an unsupervised setting to\nproduce perceptually relevant representations. We conclude\nfrom our experiments that the low and high-level features\nobtained are indeed complementary and positively impact\nthe performance of the linear regressor. A public release of\nall the codes associated with this work will be made avail-\nable on GitHub.\n",
        "question": {
            "statement": "What is the main advantage of using a Mixture of Experts approach in image quality assessment?",
            "options": [
                "Improved computational efficiency",
                "Enhanced robustness to noise",
                "Ability to generate both high-level content and low-level image quality features",
                "Increased reliance on human annotations"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Perception and Semantic Aware Regularization for Sequential Confidence\nCalibration\nZhenghua Peng1, Yu Luo1, Tianshui Chen2, Keke Xu1, Shuangping Huang1,3,*\n1South China University of Technology, 2Guangdong University of Technology, 3Pazhou Laboratory\neepzh@mail.scut.edu.cn, luoyurl@126.com, tianshuichen@gmail.com,\neexkk@mail.scut.edu.cn, eehsp@scut.edu.cn\nAbstract\nDeep sequence recognition (DSR) models receive in-\ncreasing attention due to their superior application to var-\nious applications. Most DSR models use merely the tar-\nget sequences as supervision without considering other re-\nlated sequences, leading to over-confidence in their pre-\ndictions.\nThe DSR models trained with label smoothing\nregularize labels by equally and independently smoothing\neach token, reallocating a small value to other tokens for\nmitigating overconfidence. However, they do not consider\ntokens/sequences correlations that may provide more ef-\nfective information to regularize training and thus lead\nto sub-optimal performance.\nIn this work, we find to-\nkens/sequences with high perception and semantic correla-\ntions with the target ones contain more correlated and effec-\ntive information and thus facilitate more effective regular-\nization. To this end, we propose a Perception and Semantic\naware Sequence Regularization framework, which explore\nperceptively and semantically correlated tokens/sequences\nas regularization.\nSpecifically, we introduce a semantic\ncontext-free recognition and a language model to acquire\nsimilar sequences with high perceptive similarities and se-\nmantic correlation, respectively. Moreover, over-confidence\ndegree varies across samples according to their difficul-\nties. Thus, we further design an adaptive calibration in-\ntensity module to compute a difficulty score for each sam-\nples to obtain finer-grained regularization. Extensive ex-\nperiments on canonical sequence recognition tasks, includ-\ning scene text and speech recognition, demonstrate that our\nmethod sets novel state-of-the-art results. Code is available\nat https://github.com/husterpzh/PSSR.\n",
        "question": {
            "statement": "What is a limitation of traditional deep sequence recognition models?",
            "options": [
                "They do not consider correlations between tokens or sequences",
                "They require large amounts of labeled data",
                "They are not suitable for real-time applications",
                "They are only applicable to specific domains"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Joint Token Pruning and Squeezing Towards More Aggressive Compression of\nVision Transformers\nSiyuan Wei1*\nTianzhu Ye2*\nShen Zhang1\nYao Tang1\nJiajun Liang1†\n1MEGVII Technology\n2Tsinghua University\n{weisiyuan, zhangshen,tangyao02,liangjiajun}@megvii.com,ytz20@mails.tsinghua.edu.cn\nAbstract\nAlthough vision transformers (ViTs) have shown promis-\ning results in various computer vision tasks recently, their\nhigh computational cost limits their practical applications.\nPrevious approaches that prune redundant tokens have\ndemonstrated a good trade-off between performance and\ncomputation costs. Nevertheless, errors caused by prun-\ning strategies can lead to significant information loss. Our\nquantitative experiments reveal that the impact of pruned\ntokens on performance should be noticeable. To address\nthis issue, we propose a novel joint Token Pruning &\nSqueezing module (TPS) for compressing vision transform-\ners with higher efficiency. Firstly, TPS adopts pruning to get\nthe reserved and pruned subsets. Secondly, TPS squeezes\nthe information of pruned tokens into partial reserved to-\nkens via the unidirectional nearest-neighbor matching and\nsimilarity-based fusing steps.\nCompared to state-of-the-\nart methods, our approach outperforms them under all to-\nken pruning intensities. Especially while shrinking DeiT-\ntiny&small computational budgets to 35%, it improves the\naccuracy by 1%-6% compared with baselines on ImageNet\nclassification.\nThe proposed method can accelerate the\nthroughput of DeiT-small beyond DeiT-tiny, while its accu-\nracy surpasses DeiT-tiny by 4.78%. Experiments on various\ntransformers demonstrate the effectiveness of our method,\nwhile analysis experiments prove our higher robustness to\nthe errors of the token pruning policy. Code is available at\nhttps://github.com/megvii-research/TPS-\nCVPR2023.\n",
        "question": {
            "statement": "What is a major limitation of vision transformers that recent approaches have tried to address?",
            "options": [
                "Their high computational cost",
                "Their limited ability to process sequential data",
                "Their lack of interpretability",
                "Their inability to handle multi-modal inputs"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Transformer-based Unified Recognition of Two Hands Manipulating Objects\nHoseong Cho\nChanwoo Kim\nJihyeon Kim\nSeongyeong Lee\nElkhan Ismayilzada\nSeungryul Baek\nUNIST, South Korea\nAbstract\nUnderstanding the hand-object interactions from an\negocentric video has received a great attention recently. So\nfar, most approaches are based on the convolutional neural\nnetwork (CNN) features combined with the temporal encoding\nvia the long short-term memory (LSTM) or graph convolution\nnetwork (GCN) to provide the unified understanding of two\nhands, an object and their interactions. In this paper, we\npropose the Transformer-based unified framework that provides\nbetter understanding of two hands manipulating objects. In\nour framework, we insert the whole image depicting two hands,\nan object and their interactions as input and jointly estimate\n3 information from each frame: poses of two hands, pose of an\nobject and object types. Afterwards, the action class defined\nby the hand-object interactions is predicted from the entire\nvideo based on the estimated information combined with the\ncontact map that encodes the interaction between two hands\nand an object. Experiments are conducted on H2O and FPHA\nbenchmark datasets and we demonstrated the superiority of our\nmethod achieving the state-of-the-art accuracy. Ablative studies\nfurther demonstrate the effectiveness of each proposed module.\n",
        "question": {
            "statement": "What type of deep learning model is used in the proposed framework for understanding hand-object interactions?",
            "options": [
                "Convolutional Neural Network (CNN)",
                "Graph Convolution Network (GCN)",
                "Long Short-Term Memory (LSTM)",
                "Transformer"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Simulated Annealing in Early Layers Leads to Better Generalization\nAmir M. Sarfi1,2\nZahra Karimpour1\nMuawiz Chaudhary1,2\nNasir M. Khalid 1,2\nMirco Ravanelli1,2\nSudhir Mudur1\nEugene Belilovsky1,2\n1 Concordia University\n2 Mila – Quebec AI Institute\nAbstract\nRecently, a number of iterative learning methods have\nbeen introduced to improve generalization. These typically\nrely on training for longer periods of time in exchange for\nimproved generalization. LLF (later-layer-forgetting) is a\nstate-of-the-art method in this category. It strengthens learn-\ning in early layers by periodically re-initializing the last\nfew layers of the network. Our principal innovation in this\nwork is to use Simulated annealing in EArly Layers (SEAL)\nof the network in place of re-initialization of later layers.\nEssentially, later layers go through the normal gradient de-\nscent process, while the early layers go through short stints\nof gradient ascent followed by gradient descent. Extensive\nexperiments on the popular Tiny-ImageNet dataset bench-\nmark and a series of transfer learning and few-shot learning\ntasks show that we outperform LLF by a significant margin.\nWe further show that, compared to normal training, LLF\nfeatures, although improving on the target task, degrade\nthe transfer learning performance across all datasets we ex-\nplored. In comparison, our method outperforms LLF across\nthe same target datasets by a large margin. We also show\nthat the prediction depth of our method is significantly lower\nthan that of LLF and normal training, indicating on average\nbetter prediction performance.1\n",
        "question": {
            "statement": "What is the main difference between the SEAL method and the LLF method in terms of how they treat different layers of a neural network?",
            "options": [
                "LLF uses simulated annealing in early layers, while SEAL re-initializes later layers",
                "SEAL uses simulated annealing in early layers, while LLF re-initializes later layers",
                "Both methods use gradient descent in all layers",
                "SEAL uses gradient descent in early layers, while LLF uses gradient ascent in later layers"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Token Contrast for Weakly-Supervised Semantic Segmentation\nLixiang Ru1\nHeliang Zheng2\nYibing Zhan2\nBo Du1*\n1 Institute of Artificial Intelligence, School of Computer Science,\nNational Engineering Research Center for Multimedia Software and Hubei Key Laboratory\nof Multimedia and Network Communication Engineering, Wuhan University, China.\n2 JD Explore Academy, China\n{rulixiang, dubo}@whu.edu.cn\n{zhengheliang,zhanyibing}@jd.com\nAbstract\nWeakly-Supervised Semantic Segmentation (WSSS) us-\ning image-level labels typically utilizes Class Activation\nMap (CAM) to generate the pseudo labels. Limited by the\nlocal structure perception of CNN, CAM usually cannot\nidentify the integral object regions. Though the recent Vi-\nsion Transformer (ViT) can remedy this flaw, we observe it\nalso brings the over-smoothing issue, i.e., the final patch to-\nkens incline to be uniform. In this work, we propose Token\nContrast (ToCo) to address this issue and further explore\nthe virtue of ViT for WSSS. Firstly, motivated by the obser-\nvation that intermediate layers in ViT can still retain se-\nmantic diversity, we designed a Patch Token Contrast mod-\nule (PTC). PTC supervises the final patch tokens with the\npseudo token relations derived from intermediate layers, al-\nlowing them to align the semantic regions and thus yield\nmore accurate CAM. Secondly, to further differentiate the\nlow-confidence regions in CAM, we devised a Class Token\nContrast module (CTC) inspired by the fact that class tokens\nin ViT can capture high-level semantics. CTC facilitates the\nrepresentation consistency between uncertain local regions\nand global objects by contrasting their class tokens. Exper-\niments on the PASCAL VOC and MS COCO datasets show\nthe proposed ToCo can remarkably surpass other single-\nstage competitors and achieve comparable performance\nwith state-of-the-art multi-stage methods. Code is available\nat https://github.com/rulixiang/ToCo.\n",
        "question": {
            "statement": "What is a limitation of using Class Activation Maps (CAM) in weakly-supervised semantic segmentation?",
            "options": [
                "CAM is sensitive to image resolution",
                "CAM often fails to identify integral object regions",
                "CAM is limited to specific object classes",
                "CAM requires strong supervision"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "3",
                "2"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Hybrid Active Learning via Deep Clustering for Video Action Detection\nAayush J Rana\nYogesh S Rawat\naayushjr@knights.ucf.edu\nyogesh@crcv.ucf.edu\nCenter for Research in Computer Vision (CRCV)\nUniversity of Central Florida\nAbstract\nIn this work, we focus on reducing the annotation cost\nfor video action detection which requires costly frame-wise\ndense annotations. We study a novel hybrid active learning\n(AL) strategy which performs efficient labeling using both\nintra-sample and inter-sample selection. The intra-sample\nselection leads to labeling of fewer frames in a video as op-\nposed to inter-sample selection which operates at video level.\nThis hybrid strategy reduces the annotation cost from two dif-\nferent aspects leading to significant labeling cost reduction.\nThe proposed approach utilize Clustering-Aware Uncertainty\nScoring (CLAUS), a novel label acquisition strategy which\nrelies on both informativeness and diversity for sample se-\nlection. We also propose a novel Spatio-Temporal Weighted\n(STeW) loss formulation, which helps in model training un-\nder limited annotations. The proposed approach is evaluated\non UCF-101-24 and J-HMDB-21 datasets demonstrating\nits effectiveness in significantly reducing the annotation cost\nwhere it consistently outperforms other baselines. Project\ndetails available at https://tinyurl.com/hybridclaus\n",
        "question": {
            "statement": "What is the main goal of the proposed hybrid active learning strategy in video action detection?",
            "options": [
                "Reducing the annotation cost",
                "Improving the accuracy of action detection models",
                "Increasing the number of annotated frames in a video",
                "Enhancing the diversity of actions detected in a video"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "STMixer: A One-Stage Sparse Action Detector\nTao Wu1,*\nMengqi Cao1,*\nZiteng Gao1\nGangshan Wu1\nLimin Wang1,2, ",
        "question": {
            "statement": "What is the name of the action detector proposed in the paper?",
            "options": [
                "SparseActionNet",
                "OneStageMixer",
                "STMixer",
                "STDetector"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data\nJihye Park *1, Sunwoo Kim *1, Soohyun Kim *1, Seokju Cho 1,\nJaejun Yoo 2, Youngjung Uh 3, Seungryong Kim †1\n1 Korea University, Seoul, Korea\n2 UNIST, Ulsan, Korea\n3 Yonsei University, Seoul, Korea\n1{ghp1112,sw-kim,shkim1211,seokju cho,seungryong kim}@korea.ac.kr\n2jaejun.yoo@unist.ac.kr\n3yj.uh@yonsei.ac.kr\nAbstract\nExisting techniques for image-to-image translation com-\nmonly have suffered from two critical problems: heavy\nreliance on per-sample domain annotation and/or inabil-\nity to handle multiple attributes per image. Recent truly-\nunsupervised methods adopt clustering approaches to eas-\nily provide per-sample one-hot domain labels. However,\nthey cannot account for the real-world setting: one sample\nmay have multiple attributes. In addition, the semantics of\nthe clusters are not easily coupled to human understanding.\nTo overcome these, we present LANguage-driven Image-to-\nimage Translation model, dubbed LANIT. We leverage easy-\nto-obtain candidate attributes given in texts for a dataset:\nthe similarity between images and attributes indicates per-\nsample domain labels. This formulation naturally enables\nmulti-hot labels so that users can specify the target domain\nwith a set of attributes in language.\nTo account for the\ncase that the initial prompts are inaccurate, we also present\nprompt learning. We further present domain regularization\nloss that enforces translated images to be mapped to the\ncorresponding domain. Experiments on several standard\nbenchmarks demonstrate that LANIT achieves comparable\nor superior performance to existing models. The code is\navailable at github.com/KU-CVLAB/LANIT.\n",
        "question": {
            "statement": "What is a limitation of recent unsupervised image-to-image translation methods?",
            "options": [
                "They require large amounts of labeled data",
                "They rely heavily on manual feature engineering",
                "They are limited to specific domains or datasets",
                "They cannot handle images with multiple attributes."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "2",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "DoNet: Deep De-overlapping Network for Cytology Instance Segmentation\nHao Jiang1*\nRushan Zhang1*\nYanning Zhou2\nYumeng Wang1\nHao Chen1†\n1The Hong Kong University of Science and Technology\n2Tencent AI Lab\n{hjiangaz,jhc}@cse.ust.hk, {rzhangbq,ywanglu}@connect.ust.hk, amandayzhou@tencent.com\nAbstract\nCell instance segmentation in cytology images has sig-\nnificant importance for biology analysis and cancer screen-\ning, while remains challenging due to 1) the extensive over-\nlapping translucent cell clusters that cause the ambigu-\nous boundaries, and 2) the confusion of mimics and de-\nbris as nuclei. In this work, we proposed a De-overlapping\nNetwork (DoNet) in a decompose-and-recombined strat-\negy. A Dual-path Region Segmentation Module (DRM) ex-\nplicitly decomposes the cell clusters into intersection and\ncomplement regions, followed by a Semantic Consistency-\nguided Recombination Module (CRM) for integration. To\nfurther introduce the containment relationship of the nu-\ncleus in the cytoplasm, we design a Mask-guided Region\nProposal Strategy (MRP) that integrates the cell attention\nmaps for inner-cell instance prediction.\nWe validate the\nproposed approach on ISBI2014 and CPS datasets.\nEx-\nperiments show that our proposed DoNet significantly out-\nperforms other state-of-the-art (SOTA) cell instance seg-\nmentation methods.\nThe code is available at\nhttps:\n//github.com/DeepDoNet/DoNet.\n",
        "question": {
            "statement": "What is a major challenge in cell instance segmentation in cytology images?",
            "options": [
                "overlapping translucent cell clusters",
                "variations in nucleus shape",
                "insufficient training data",
                "low image resolution"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "8",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "DBARF: Deep Bundle-Adjusting Generalizable Neural Radiance Fields\nYu Chen\nGim Hee Lee\nDepartment of Computer Science, National University of Singapore\n{chenyu, gimhee.lee}@nus.edu.sg\nAbstract\nRecent works such as BARF and GARF can bundle ad-\njust camera poses with neural radiance fields (NeRF) which\nis based on coordinate-MLPs. Despite the impressive re-\nsults, these methods cannot be applied to Generalizable\nNeRFs (GeNeRFs) which require image feature extractions\nthat are often based on more complicated 3D CNN or trans-\nformer architectures. In this work, we first analyze the dif-\nficulties of jointly optimizing camera poses with GeNeRFs,\nand then further propose our DBARF to tackle these issues.\nOur DBARF which bundle adjusts camera poses by tak-\ning a cost feature map as an implicit cost function can be\njointly trained with GeNeRFs in a self-supervised manner.\nUnlike BARF and its follow-up works, which can only be\napplied to per-scene optimized NeRFs and need accurate\ninitial camera poses with the exception of forward-facing\nscenes, our method can generalize across scenes and does\nnot require any good initialization. Experiments show the\neffectiveness and generalization ability of our DBARF when\nevaluated on real-world datasets. Our code is available at\nhttps://aibluefisher.github.io/dbarf.\n",
        "question": {
            "statement": "What is a limitation of existing methods such as BARF and GARF?",
            "options": [
                "They require large amounts of training data",
                "They can only be applied to per-scene optimized NeRFs",
                "They are only applicable to synthetic datasets",
                "They are limited to forward-facing scenes only"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "3",
                "5"
            ]
        },
        "difference": 5,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Neuralangelo: High-Fidelity Neural Surface Reconstruction\nZhaoshuo Li1,2\nThomas Müller1\nAlex Evans1\nRussell H. Taylor2\nMathias Unberath2\nMing-Yu Liu1\nChen-Hsuan Lin1\n1NVIDIA Research\n2Johns Hopkins University\nhttps://research.nvidia.com/labs/dir/neuralangelo\nNeuralangelo\n3D surface\nreconstruction\nsurface\nnormal\nFigure 1. We present Neuralangelo, a framework for high-fidelity 3D surface reconstruction from RGB images using neural volume\nrendering, even without auxiliary data such as segmentation or depth. Shown in the figure is an extracted 3D mesh of a courthouse.\nAbstract\nNeural surface reconstruction has been shown to be pow-\nerful for recovering dense 3D surfaces via image-based neu-\nral rendering. However, current methods struggle to recover\ndetailed structures of real-world scenes. To address the\nissue, we present Neuralangelo, which combines the rep-\nresentation power of multi-resolution 3D hash grids with\nneural surface rendering. Two key ingredients enable our ap-\nproach: (1) numerical gradients for computing higher-order\nderivatives as a smoothing operation and (2) coarse-to-fine\noptimization on the hash grids controlling different levels of\ndetails. Even without auxiliary inputs such as depth, Neu-\nralangelo can effectively recover dense 3D surface structures\nfrom multi-view images with fidelity significantly surpass-\ning previous methods, enabling detailed large-scale scene\nreconstruction from RGB video captures.\n",
        "question": {
            "statement": "What is the main advantage of Neuralangelo over other neural surface reconstruction methods?",
            "options": [
                "It can only be applied to small-scale scenes",
                "It relies heavily on manual segmentation of input images",
                "It uses a single resolution 3D grid for surface rendering",
                "It can recover dense 3D surface structures from multi-view images without requiring auxiliary inputs"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Guiding Pseudo-labels with Uncertainty Estimation for Source-free Unsupervised\nDomain Adaptation\nMattia Litrico\nAlessio Del Bue\nPietro Morerio\nPattern Analysis and Computer Vision (PAVIS) - Istituto Italiano di Tecnologia\nmattia.litrico@phd.unict.it, alessio.delbue@iit.it, pietro.morerio@iit.it\nAbstract\nStandard Unsupervised Domain Adaptation (UDA) meth-\nods assume the availability of both source and target data\nduring the adaptation. In this work, we investigate Source-\nfree Unsupervised Domain Adaptation (SF-UDA), a specific\ncase of UDA where a model is adapted to a target domain\nwithout access to source data. We propose a novel approach\nfor the SF-UDA setting based on a loss reweighting strategy\nthat brings robustness against the noise that inevitably af-\nfects the pseudo-labels. The classification loss is reweighted\nbased on the reliability of the pseudo-labels that is measured\nby estimating their uncertainty. Guided by such reweight-\ning strategy, the pseudo-labels are progressively refined by\naggregating knowledge from neighbouring samples. Further-\nmore, a self-supervised contrastive framework is leveraged\nas a target space regulariser to enhance such knowledge\naggregation. A novel negative pairs exclusion strategy is pro-\nposed to identify and exclude negative pairs made of samples\nsharing the same class, even in presence of some noise in the\npseudo-labels. Our method outperforms previous methods\non three major benchmarks by a large margin. We set the new\nSF-UDA state-of-the-art on VisDA-C and DomainNet with\na performance gain of +1.8% on both benchmarks and on\nPACS with +12.3% in the single-source setting and +6.6%\nin multi-target adaptation. Additional analyses demonstrate\nthat the proposed approach is robust to the noise, which re-\nsults in significantly more accurate pseudo-labels compared\nto state-of-the-art approaches.\n",
        "question": {
            "statement": "What is a key challenge in source-free unsupervised domain adaptation, and how does the proposed approach address it?",
            "options": [
                "The key challenge is overfitting, and the approach addresses it by using early stopping.",
                "The key challenge is lack of target data, and the approach addresses it by generating synthetic data.",
                "The key challenge is class imbalance, and the approach addresses it by oversampling minority classes.",
                "The key challenge is noisy pseudo-labels, and the approach addresses it by reweighting the classification loss based on the reliability of the pseudo-labels."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Neural Map Prior for Autonomous Driving\nXuan Xiong1\nYicheng Liu1\nTianyuan Yuan2\nYue Wang3\nYilun Wang2\nHang Zhao2,1*\n1Shanghai Qi Zhi Institute\n2IIIS, Tsinghua University\n3MIT\nAbstract\nHigh-definition (HD) semantic maps are crucial for au-\ntonomous vehicles navigating urban environments.\nTra-\nditional offline HD maps, created through labor-intensive\nmanual annotation processes, are both costly and incapable\nof accommodating timely updates. Recently, researchers\nhave proposed inferring local maps based on online sensor\nobservations; however, this approach is constrained by the\nsensor perception range and is susceptible to occlusions. In\nthis work, we propose Neural Map Prior (NMP), a neu-\nral representation of global maps that facilitates automatic\nglobal map updates and improves local map inference per-\nformance. To incorporate the strong map prior into local\nmap inference, we employ cross-attention that dynamically\ncaptures correlations between current features and prior\nfeatures.\nFor updating the global neural map prior, we\nuse a learning-based fusion module to guide the network\nin fusing features from previous traversals.\nThis design\nallows the network to capture a global neural map prior\nduring sequential online map predictions. Experimental re-\nsults on the nuScenes dataset demonstrate that our frame-\nwork is highly compatible with various map segmentation\nand detection architectures and considerably strengthens\nmap prediction performance, even under adverse weather\nconditions and across longer horizons. To the best of our\nknowledge, this represents the first learning-based system\nfor constructing a global map prior.\n",
        "question": {
            "statement": "What is a major limitation of traditional high-definition semantic maps for autonomous driving?",
            "options": [
                "They are only used for rural areas",
                "They require human drivers",
                "They are difficult to update in real-time",
                "They are not detailed enough"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Paint by Example: Exemplar-based Image Editing with Diffusion Models\nBinxin Yang1*\nShuyang Gu2\nBo Zhang2†\nTing Zhang2\nXuejin Chen1\nXiaoyan Sun1\nDong Chen2\nFang Wen2\n1 University of Science and Technology of China\n2 Microsoft Research Asia\nFigure 1. Paint by Example. Users are able to edit a scene by painting with a conditional image. Our approach can automatically alter the\nreference image and merge it into the source image, and achieve a high-quality result.\nAbstract\nLanguage-guided image editing has achieved great suc-\ncess recently. In this paper, we investigate exemplar-guided\nimage editing for more precise control.\nWe achieve this\ngoal by leveraging self-supervised training to disentangle\nand re-organize the source image and the exemplar. How-\never, the naive approach will cause obvious fusing artifacts.\nWe carefully analyze it and propose a content bottleneck\nand strong augmentations to avoid the trivial solution of di-\nrectly copying and pasting the exemplar image. Meanwhile,\nto ensure the controllability of the editing process, we de-\nsign an arbitrary shape mask for the exemplar image and\nleverage the classiﬁer-free guidance to increase the similar-\nity to the exemplar image. The whole framework involves\na single forward of the diffusion model without any itera-\ntive optimization. We demonstrate that our method achieves\nan impressive performance and enables controllable edit-\ning on in-the-wild images with high ﬁdelity. The code and\n*Work is done during the internship at Microsoft Research Asia.\n†Corresponding author.\npretrained models are available at https://github.\ncom/Fantasy-Studio/Paint-by-Example.\n",
        "question": {
            "statement": "What is the main goal of the 'Paint by Example' approach in image editing?",
            "options": [
                "To directly copy and paste the exemplar image onto the source image",
                "To use language-guided image editing for more flexibility",
                "To remove the background of the exemplar image",
                "To enable precise control over the editing process by using an exemplar image"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Transfer Knowledge from Head to Tail:\nUncertainty Calibration under Long-tailed Distribution\nJiahao Chen1 2, Bing Su1 2 †\n1 Gaoling School of Artificial Intelligence, Renmin University of China\n2 Beijing Key Laboratory of Big Data Management and Analysis Methods\n{nicelemon666, subingats}@gmail.com\nAbstract\nHow to estimate the uncertainty of a given model is a\ncrucial problem. Current calibration techniques treat dif-\nferent classes equally and thus implicitly assume that the\ndistribution of training data is balanced, but ignore the fact\nthat real-world data often follows a long-tailed distribu-\ntion. In this paper, we explore the problem of calibrating\nthe model trained from a long-tailed distribution. Due to\nthe difference between the imbalanced training distribution\nand balanced test distribution, existing calibration methods\nsuch as temperature scaling can not generalize well to this\nproblem. Specific calibration methods for domain adapta-\ntion are also not applicable because they rely on unlabeled\ntarget domain instances which are not available. Models\ntrained from a long-tailed distribution tend to be more over-\nconfident to head classes. To this end, we propose a novel\nknowledge-transferring-based calibration method by esti-\nmating the importance weights for samples of tail classes to\nrealize long-tailed calibration. Our method models the dis-\ntribution of each class as a Gaussian distribution and views\nthe source statistics of head classes as a prior to calibrate\nthe target distributions of tail classes. We adaptively trans-\nfer knowledge from head classes to get the target probability\ndensity of tail classes. The importance weight is estimated\nby the ratio of the target probability density over the source\nprobability density. Extensive experiments on CIFAR-10-\nLT, MNIST-LT, CIFAR-100-LT, and ImageNet-LT datasets\ndemonstrate the effectiveness of our method.\n",
        "question": {
            "statement": "What is a common issue with current calibration techniques when dealing with real-world data?",
            "options": [
                "They assume a balanced distribution of training data",
                "They require labeled test data",
                "They are only applicable to binary classification problems",
                "They are too computationally expensive"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object\nInteractions\nJuze Zhang 1,2,3,4,*, Haimin Luo 1,4, 5,*, Hongdi Yang 1,4, Xinru Xu 1,4, Qianyang Wu 1,4, Ye Shi 1,4,\nJingyi Yu 1,4, Lan Xu 1,4, †, Jingya Wang 1,4,†\n1 ShanghaiTech University 2 Shanghai Advanced Research Institute, Chinese Academy of Sciences\n3 University of Chinese Academy of Sciences\n4 Shanghai Engineering Research Center of Intelligent Vision and Imaging 5 LumiAni Technology\n{zhangjz,luohm,yanghd,xuxr2022,wuqy2022,shiye,yujingyi,xulan1,wangjingya}@shanghaitech.edu.cn\nFigure 1. Our NeuralDome pipeline for processing multi-view video sequences on human object interactions. NeuralDome supports\ntracking, modeling, and rendering of individual human subjects and objects. To validate NeuralDome, we collect a large dataset HODome\nover a total of 71 million video frames across 76 viewpoints and process the datasets using NeuralDome for a variety of inference and\nneural modeling and rendering tasks.\nAbstract\nHumans constantly interact with objects in daily life\ntasks. Capturing such processes and subsequently conduct-\ning visual inferences from a fixed viewpoint suffers from\nocclusions, shape and texture ambiguities, motions, etc.\nTo mitigate the problem, it is essential to build a train-\ning dataset that captures free-viewpoint interactions. We\nconstruct a dense multi-view dome to acquire a complex\nhuman object interaction dataset, named HODome, that\nconsists of ∼71M frames on 10 subjects interacting with\n23 objects. To process the HODome dataset, we develop\nNeuralDome, a layer-wise neural processing pipeline tai-\n* These authors contributed equally.\n†Corresponding author.\nlored for multi-view video inputs to conduct accurate track-\ning, geometry reconstruction and free-view rendering, for\nboth human subjects and objects. Extensive experiments\non the HODome dataset demonstrate the effectiveness of\nNeuralDome on a variety of inference, modeling, and ren-\ndering tasks. Both the dataset and the NeuralDome tools\nwill be disseminated to the community for further devel-\nopment, which can be found at https://juzezhang.\ngithub.io/NeuralDome\n",
        "question": {
            "statement": "What is the main advantage of capturing human-object interactions from multiple viewpoints?",
            "options": [
                "It allows for more efficient data collection",
                "It mitigates problems such as occlusions, shape and texture ambiguities, and motion",
                "It reduces the need for high-quality cameras",
                "It enables the use of simpler computer vision algorithms"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Cooperation or Competition: Avoiding Player Domination for Multi-Target\nRobustness via Adaptive Budgets\nYimu Wang\nUniversity of Waterloo\nWaterloo, Canada\nyimu.wang@uwaterloo.ca\nDinghuai Zhang\nMila, University of Montreal\nMontreal, Canada\ndinghuai.zhang@mila.quebec\nYihan Wu\nUniversity of Pittsburgh\nPittsburgh, United States\nyiw154@pitt.edu\nHeng Huang\nUniversity of Pittsburgh\nPittsburgh, United States\nhenghuanghh@gmail.com\nHongyang Zhang *\nUniversity of Waterloo\nWaterloo, Canada\nhongyang.zhang@uwaterloo.ca\nAbstract\nDespite incredible advances, deep learning has been\nshown to be susceptible to adversarial attacks. Numerous\napproaches have been proposed to train robust networks\nboth empirically and certiﬁably. However, most of them de-\nfend against only a single type of attack, while recent work\ntakes steps forward in defending against multiple attacks. In\nthis paper, to understand multi-target robustness, we view\nthis problem as a bargaining game in which different players\n(adversaries) negotiate to reach an agreement on a joint\ndirection of parameter updating. We identify a phenomenon\nnamed player domination in the bargaining game, namely\nthat the existing max-based approaches, such as MAX and\nMSD, do not converge. Based on our theoretical analysis, we\ndesign a novel framework that adjusts the budgets of differ-\nent adversaries to avoid any player dominance. Experiments\non standard benchmarks show that employing the proposed\nframework to the existing approaches signiﬁcantly advances\nmulti-target robustness.\n",
        "question": {
            "statement": "What is a major limitation of existing approaches to defending against multiple types of adversarial attacks in deep learning?",
            "options": [
                "They are ineffective against single-type attacks.",
                "They require large amounts of training data.",
                "They tend to prioritize one type of attack over others.",
                "They are too computationally expensive."
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks?\nA: Self-Train on Unlabeled Images!\nZaid Khan†∗Vijay Kumar BG♣Samuel Schulter♣Xiang Yu♢∗Yun Fu† Manmohan Chandraker♣♡\n†Northeastern University, ♣NEC Labs America, ♢Amazon, ♡UC San Diego\nAbstract\nFinetuning a large vision language model (VLM) on a\ntarget dataset after large scale pretraining is a dominant\nparadigm in visual question answering (VQA). Datasets for\nspecialized tasks such as knowledge-based VQA or VQA\nin non natural-image domains are orders of magnitude\nsmaller than those for general-purpose VQA. While col-\nlecting additional labels for specialized tasks or domains\ncan be challenging, unlabeled images are often available.\nWe introduce SelTDA (Self-Taught Data Augmentation),\na strategy for finetuning large VLMs on small-scale VQA\ndatasets. SelTDA uses the VLM and target dataset to build a\nteacher model that can generate question-answer pseudola-\nbels directly conditioned on an image alone, allowing us to\npseudolabel unlabeled images. SelTDA then finetunes the\ninitial VLM on the original dataset augmented with freshly\npseudolabeled images. We describe a series of experiments\nshowing that our self-taught data augmentation increases ro-\nbustness to adversarially searched questions, counterfactual\nexamples and rephrasings, improves domain generalization,\nand results in greater retention of numerical reasoning skills.\nThe proposed strategy requires no additional annotations or\narchitectural modifications, and is compatible with any mod-\nern encoder-decoder multimodal transformer. Code avail-\nable at https://github.com/codezakh/SelTDA.\n",
        "question": {
            "statement": "What is a potential advantage of using self-taught data augmentation when fine-tuning large vision-language models on small-scale datasets?",
            "options": [
                "It is only compatible with traditional computer vision models",
                "It does not require collecting additional labels for specialized tasks or domains",
                "It requires significant architectural modifications to the model",
                "It always leads to state-of-the-art performance on all VQA tasks"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "PROTOCON: Pseudo-label Refinement via Online Clustering and Prototypical\nConsistency for Efficient Semi-supervised Learning\nIslam Nassar1\nMunawar Hayat1\nEhsan Abbasnejad2\nHamid Rezatofighi1\nGholamreza Haffari1\n1 Data Science and AI Department, Monash University, Australia – firstname.lastname@monash.edu\n2 Australian Institute for Machine Learning, The University of Adelaide, Australia – firstname.lastname@adelaide.edu.au\nAbstract\nConfidence-based pseudo-labeling is among the domi-\nnant approaches in semi-supervised learning (SSL). It re-\nlies on including high-confidence predictions made on un-\nlabeled data as additional targets to train the model. We\npropose PROTOCON, a novel SSL method aimed at the less-\nexplored label-scarce SSL where such methods usually un-\nderperform. PROTOCON refines the pseudo-labels by lever-\naging their nearest neighbours’ information. The neigh-\nbours are identified as the training proceeds using an on-\nline clustering approach operating in an embedding space\ntrained via a prototypical loss to encourage well-formed\nclusters.\nThe online nature of PROTOCON allows it to\nutilise the label history of the entire dataset in one train-\ning cycle to refine labels in the following cycle without the\nneed to store image embeddings. Hence, it can seamlessly\nscale to larger datasets at a low cost. Finally, PROTOCON\naddresses the poor training signal in the initial phase of\ntraining (due to fewer confident predictions) by introduc-\ning an auxiliary self-supervised loss. It delivers significant\ngains and faster convergence over state-of-the-art across 5\ndatasets, including CIFARs, ImageNet and DomainNet.\n",
        "question": {
            "statement": "What is a common limitation of confidence-based pseudo-labeling approaches in semi-supervised learning?",
            "options": [
                "They are computationally expensive",
                "They are sensitive to hyperparameter tuning",
                "They require large amounts of labeled data",
                "They often underperform in label-scarce scenarios"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "MaLP: Manipulation Localization Using a Proactive Scheme\nVishal Asnani1,\nXi Yin2,\nTal Hassner2,\nXiaoming Liu1\n1*Michigan State University, 2Meta AI\n1{asnanivi, liuxm}@msu.edu, 2{yinxi, thassner}@meta.com\nAbstract\nAdvancements in the generation quality of various Gen-\nerative Models (GMs) has made it necessary to not only\nperform binary manipulation detection but also localize the\nmodified pixels in an image. However, prior works termed\nas passive for manipulation localization exhibit poor gener-\nalization performance over unseen GMs and attribute mod-\nifications. To combat this issue, we propose a proactive\nscheme for manipulation localization, termed MaLP. We\nencrypt the real images by adding a learned template. If\nthe image is manipulated by any GM, this added protec-\ntion from the template not only aids binary detection but\nalso helps in identifying the pixels modified by the GM.\nThe template is learned by leveraging local and global-level\nfeatures estimated by a two-branch architecture. We show\nthat MaLP performs better than prior passive works. We\nalso show the generalizability of MaLP by testing on 22\ndifferent GMs, providing a benchmark for future research\non manipulation localization. Finally, we show that MaLP\ncan be used as a discriminator for improving the gener-\nation quality of GMs. Our models/codes are available at\nwww.github.com/vishal3477/pro_loc.\n",
        "question": {
            "statement": "What is the primary goal of the proposed scheme MaLP in the context of image manipulation?",
            "options": [
                "To improve the training process of Generative Models",
                "To detect whether an image is manipulated or not",
                "To generate high-quality images using Generative Models",
                "To identify the modified pixels in an image"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "5",
                "0",
                "10"
            ]
        },
        "difference": 5,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "CAMS: CAnonicalized Manipulation Spaces for\nCategory-Level Functional Hand-Object Manipulation Synthesis\nJuntian Zheng*,1,3\nQingyuan Zheng*,3\nLixing Fang*,1,3\nYun Liu1\nLi Yi†,1,2,3\n1IIIS, Tsinghua University\n2Shanghai Artificial Intelligence Laboratory\n3Shanghai Qi Zhi Institute\nFigure 1. Given a sequence of manipulation goals, our method can generate realistic and diverse functional manipulation motions consistent\nwith the goal sequence. The motions are expressed in snapshots at several keyframes. On the left side, we show a goal sequence of opening\na laptop and two different manipulation patterns that can be generated by our method. On the right side, we show our generation results of\nthree manipulation tasks corresponding to different object categories.\nAbstract\nIn this work, we focus on a novel task of category-\nlevel functional hand-object manipulation synthesis cover-\ning both rigid and articulated object categories. Given an\nobject geometry, an initial human hand pose as well as a\nsparse control sequence of object poses, our goal is to gen-\nerate a physically reasonable hand-object manipulation se-\nquence that performs like human beings. To address such\na challenge, we first design CAnonicalized Manipulation\nSpaces (CAMS), a two-level space hierarchy that canon-\nicalizes the hand poses in an object-centric and contact-\ncentric view. Benefiting from the representation capability\nof CAMS, we then present a two-stage framework for syn-\nthesizing human-like manipulation animations. Our frame-\nwork achieves state-of-the-art performance for both rigid\nand articulated categories with impressive visual effects.\nCodes and video results can be found at our project home-\npage: https://cams-hoi.github.io/.\n*Equal contribution with the order determined by rolling dice.\n†Corresponding author.\n",
        "question": {
            "statement": "What is the main goal of the proposed method in generating hand-object manipulation sequences?",
            "options": [
                "to recognize objects from images",
                "to generate physically reasonable hand-object manipulation sequences that perform like human beings",
                "to classify objects into categories",
                "to track hand movements in real-time"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Frame-Event Alignment and Fusion Network for High Frame Rate Tracking\nJiqing Zhang1, Yuanchen Wang1, Wenxi Liu2, Meng Li3, Jinpeng Bai1, Baocai Yin1, Xin Yang1,⋆\n1Dalian University of Technology, 2Fuzhou University, 3HiSilicon(Shanghai) Technologies Co.,Ltd\nAbstract\nMost existing RGB-based trackers target low frame rate\nbenchmarks of around 30 frames per second. This setting\nrestricts the tracker’s functionality in the real world, espe-\ncially for fast motion. Event-based cameras as bioinspired\nsensors provide considerable potential for high frame rate\ntracking due to their high temporal resolution. However,\nevent-based cameras cannot offer fine-grained texture in-\nformation like conventional cameras. This unique comple-\nmentarity motivates us to combine conventional frames and\nevents for high frame rate object tracking under various\nchallenging conditions. In this paper, we propose an end-to-\nend network consisting of multi-modality alignment and fu-\nsion modules to effectively combine meaningful information\nfrom both modalities at different measurement rates. The\nalignment module is responsible for cross-style and cross-\nframe-rate alignment between frame and event modalities\nunder the guidance of the moving cues furnished by events.\nWhile the fusion module is accountable for emphasizing\nvaluable features and suppressing noise information by the\nmutual complement between the two modalities.\nExten-\nsive experiments show that the proposed approach outper-\nforms state-of-the-art trackers by a significant margin in\nhigh frame rate tracking. With the FE240hz dataset, our\napproach achieves high frame rate tracking up to 240Hz.\n",
        "question": {
            "statement": "What advantage do event-based cameras have over traditional cameras when it comes to tracking fast motion?",
            "options": [
                "Higher temporal resolution",
                "Lower power consumption",
                "Better image quality",
                "Increased storage capacity"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "MISC210K: A Large-Scale Dataset for Multi-Instance Semantic Correspondence\nYixuan Sun1,∗, Yiwen Huang2,∗, Haijing Guo2, Yuzhou Zhao2, Runmin Wu3,\nYizhou Yu3, Weifeng Ge2,†, Wenqiang Zhang1,2,†\n1Academy of Engineering & Technology, Fudan University, Shanghai, China\n2School of Computer Science, Fudan University, Shanghai, China\n3The University of Hong Kong, Hong Kong, China\n{wfge, wqzhang}@fudan.edu.cn\nAbstract\nSemantic correspondence have built up a new way for\nobject recognition. However current single-object match-\ning schema can be hard for discovering commonalities for\na category and far from the real-world recognition tasks. To\nfill this gap, we design the multi-instance semantic corre-\nspondence task which aims at constructing the correspon-\ndence between multiple objects in an image pair. To sup-\nport this task, we build a multi-instance semantic corre-\nspondence (MISC) dataset from COCO Detection 2017 task\ncalled MISC210K. We construct our dataset as three steps:\n(1) category selection and data cleaning; (2) keypoint de-\nsign based on 3D models and object description rules; (3)\nhuman-machine collaborative annotation. Following these\nsteps, we select 34 classes of objects with 4,812 challenging\nimages annotated via a well designed semi-automatic work-\nflow, and finally acquire 218,179 image pairs with instance\nmasks and instance-level keypoint pairs annotated. We de-\nsign a dual-path collaborative learning pipeline to train\ninstance-level co-segmentation task and fine-grained level\ncorrespondence task together. Benchmark evaluation and\nfurther ablation results with detailed analysis are provided\nwith three future directions proposed. Our project is avail-\nable on https://github.com/YXSUNMADMAX/MISC210K.\n",
        "question": {
            "statement": "What is the primary goal of the multi-instance semantic correspondence task?",
            "options": [
                "To establish correspondences between multiple objects within an image pair",
                "To recognize a single object within an image",
                "To classify images into predefined categories",
                "To detect keypoints on a single object"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "3D Registration with Maximal Cliques\nXiyu Zhang\nJiaqi Yang*\nShikun Zhang\nYanning Zhang\nSchool of Computer Science, Northwestern Polytechnical University, China\n{2426988253, zhangshikun}@mail.nwpu.edu.cn; {jqyang, ynzhang}@nwpu.edu.cn\nAbstract\nAs a fundamental problem in computer vision, 3D point\ncloud registration (PCR) aims to seek the optimal pose to\nalign a point cloud pair. In this paper, we present a 3D reg-\nistration method with maximal cliques (MAC). The key in-\nsight is to loosen the previous maximum clique constraint,\nand mine more local consensus information in a graph for\naccurate pose hypotheses generation: 1) A compatibility\ngraph is constructed to render the affinity relationship be-\ntween initial correspondences. 2) We search for maximal\ncliques in the graph, each of which represents a consensus\nset. We perform node-guided clique selection then, where\neach node corresponds to the maximal clique with the great-\nest graph weight. 3) Transformation hypotheses are com-\nputed for the selected cliques by the SVD algorithm and\nthe best hypothesis is used to perform registration.\nEx-\ntensive experiments on U3M, 3DMatch, 3DLoMatch and\nKITTI demonstrate that MAC effectively increases registra-\ntion accuracy, outperforms various state-of-the-art meth-\nods and boosts the performance of deep-learned methods.\nMAC combined with deep-learned methods achieves state-\nof-the-art registration recall of 95.7% / 78.9% on 3DMatch\n/ 3DLoMatch.\n",
        "question": {
            "statement": "What is the primary goal of 3D point cloud registration?",
            "options": [
                "To identify individual points within a point cloud",
                "To visualize a point cloud in 3D space",
                "To find the optimal pose to align a point cloud pair",
                "To reduce the number of points in a point cloud"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Learning Discriminative Representations for Skeleton Based Action Recognition\nHuanyu Zhou1, Qingjie Liu1,2,3,∗, Yunhong Wang1\n1State Key Laboratory of Virtual Reality Technology and Systems, Beihang University, Beijing, China\n2Zhongguancun Laboratory, 3Hangzhou Innovation Institute of Beihang University\n{zhysora, qingjie.liu, yhwang}@buaa.edu.cn\nAbstract\nHuman action recognition aims at classifying the cate-\ngory of human action from a segment of a video. Recently,\npeople have dived into designing GCN-based models to ex-\ntract features from skeletons for performing this task, be-\ncause skeleton representations are much more efﬁcient and\nrobust than other modalities such as RGB frames. However,\nwhen employing the skeleton data, some important clues\nlike related items are also discarded.\nIt results in some\nambiguous actions that are hard to be distinguished and\ntend to be misclassiﬁed. To alleviate this problem, we pro-\npose an auxiliary feature reﬁnement head (FR Head), which\nconsists of spatial-temporal decoupling and contrastive fea-\nture reﬁnement, to obtain discriminative representations of\nskeletons. Ambiguous samples are dynamically discovered\nand calibrated in the feature space. Furthermore, FR Head\ncould be imposed on different stages of GCNs to build a\nmulti-level reﬁnement for stronger supervision. Extensive\nexperiments are conducted on NTU RGB+D, NTU RGB+D\n120, and NW-UCLA datasets. Our proposed models obtain\ncompetitive results from state-of-the-art methods and can\nhelp to discriminate those ambiguous samples. Codes are\navailable at https://github.com/zhysora/FR-Head.\n",
        "question": {
            "statement": "What is the main advantage of using skeleton representations over other modalities such as RGB frames in human action recognition?",
            "options": [
                "They provide more detailed information about the environment",
                "They are more efficient and robust",
                "They are easier to collect and process",
                "They are less prone to noise and occlusion"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "5"
            ]
        },
        "difference": 5,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "TOPLight: Lightweight Neural Networks with Task-Oriented Pretraining for\nVisible-Infrared Recognition\nHao Yu1,\nXu Cheng1*\n,\nWei Peng2\n1School of Computer Science, Nanjing University of Information Science and Technology, China\n2Department of Psychiatry and Behavioral Sciences, Stanford University\n{yuhao,xcheng}@nuist.edu.cn, wepeng@stanford.edu\nAbstract\nVisible-infrared recognition (VI recognition) is a chal-\nlenging task due to the enormous visual difference across\nheterogeneous images. Most existing works achieve promis-\ning results by transfer learning, such as pretraining on\nthe ImageNet, based on advanced neural architectures like\nResNet and ViT. However, such methods ignore the neg-\native influence of the pretrained colour prior knowledge,\nas well as their heavy computational burden makes them\nhard to deploy in actual scenarios with limited resources.\nIn this paper, we propose a novel task-oriented pretrained\nlightweight neural network (TOPLight) for VI recognition.\nSpecifically, the TOPLight method simulates the domain\nconflict and sample variations with the proposed fake do-\nmain loss in the pretraining stage, which guides the network\nto learn how to handle those difficulties, such that a more\ngeneral modality-shared feature representation is learned\nfor the heterogeneous images. Moreover, an effective fine-\ngrained dependency reconstruction module (FDR) is devel-\noped to discover substantial pattern dependencies shared\nin two modalities. Extensive experiments on VI person re-\nidentification and VI face recognition datasets demonstrate\nthe superiority of the proposed TOPLight, which signifi-\ncantly outperforms the current state of the arts while de-\nmanding fewer computational resources.\n",
        "question": {
            "statement": "What is a major limitation of existing approaches to visible-infrared recognition?",
            "options": [
                "The lack of large-scale datasets",
                "The difficulty in collecting infrared data",
                "The inability to generalize to new domains",
                "Their heavy computational requirements"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "3",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Affordance Grounding from Demonstration Video to Target Image\nJoya Chen\nDifei Gao\nKevin Qinghong Lin\nMike Zheng Shou†\nShow Lab, National University of Singapore\n{joyachen,qinghonglin}@u.nus.edu\n{daniel.difei.gao,mike.zheng.shou}@gmail.com\nAbstract\nHumans excel at learning from expert demonstrations\nand solving their own problems. To equip intelligent robots\nand assistants, such as AR glasses, with this ability, it is\nessential to ground human hand interactions (i.e., affor-\ndances) from demonstration videos and apply them to a\ntarget image like a user’s AR glass view. This video-to-\nimage affordance grounding task is challenging due to (1)\nthe need to predict fine-grained affordances, and (2) the lim-\nited training data, which inadequately covers video-image\ndiscrepancies and negatively impacts grounding. To tackle\nthem, we propose Affordance Transformer (Afformer),\nwhich has a fine-grained transformer-based decoder that\ngradually refines affordance grounding.\nMoreover, we\nintroduce Mask Affordance Hand (MaskAHand), a self-\nsupervised pre-training technique for synthesizing video-\nimage data and simulating context changes, enhancing af-\nfordance grounding across video-image discrepancies. Af-\nformer with MaskAHand pre-training achieves state-of-the-\nart performance on multiple benchmarks, including a sub-\nstantial 37% improvement on the OPRA dataset. Code is\nmade available at https://github.com/showlab/afformer.\n",
        "question": {
            "statement": "What is a key challenge in grounding human hand interactions from demonstration videos and applying them to target images?",
            "options": [
                "Limited training data that does not adequately cover video-image discrepancies",
                "Complexity of human hand anatomy",
                "Lack of expert demonstrators",
                "Insufficient computational power"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Therbligs in Action: Video Understanding through Motion Primitives\nEadom Dessalene, Michael Maynord, Cornelia Ferm¨\nuller, Yiannis Aloimonos\nUniversity of Maryland, College Park\nCollege Park, MD 20742, USA\n{edessale,maynord,fermulcm,jyaloimo@umd.edu}\nAbstract\nIn this paper we introduce a rule-based, compositional,\nand hierarchical modeling of action using Therbligs as our\natoms. Introducing these atoms provides us with a con-\nsistent, expressive, contact-centered representation of ac-\ntion. Over the atoms we introduce a differentiable method of\nrule-based reasoning to regularize for logical consistency.\nOur approach is complementary to other approaches in that\nthe Therblig-based representations produced by our archi-\ntecture augment rather than replace existing architectures’\nrepresentations. We release the first Therblig-centered an-\nnotations over two popular video datasets - EPIC Kitchens\n100 and 50-Salads.\nWe also broadly demonstrate bene-\nfits to adopting Therblig representations through evalua-\ntion on the following tasks: action segmentation, action\nanticipation, and action recognition - observing an aver-\nage 10.5%/7.53%/6.5% relative improvement, respectively,\nover EPIC Kitchens and an average 8.9%/6.63%/4.8% rel-\native improvement, respectively, over 50 Salads. Code and\ndata will be made publicly available.\n",
        "question": {
            "statement": "What is the main advantage of using Therblig-based representations in action understanding?",
            "options": [
                "They are only applicable to specific video datasets",
                "They provide a consistent and expressive representation of action",
                "They require manual annotation of videos",
                "They are limited to action recognition tasks"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "8",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "ARKitTrack: A New Diverse Dataset for Tracking Using Mobile RGB-D Data\nHaojie Zhao1†\nJunsong Chen1†\nLijun Wang1*\nHuchuan Lu1,2\n1Dalian University of Technology, China\n2Peng Cheng Laboratory, China\n{haojie zhao,jschen}@mail.dlut.edu.cn\n{ljwang,lhchuan}@dlut.edu.cn\nFigure 1. Samples from ARKitTrack. We capture both indoor and outdoor sequences (1st row) in many scenes, including zoo, market,\noffice, square, corridor, etc. Lots of scenarios are presented in our dataset, e.g., low or high light conditions (2nd row), surrounding clutter\n(3rd row), out-of-plane rotation, motion blur, deformation, etc. (4th row). Besides, we annotate each frame with object masks.\nAbstract\nCompared with traditional RGB-only visual tracking,\nfew datasets have been constructed for RGB-D tracking. In\nthis paper, we propose ARKitTrack, a new RGB-D track-\ning dataset for both static and dynamic scenes captured\nby consumer-grade LiDAR scanners equipped on Apple’s\niPhone and iPad.\nARKitTrack contains 300 RGB-D se-\nquences, 455 targets, and 229.7K video frames in total.\nAlong with the bounding box annotations and frame-level\nattributes, we also annotate this dataset with 123.9K pixel-\nlevel target masks. Besides, the camera intrinsic and cam-\nera pose of each frame are provided for future develop-\nments.\nTo demonstrate the potential usefulness of this\ndataset, we further present a unified baseline for both box-\nlevel and pixel-level tracking, which integrates RGB fea-\ntures with bird’s-eye-view representations to better explore\ncross-modality 3D geometry.\nIn-depth empirical analy-\n†Equal contribution\n*Corresponding author: Dr. Lijun Wang, ljwang@dlut.edu.cn\nsis has verified that the ARKitTrack dataset can signif-\nicantly facilitate RGB-D tracking and that the proposed\nbaseline method compares favorably against the state of\nthe arts. The code and dataset is available at https:\n//arkittrack.github.io.\n",
        "question": {
            "statement": "What type of data does the ARKitTrack dataset contain?",
            "options": [
                "LiDAR-only",
                "RGB-only",
                "RGB-D",
                "Audio"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "GLIGEN: Open-Set Grounded Text-to-Image Generation\nYuheng Li1§, Haotian Liu1§, Qingyang Wu2, Fangzhou Mu1, Jianwei Yang3, Jianfeng Gao3,\nChunyuan Li3¶, Yong Jae Lee1¶\n1University of Wisconsin-Madison\n2Columbia University\n3Microsoft\nhttps://gligen.github.io/\nCaption: “A woman sitting in a restaurant with a pizza in front of her ”\nGrounded text: table, pizza, person, wall, car, paper, chair, window, bottle, cup\nCaption: “a baby girl / monkey / Hormer Simpson / is scratching her/its head”\nGrounded keypoints: plotted dots on the left image \nCaption: “A dog / bird / helmet / backpack is on the grass”\nGrounded image: red inset\nCaption: “Elon Musk and Emma Watson on a movie poster”\nGrounded text: Elon Musk, Emma Watson; Grounded style image: blue inset\nCaption: “A vibrant colorful bird sitting on tree branch”\nGrounded depth map: the left image\nCaption: “A young boy with white powder on his face looks away”\nGrounded HED map: the left image\nCaption: “Cars park on the snowy street”\nGrounded normal map: the left image\nCaption: “A living room filled with lots of furniture and plants”\nGrounded semantic map: the left image\n(a)\n(c)\n(b)\n(e)\n(g)\n(d)\n(f)\n(h)\nFigure 1. GLIGEN enables versatile grounding capabilities for a frozen text-to-image generation model, by feeding different grounding\nconditions. GLIGEN supports (a) text entity + box, (b) image entity + box, (c) image style and text + box, (d) keypoints, (e) depth map, (f)\nedge map, (g) normal map, and (h) semantic map.\nAbstract\nLarge-scale text-to-image diffusion models have made\namazing advances.\nHowever, the status quo is to use\ntext input alone, which can impede controllability. In this\nwork, we propose GLIGEN, Grounded-Language-to-Image\nGeneration, a novel approach that builds upon and extends\nthe functionality of existing pre-trained text-to-image dif-\nfusion models by enabling them to also be conditioned on\ngrounding inputs. To preserve the vast concept knowledge of\nthe pre-trained model, we freeze all of its weights and inject\nthe grounding information into new trainable layers via a\ngated mechanism. Our model achieves open-world grounded\ntext2img generation with caption and bounding box condi-\ntion inputs, and the grounding ability generalizes well to\nnovel spatial configurations and concepts. GLIGEN’s zero-\nshot performance on COCO and LVIS outperforms existing\nsupervised layout-to-image baselines by a large margin.\n§ Part of the work performed at Microsoft; ¶ Co-senior authors\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22511\n",
        "question": {
            "statement": "What is the main advantage of using a grounded-language-to-image generation model compared to traditional text-to-image diffusion models?",
            "options": [
                "It requires less computational resources",
                "It generates higher quality images",
                "It allows for more controllable image generation",
                "It is only suitable for generating images of specific objects"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Chat2Map: Efficient Scene Mapping from Multi-Ego Conversations\nSagnik Majumder1,2,3\nHao Jiang2\nPierre Moulon2\nEthan Henderson2\nPaul Calamia2\nKristen Grauman1,3*\nVamsi Krishna Ithapu2∗\n1UT Austin\n2Reality Labs Research, Meta\n3FAIR\nAbstract\nCan conversational videos captured from multiple egocen-\ntric viewpoints reveal the map of a scene in a cost-efficient\nway? We seek to answer this question by proposing a new\nproblem: efficiently building the map of a previously un-\nseen 3D environment by exploiting shared information in\nthe egocentric audio-visual observations of participants in\na natural conversation. Our hypothesis is that as multi-\nple people (“egos\") move in a scene and talk among them-\nselves, they receive rich audio-visual cues that can help\nuncover the unseen areas of the scene. Given the high cost\nof continuously processing egocentric visual streams, we\nfurther explore how to actively coordinate the sampling of\nvisual information, so as to minimize redundancy and re-\nduce power use. To that end, we present an audio-visual\ndeep reinforcement learning approach that works with our\nshared scene mapper to selectively turn on the camera to ef-\nficiently chart out the space. We evaluate the approach using\na state-of-the-art audio-visual simulator for 3D scenes as\nwell as real-world video. Our model outperforms previous\nstate-of-the-art mapping methods, and achieves an excellent\ncost-accuracy tradeoff. Project: http://vision.cs.\nutexas.edu/projects/chat2map.\n",
        "question": {
            "statement": "What is the main goal of the Chat2Map project?",
            "options": [
                "To design a state-of-the-art audio-visual simulator for 3D scenes",
                "To develop a new audio-visual deep reinforcement learning approach",
                "To analyze the effectiveness of egocentric visual streams in conversational videos",
                "To efficiently build a map of a previously unseen 3D environment using multi-ego conversations"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "2",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Distilling Self-Supervised Vision Transformers\nfor Weakly-Supervised Few-Shot Classification & Segmentation\nDahyun Kang1,2*\nPiotr Koniusz3,4\nMinsu Cho2\nNaila Murray1\n1Meta AI\n2POSTECH\n3Data61rCSIRO\n4Australian National University\nAbstract\nWe address the task of weakly-supervised few-shot im-\nage classification and segmentation, by leveraging a Vision\nTransformer (ViT) pretrained with self-supervision.\nOur\nproposed method takes token representations from the self-\nsupervised ViT and leverages their correlations, via self-\nattention, to produce classification and segmentation pre-\ndictions through separate task heads. Our model is able\nto effectively learn to perform classification and segmen-\ntation in the absence of pixel-level labels during train-\ning, using only image-level labels. To do this it uses at-\ntention maps, created from tokens generated by the self-\nsupervised ViT backbone, as pixel-level pseudo-labels. We\nalso explore a practical setup with “mixed” supervision,\nwhere a small number of training images contains ground-\ntruth pixel-level labels and the remaining images have only\nimage-level labels. For this mixed setup, we propose to im-\nprove the pseudo-labels using a pseudo-label enhancer that\nwas trained using the available ground-truth pixel-level la-\nbels. Experiments on Pascal-5i and COCO-20i demonstrate\nsignificant performance gains in a variety of supervision\nsettings, and in particular when little-to-no pixel-level la-\nbels are available.\n",
        "question": {
            "statement": "What type of transformer is used in the proposed method for weakly-supervised few-shot image classification and segmentation?",
            "options": [
                "Text Transformer",
                "Language Transformer",
                "Vision Transformer",
                "Audio Transformer"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Geometry and Uncertainty-Aware 3D Point Cloud Class-Incremental Semantic\nSegmentation\nYuwei Yang1\nMunawar Hayat2\nZhao Jin1\nChao Ren1\nYinjie Lei1,\n1Sichuan University\n2Monash University\nyuwei@stu.scu.edu.cn\nmunawar.hayat@monash.edu\njinzhao@stu.scu.edu.cn\nchaoren@scu.edu.cn\nyinjie@scu.edu.cn\nAbstract\nDespite the signiﬁcant recent progress made on 3D\npoint cloud semantic segmentation, the current methods re-\nquire training data for all classes at once, and are not suit-\nable for real-life scenarios where new categories are be-\ning continuously discovered. Substantial memory storage\nand expensive re-training is required to update the model\nto sequentially arriving data for new concepts.\nIn this\npaper, to continually learn new categories using previous\nknowledge, we introduce class-incremental semantic seg-\nmentation of 3D point cloud. Unlike 2D images, 3D point\nclouds are disordered and unstructured, making it difﬁcult\nto store and transfer knowledge especially when the pre-\nvious data is not available. We further face the challenge\nof semantic shift, where previous/future classes are indis-\ncriminately collapsed and treated as the background in the\ncurrent step, causing a dramatic performance drop on past\nclasses. We exploit the structure of point cloud and pro-\npose two strategies to address these challenges. First, we\ndesign a geometry-aware distillation module that transfers\npoint-wise feature associations in terms of their geomet-\nric characteristics.\nTo counter forgetting caused by the\nsemantic shift, we further develop an uncertainty-aware\npseudo-labelling scheme that eliminates noise in uncertain\npseudo-labels by label propagation within a local neighbor-\nhood. Our extensive experiments on S3DIS and ScanNet in\na class-incremental setting show impressive results compa-\nrable to the joint training strategy (upper bound). Code is\navailable at: https://github.com/leolyj/3DPC-CISS\n",
        "question": {
            "statement": "What is a major challenge in incremental learning of 3D point cloud semantic segmentation?",
            "options": [
                "Limited availability of labeled data for all classes",
                "Insufficient training data for new classes",
                "High computational cost of retraining the model",
                "Forgetting of previously learned classes due to semantic shift"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "7",
                "10"
            ]
        },
        "difference": 3,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation\nUsing Scene Object Spectrum Grounding\nMinyoung Hwang1, Jaeyeon Jeong1, Minsoo Kim3, Yoonseon Oh2, Songhwai Oh1\n1Electrical and Computer Engineering and ASRI, Seoul National University\n2Department of Electronic Engineering, Hanyang University\n3Interdisciplinary Major in Artiﬁcial Intelligence, Seoul National University\n{minyoung.hwang, jaeyeon.jeong}@rllab.snu.ac.kr, goldbird5@snu.ac.kr, yoh21@hanyang.ac.kr, songhwai@snu.ac.kr\nAbstract\nThe main challenge in vision-and-language navigation\n(VLN) is how to understand natural-language instructions\nin an unseen environment. The main limitation of conven-\ntional VLN algorithms is that if an action is mistaken, the\nagent fails to follow the instructions or explores unneces-\nsary regions, leading the agent to an irrecoverable path. To\ntackle this problem, we propose Meta-Explore, a hierarchi-\ncal navigation method deploying an exploitation policy to\ncorrect misled recent actions. We show that an exploitation\npolicy, which moves the agent toward a well-chosen local\ngoal among unvisited but observable states, outperforms a\nmethod which moves the agent to a previously visited state.\nWe also highlight the demand for imagining regretful explo-\nrations with semantically meaningful clues. The key to our\napproach is understanding the object placements around the\nagent in spectral-domain. Speciﬁcally, we present a novel\nvisual representation, called scene object spectrum (SOS),\nwhich performs category-wise 2D Fourier transform of de-\ntected objects. Combining exploitation policy and SOS fea-\ntures, the agent can correct its path by choosing a promis-\ning local goal. We evaluate our method in three VLN bench-\nmarks: R2R, SOON, and REVERIE. Meta-Explore outper-\nforms other baselines and shows signiﬁcant generalization\nperformance. In addition, local goal search using the pro-\nposed spectral-domain SOS features signiﬁcantly improves\nthesuccessrateby17.1%andSPLby20.6%againstthestate-\nof-the-art method of the SOON benchmark. Project page:\nhttps://rllab-snu.github.io/projects/Meta-Explore/doc.html\n",
        "question": {
            "statement": "What is the main challenge in vision-and-language navigation?",
            "options": [
                "finding the shortest path to the target location",
                "recognizing objects in the visual scene",
                "generating fluent language descriptions of the scene",
                "understanding natural-language instructions in an unseen environment"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Finding Geometric Models by Clustering in the Consensus Space\nDaniel Barath1, Denys Rozumnyi2,1, Ivan Eichhardt3,4, Levente Hajder3, Jiri Matas2\n1Computer Vision and Geometry Group, ETH Zurich, Switzerland,\n2VRG, Faculty of Electrical Engineering, CTU in Prague, Czech Republic,\n3TMEIC Corporation Americas, Roanoke, VA, USA\n4E¨\notv¨\nos Lor´\nand University, Budapest, Hungary\nAbstract\nWe propose a new algorithm for finding an unknown\nnumber of geometric models, e.g., homographies.\nThe\nproblem is formalized as finding dominant model instances\nprogressively without forming crisp point-to-model assign-\nments. Dominant instances are found via a RANSAC-like\nsampling and a consolidation process driven by a model\nquality function considering previously proposed instances.\nNew ones are found by clustering in the consensus space.\nThis new formulation leads to a simple iterative algorithm\nwith state-of-the-art accuracy while running in real-time\non a number of vision problems – at least two orders of\nmagnitude faster than the competitors on two-view motion\nestimation. Also, we propose a deterministic sampler re-\nflecting the fact that real-world data tend to form spatially\ncoherent structures. The sampler returns connected com-\nponents in a progressively densified neighborhood-graph.\nWe present a number of applications where the use of mul-\ntiple geometric models improves accuracy. These include\npose estimation from multiple generalized homographies;\ntrajectory estimation of fast-moving objects; and we also\npropose a way of using multiple homographies in global\nSfM algorithms. Source code: https://github.com/\ndanini/clustering-in-consensus-space.\n",
        "question": {
            "statement": "What approach is used to find new geometric model instances in the proposed algorithm?",
            "options": [
                "forming crisp point-to-model assignments",
                "clustering in the consensus space",
                "iterative sampling and consolidation",
                "using a single model quality function"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Demystifying Causal Features on Adversarial Examples and Causal Inoculation\nfor Robust Network by Adversarial Instrumental Variable Regression\nJunho Kim*\n, Byung-Kwan Lee*\n, Yong Man Ro†\nImage and Video Systems Lab, School of Electrical Engineering, KAIST, South Korea\n{arkimjh, leebk, ymro}@kaist.ac.kr\nAbstract\nThe origin of adversarial examples is still inexplicable in\nresearch fields, and it arouses arguments from various view-\npoints, albeit comprehensive investigations. In this paper,\nwe propose a way of delving into the unexpected vulnera-\nbility in adversarially trained networks from a causal per-\nspective, namely adversarial instrumental variable (IV) re-\ngression. By deploying it, we estimate the causal relation of\nadversarial prediction under an unbiased environment dis-\nsociated from unknown confounders. Our approach aims\nto demystify inherent causal features on adversarial exam-\nples by leveraging a zero-sum optimization game between a\ncasual feature estimator (i.e., hypothesis model) and worst-\ncase counterfactuals (i.e., test function) disturbing to find\ncausal features. Through extensive analyses, we demon-\nstrate that the estimated causal features are highly related\nto the correct prediction for adversarial robustness, and the\ncounterfactuals exhibit extreme features significantly devi-\nating from the correct prediction. In addition, we present\nhow to effectively inoculate CAusal FEatures (CAFE) into\ndefense networks for improving adversarial robustness.\n",
        "question": {
            "statement": "What is the primary goal of using adversarial instrumental variable regression in machine learning?",
            "options": [
                "To improve the accuracy of defense networks against attacks",
                "To reduce the computational cost of training models",
                "To generate more realistic adversarial examples",
                "To identify the underlying causal relationships between features and predictions"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "4",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 6,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Dual Alignment Unsupervised Domain Adaptation for Video-Text Retrieval\nXiaoshuai Hao1,2, Wanqian Zhang1*\n, Dayan Wu1, Fei Zhu1,2, Bo Li1,2\n1Institute of Information Engineering, Chinese Academy of Sciences\n2School of Cyber Security, University of Chinese Academy of Sciences\n{haoxiaoshuai,zhangwanqian,wudayan,zhufei,libo}@iie.ac.cn\nAbstract\nVideo-text retrieval is an emerging stream in both com-\nputer vision and natural language processing communi-\nties, which aims to find relevant videos given text queries.\nIn this paper, we study the notoriously challenging task,\ni.e., Unsupervised Domain Adaptation Video-text Retrieval\n(UDAVR), wherein training and testing data come from dif-\nferent distributions.\nPrevious works merely alleviate the\ndomain shift, which however overlook the pairwise mis-\nalignment issue in target domain, i.e., there exist no se-\nmantic relationships between target videos and texts. To\ntackle this, we propose a novel method named Dual Align-\nment Domain Adaptation (DADA). Specifically, we first in-\ntroduce the cross-modal semantic embedding to generate\ndiscriminative source features in a joint embedding space.\nBesides, we utilize the video and text domain adaptations\nto smoothly balance the minimization of the domain shifts.\nTo tackle the pairwise misalignment in target domain, we\npropose the Dual Alignment Consistency (DAC) to fully ex-\nploit the semantic information of both modalities in target\ndomain. The proposed DAC adaptively aligns the video-\ntext pairs which are more likely to be relevant in target do-\nmain, enabling that positive pairs are increasing progres-\nsively and the noisy ones will potentially be aligned in the\nlater stages. To that end, our method can generate more\ntruly aligned target pairs and ensure the discriminability of\ntarget features. Compared with the state-of-the-art meth-\nods, DADA achieves 20.18% and 18.61% relative improve-\nments on R@1 under the setting of TGIF→MSR-VTT and\nTGIF→MSVD respectively, demonstrating the superiority\nof our method.\n",
        "question": {
            "statement": "What is the main challenge addressed by the Dual Alignment Domain Adaptation (DADA) method in Video-Text Retrieval?",
            "options": [
                "The lack of semantic relationships between target videos and texts",
                "The limited availability of training data",
                "The high computational cost of feature extraction",
                "The difference in video and text formats"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "SMOC-Net: Leveraging Camera Pose for Self-Supervised Monocular Object\nPose Estimation\nTao Tan1,2, Qiulei Dong1,2,3\n1School of Artificial Intelligence, UCAS\n2State Key Laboratory of Multimodal Artificial Intelligence Systems, CASIA\n3Center for Excellence in Brain Science and Intelligence Technology, CAS\ntantao2022@ia.ac.cn,qldong@nlpr.ia.ac.cn,Corresponding author:Qiulei Dong\nAbstract\nRecently, self-supervised 6D object pose estimation,\nwhere synthetic images with object poses (sometimes jointly\nwith un-annotated real images) are used for training, has\nattracted much attention in computer vision. Some typical\nworks in literature employ a time-consuming differentiable\nrenderer for object pose prediction at the training stage, so\nthat (i) their performances on real images are generally lim-\nited due to the gap between their rendered images and real\nimages and (ii) their training process is computationally ex-\npensive. To address the two problems, we propose a novel\nNetwork for Self-supervised Monocular Object pose esti-\nmation by utilizing the predicted Camera poses from un-\nannotated real images, called SMOC-Net. The proposed\nnetwork is explored under a knowledge distillation frame-\nwork, consisting of a teacher model and a student model.\nThe teacher model contains a backbone estimation module\nfor initial object pose estimation, and an object pose refiner\nfor refining the initial object poses using a geometric con-\nstraint (called relative-pose constraint) derived from rela-\ntive camera poses. The student model gains knowledge for\nobject pose estimation from the teacher model by impos-\ning the relative-pose constraint. Thanks to the relative-pose\nconstraint, SMOC-Net could not only narrow the domain\ngap between synthetic and real data but also reduce the\ntraining cost. Experimental results on two public datasets\ndemonstrate that SMOC-Net outperforms several state-of-\nthe-art methods by a large margin while requiring much less\ntraining time than the differentiable-renderer-based meth-\nods.\n",
        "question": {
            "statement": "What is a major limitation of some existing approaches to self-supervised 6D object pose estimation?",
            "options": [
                "Limited computational resources",
                "The gap between rendered images and real images",
                "Insufficient training data",
                "Inability to handle varied lighting conditions"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Alias-Free Convnets: Fractional Shift Invariance via Polynomial Activations\nHagay Michaeli\nTomer Michaeli\nDaniel Soudry\nDepartment of Electrical and Computer Engineering, Technion\nHaifa, Israel\n{hagaymichaeli, daniel.soudry}@gmail.com, tomer.m@ee.technion.ac.il\nAbstract\nAlthough CNNs are believed to be invariant to transla-\ntions, recent works have shown this is not the case due to\naliasing effects that stem from down-sampling layers. The\nexisting architectural solutions to prevent the aliasing ef-\nfects are partial since they do not solve those effects that\noriginate in non-linearities. We propose an extended anti-\naliasing method that tackles both down-sampling and non-\nlinear layers, thus creating truly alias-free, shift-invariant\nCNNs1. We show that the presented model is invariant to\ninteger as well as fractional (i.e., sub-pixel) translations,\nthus outperforming other shift-invariant methods in terms\nof robustness to adversarial translations.\n",
        "question": {
            "statement": "What is a limitation of existing architectural solutions for preventing aliasing effects in Convolutional Neural Networks (CNNs)?",
            "options": [
                "They are incompatible with common deep learning frameworks",
                "They are only applicable to integer translations",
                "They do not address aliasing effects stemming from non-linearities",
                "They require significant computational resources"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Recognizability Embedding Enhancement for Very Low-Resolution Face\nRecognition and Quality Estimation\nJacky Chen Long Chai1 Tiong-Sik Ng1 Cheng-Yaw Low2 Jaewoo Park1 Andrew Beng Jin Teoh1\n1Yonsei University 2Institute for Basic Science\n1{jackyccl,ngtiongsik,julypraise,bjteoh}@yonsei.ac.kr\n2{chengyawlow}@ibs.re.kr\nFigure 1. A deep face model pretrained on high-resolution face images introduces a cluster of unrecognizable instances (grey spike),\ndubbed unrecognizable identities (UIs) in [9]. (a) shows the bimodal distribution for a very low-resolution face dataset [6] based on distance\nagainst the UIs. Interestingly, a portion of hard-to-recognize faces (red peak) lie close to the UIs, indicating their low recognizability. (b)\nWe propose to improve the recognizability of hard-to-recognize instances by pushing them away from the UIs center. Consequently,\nfaces with higher recognizability indexes are further apart from UIs center in the embedding space. Our method not only induces more\ndiscriminative representations but also translates face quality into a measurable indicator that closely matches human cognition.\nAbstract\nVery low-resolution face recognition (VLRFR) poses\nunique challenges, such as tiny regions of interest and poor\nresolution due to extreme standoff distance or wide viewing\nangle of the acquisition devices. In this paper, we study\nprincipled approaches to elevate the recognizability of a\nface in the embedding space instead of the visual quality.\nWe first formulate a robust learning-based face recogniz-\nability measure, namely recognizability index (RI), based on\ntwo criteria: (i) proximity of each face embedding against\nthe unrecognizable faces cluster center and (ii) closeness\nof each face embedding against its positive and negative\nclass prototypes. We then devise an index diversion loss\nto push the hard-to-recognize face embedding with low RI\naway from unrecognizable faces cluster to boost the RI,\nwhich reflects better recognizability. Additionally, a percep-\ntibility attention mechanism is introduced to attend to the\nmost recognizable face regions, which offers better explana-\ntory and discriminative traits for embedding learning. Our\nproposed model is trained end-to-end and simultaneously\nserves recognizability-aware embedding learning and face\nquality estimation. To address VLRFR, our extensive eval-\nuations on three challenging low-resolution datasets and\nface quality assessment demonstrate the superiority of the\nproposed model over the state-of-the-art methods.\n",
        "question": {
            "statement": "What is the main goal of the proposed model in very low-resolution face recognition?",
            "options": [
                "To decrease the importance of face quality estimation",
                "To reduce the number of unrecognizable face instances",
                "To increase the recognizability of hard-to-recognize face instances",
                "To improve the visual quality of low-resolution face images"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Local Connectivity-Based Density Estimation for Face Clustering\nJunho Shin, Hyo-Jun Lee, Hyunseop Kim, Jong-Hyeon Baek, Daehyun Kim, Yeong Jun Koh∗\nChungnam National University\n{wnsgh8190, gywns6287, hyunseop95, whdgusdl97, seven776484}@gmail.com, yjkoh@cnu.ac.kr\nAbstract\nRecent graph-based face clustering methods predict the\nconnectivity of enormous edges, including false positive\nedges that link nodes with different classes. However, those\nfalse positive edges, which connect negative node pairs,\nhave the risk of integration of different clusters when their\nconnectivity is incorrectly estimated. This paper proposes a\nnovel face clustering method to address this problem. The\nproposed clustering method employs density-based cluster-\ning, which maintains edges that have higher density. For\nthis purpose, we propose a reliable density estimation algo-\nrithm based on local connectivity between K nearest neigh-\nbors (KNN). We effectively exclude negative pairs from the\nKNN graph based on the reliable density while maintaining\nsufficient positive pairs. Furthermore, we develop a pair-\nwise connectivity estimation network to predict the connec-\ntivity of the selected edges. Experimental results demon-\nstrate that the proposed clustering method significantly out-\nperforms the state-of-the-art clustering methods on large-\nscale face clustering datasets and fashion image clustering\ndatasets. Our code is available at https://github.\ncom/illian01/LCE-PCENet\n",
        "question": {
            "statement": "What is a common issue with graph-based face clustering methods?",
            "options": [
                "They require manual labeling of all face images",
                "They incorrectly estimate the connectivity of edges between nodes with different classes",
                "They are unable to handle large-scale face clustering datasets",
                "They are limited to clustering faces into only two categories"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Learning and Aggregating Lane Graphs for Urban Automated Driving\nMartin B¨\nuchner1* Jannik Z¨\nurn1∗\nIon-George Todoran2\nAbhinav Valada1\nWolfram Burgard3\n1University of Freiburg\n2Woven by Toyota\n3University of Technology Nuremberg\nAbstract\nLane graph estimation is an essential and highly challeng-\ning task in automated driving and HD map learning. Exist-\ning methods using either onboard or aerial imagery struggle\nwith complex lane topologies, out-of-distribution scenar-\nios, or significant occlusions in the image space. Moreover,\nmerging overlapping lane graphs to obtain consistent large-\nscale graphs remains difficult. To overcome these challenges,\nwe propose a novel bottom-up approach to lane graph esti-\nmation from aerial imagery that aggregates multiple over-\nlapping graphs into a single consistent graph. Due to its\nmodular design, our method allows us to address two com-\nplementary tasks: predicting ego-respective successor lane\ngraphs from arbitrary vehicle positions using a graph neural\nnetwork and aggregating these predictions into a consistent\nglobal lane graph. Extensive experiments on a large-scale\nlane graph dataset demonstrate that our approach yields\nhighly accurate lane graphs, even in regions with severe\nocclusions. The presented approach to graph aggregation\nproves to eliminate inconsistent predictions while increas-\ning the overall graph quality. We make our large-scale\nurban lane graph dataset and code publicly available at\nhttp://urbanlanegraph.cs.uni-freiburg.de.\n",
        "question": {
            "statement": "What is a major challenge in lane graph estimation for automated driving?",
            "options": [
                "High cost of sensor equipment",
                "Insufficient computing power",
                "Limited availability of aerial imagery",
                "Complex lane topologies"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "3D Video Loops from Asynchronous Input\nLi Ma1\nXiaoyu Li2\nJing Liao3\nPedro V. Sander1\n1The Hong Kong University of Science and Technology\n2Tencent AI Lab\n3City University of Hong Kong\n(a) Reconstructed 3D Video Representation\n(b) View and Time Control\n(c) Real Time Demo\nFigure 1. Given a set of asynchronous multi-view videos, we propose a pipeline to construct a novel 3D looping video representation\n(a), which consists of a static texture atlas, a dynamic texture atlas, and multiple tiles as the geometry proxy. The 3D video loops allow\nboth view and time control (b), and can be rendered in real time even on mobile devices (c). We strongly recommend readers refer to the\nsupplementary material for video results.\nAbstract\nLooping videos are short video clips that can be looped\nendlessly without visible seams or artifacts. They provide\na very attractive way to capture the dynamism of natu-\nral scenes. Existing methods have been mostly limited to\n2D representations. In this paper, we take a step forward\nand propose a practical solution that enables an immer-\nsive experience on dynamic 3D looping scenes. The key\nchallenge is to consider the per-view looping conditions\nfrom asynchronous input while maintaining view consis-\ntency for the 3D representation. We propose a novel sparse\n3D video representation, namely Multi-Tile Video (MTV),\nwhich not only provides a view-consistent prior, but also\ngreatly reduces memory usage, making the optimization of\na 4D volume tractable.\nThen, we introduce a two-stage\npipeline to construct the 3D looping MTV from completely\nasynchronous multi-view videos with no time overlap. A\nnovel looping loss based on video temporal retargeting al-\ngorithms is adopted during the optimization to loop the 3D\nscene. Experiments of our framework have shown promise\nin successfully generating and rendering photorealistic 3D\nlooping videos in real time even on mobile devices. The\ncode, dataset, and live demos are available in https:\n//limacv.github.io/VideoLoop3D_web/.\n",
        "question": {
            "statement": "What is the main challenge when creating 3D looping videos from asynchronous multi-view inputs?",
            "options": [
                "Achieving high frame rates",
                "Reducing memory usage",
                "Increasing video resolution",
                "Maintaining view consistency"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Robust Outlier Rejection for 3D Registration with Variational Bayes\nHaobo Jiang1, Zheng Dang2, Zhen Wei2, Jin Xie∗1, Jian Yang∗1, and Mathieu Salzmann∗2\n1PCA Lab, Nanjing University of Science and Technology, China\n2CVLab, EPFL, Switzerland\n{jiang.hao.bo, csjxie, csjyang}@njust.edu.cn\n{zheng.dang, zhen.wei, mathieu.salzmann}@epfl.ch\nAbstract\nLearning-based outlier (mismatched correspondence)\nrejection for robust 3D registration generally formulates\nthe outlier removal as an inlier/outlier classification prob-\nlem. The core for this to be successful is to learn the dis-\ncriminative inlier/outlier feature representations.\nIn this\npaper, we develop a novel variational non-local network-\nbased outlier rejection framework for robust alignment. By\nreformulating the non-local feature learning with varia-\ntional Bayesian inference, the Bayesian-driven long-range\ndependencies can be modeled to aggregate discriminative\ngeometric context information for inlier/outlier distinction.\nSpecifically, to achieve such Bayesian-driven contextual de-\npendencies, each query/key/value component in our non-\nlocal network predicts a prior feature distribution and a\nposterior one. Embedded with the inlier/outlier label, the\nposterior feature distribution is label-dependent and dis-\ncriminative. Thus, pushing the prior to be close to the dis-\ncriminative posterior in the training step enables the fea-\ntures sampled from this prior at test time to model high-\nquality long-range dependencies. Notably, to achieve ef-\nfective posterior feature guidance, a specific probabilis-\ntic graphical model is designed over our non-local model,\nwhich lets us derive a variational low bound as our op-\ntimization objective for model training. Finally, we pro-\npose a voting-based inlier searching strategy to cluster the\nhigh-quality hypothetical inliers for transformation estima-\ntion. Extensive experiments on 3DMatch, 3DLoMatch, and\nKITTI datasets verify the effectiveness of our method. Code\nis available at https://github.com/Jiang-HB/VBReg.\n∗Corresponding authors\nHaobo Jiang, Jin Xie, and Jian Yang are with PCA Lab, Key Lab of\nIntelligent Perception and Systems for High-Dimensional Information of\nMinistry of Education, and Jiangsu Key Lab of Image and Video Under-\nstanding for Social Security, School of Computer Science and Engineering,\nNanjing University of Science and Technology, China.\n",
        "question": {
            "statement": "What is the primary goal of learning-based outlier rejection methods in 3D registration?",
            "options": [
                "to align 3D points clouds perfectly",
                "to learn discriminative inlier/outlier feature representations",
                "to reduce computational complexity",
                "to remove all outliers from the dataset"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Robust and Scalable Gaussian Process Regression and Its Applications\nYifan Lu1, Jiayi Ma1*, Leyuan Fang2, Xin Tian1, and Junjun Jiang3\n1 Wuhan University, China\n2 Hunan University, China\n3 Harbin Institute of Technology, China\n{lyf048, xin.tian}@whu.edu.cn, {jyma2010, fangleyuan}@gmail.com, jiangjunjun@hit.edu.cn\nAbstract\nThis paper introduces a robust and scalable Gaussian\nprocess regression (GPR) model via variational learning.\nThis enables the application of Gaussian processes to a\nwide range of real data, which are often large-scale and\ncontaminated by outliers.\nTowards this end, we employ\na mixture likelihood model where outliers are assumed to\nbe sampled from a uniform distribution.\nWe next derive\na variational formulation that jointly infers the mode of\ndata, i.e., inlier or outlier, as well as hyperparameters by\nmaximizing a lower bound of the true log marginal like-\nlihood. Compared to previous robust GPR, our formula-\ntion approximates the exact posterior distribution. The in-\nducing variable approximation and stochastic variational\ninference are further introduced to our variational frame-\nwork, extending our model to large-scale data.\nWe ap-\nply our model to two challenging real-world applications,\nnamely feature matching and dense gene expression impu-\ntation. Extensive experiments demonstrate the superiority\nof our model in terms of robustness and speed. Notably,\nwhen matching 4k feature points, its inference is completed\nin milliseconds with almost no false matches. The code is at\ngithub.com/YifanLu2000/Robust-Scalable-GPR.\n",
        "question": {
            "statement": "What is the main advantage of using a mixture likelihood model in Gaussian process regression?",
            "options": [
                "It improves the interpretability of the results",
                "It increases the accuracy of the predictions",
                "It reduces the computational complexity",
                "It allows for robustness against outliers"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "VLPD: Context-Aware Pedestrian Detection\nvia Vision-Language Semantic Self-Supervision\nMengyin Liu1*\nJie Jiang2*\nChao Zhu1†\nXu-Cheng Yin1\n1School of Computer and Communication Engineering,\nUniversity of Science and Technology Beijing, Beijing, China\n2Data Platform Department, Tencent, Shenzhen, China\nblean@live.cn, zeus@tencent.com, {chaozhu, xuchengyin}@ustb.edu.cn\nAbstract\nDetecting pedestrians accurately in urban scenes is sig-\nnificant for realistic applications like autonomous driving\nor video surveillance. However, confusing human-like ob-\njects often lead to wrong detections, and small scale or\nheavily occluded pedestrians are easily missed due to their\nunusual appearances. To address these challenges, only\nobject regions are inadequate, thus how to fully utilize\nmore explicit and semantic contexts becomes a key problem.\nMeanwhile, previous context-aware pedestrian detectors ei-\nther only learn latent contexts with visual clues, or need\nlaborious annotations to obtain explicit and semantic con-\ntexts. Therefore, we propose in this paper a novel approach\nvia Vision-Language semantic self-supervision for context-\naware Pedestrian Detection (VLPD) to model explicitly se-\nmantic contexts without any extra annotations. Firstly, we\npropose a self-supervised Vision-Language Semantic (VLS)\nsegmentation method, which learns both fully-supervised\npedestrian detection and contextual segmentation via self-\ngenerated explicit labels of semantic classes by vision-\nlanguage models. Furthermore, a self-supervised Prototyp-\nical Semantic Contrastive (PSC) learning method is pro-\nposed to better discriminate pedestrians and other classes,\nbased on more explicit and semantic contexts obtained from\nVLS. Extensive experiments on popular benchmarks show\nthat our proposed VLPD achieves superior performances\nover the previous state-of-the-arts, particularly under chal-\nlenging circumstances like small scale and heavy occlusion.\nCode is available at https://github.com/lmy98129/VLPD.\n",
        "question": {
            "statement": "What is a major challenge in pedestrian detection in urban scenes?",
            "options": [
                "Confusing human-like objects and missing small scale or heavily occluded pedestrians",
                "Limited computational resources",
                "Insufficient training data",
                "Inadequate camera resolution"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Unsupervised Cumulative Domain Adaptation for Foggy Scene Optical Flow\nHanyu Zhou1, Yi Chang1*\n, Wending Yan2, Luxin Yan1\n1 National Key Laboratory of Science and Technology on Multispectral Information Processing,\nSchool of Artiﬁcial Intelligence and Automation, Huazhong University of Science and Technology\n2 Huawei International Co. Ltd.\n{hyzhou, yichang, yanluxin}@hust.edu.cn, yan.wending@huawei.com\nAbstract\nOptical ﬂow has achieved great success under clean\nscenes, but suffers from restricted performance under foggy\nscenes. To bridge the clean-to-foggy domain gap, the ex-\nisting methods typically adopt the domain adaptation to\ntransfer the motion knowledge from clean to synthetic foggy\ndomain. However, these methods unexpectedly neglect the\nsynthetic-to-real domain gap, and thus are erroneous when\napplied to real-world scenes. To handle the practical optical\nﬂow under real foggy scenes, in this work, we propose a\nnovel unsupervised cumulative domain adaptation optical\nﬂow (UCDA-Flow) framework: depth-association motion\nadaptation and correlation-alignment motion adaptation.\nSpeciﬁcally, we discover that depth is a key ingredient to in-\nﬂuence the optical ﬂow: the deeper depth, the inferior optical\nﬂow, which motivates us to design a depth-association mo-\ntion adaptation module to bridge the clean-to-foggy domain\ngap. Moreover, we ﬁgure out that the cost volume correlation\nshares similar distribution of the synthetic and real foggy im-\nages, which enlightens us to devise a correlation-alignment\nmotion adaptation module to distill motion knowledge of the\nsynthetic foggy domain to the real foggy domain. Note that\nsynthetic fog is designed as the intermediate domain. Under\nthis uniﬁed framework, the proposed cumulative adaptation\nprogressively transfers knowledge from clean scenes to real\nfoggy scenes. Extensive experiments have been performed\nto verify the superiority of the proposed method.\n",
        "question": {
            "statement": "What is a major limitation of existing methods for optical flow estimation in foggy scenes?",
            "options": [
                "They require manual annotation of foggy images",
                "They neglect the synthetic-to-real domain gap",
                "They are limited to estimating optical flow in static scenes",
                "They are only applicable to clean scenes"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "A Light Weight Model for Active Speaker Detection\nJunhua Liao1, Haihan Duan2, Kanghui Feng1, Wanbing Zhao1, Yanbing Yang1,3, Liangyin Chen1,3*\n1 College of Computer Science, Sichuan University, Chengdu, China.\n2 The Chinese University of Hong Kong, Shenzhen, China.\n3 The Institute for Industrial Internet Research, Sichuan University, Chengdu, China.\n{liaojunhua, fengkanghui, wanbingzhao}@stu.scu.edu.cn;\nhaihanduan@link.cuhk.edu.cn; {yangyanbing, chenliangyin}@scu.edu.cn\nAbstract\nActive speaker detection is a challenging task in audio-\nvisual scenarios, with the aim to detect who is speaking in\none or more speaker scenarios. This task has received con-\nsiderable attention because it is crucial in many applica-\ntions. Existing studies have attempted to improve the per-\nformance by inputting multiple candidate information and\ndesigning complex models. Although these methods have\nachieved excellent performance, their high memory and\ncomputational power consumption render their application\nto resource-limited scenarios difficult.\nTherefore, in this\nstudy, a lightweight active speaker detection architecture\nis constructed by reducing the number of input candidates,\nsplitting 2D and 3D convolutions for audio-visual feature\nextraction, and applying gated recurrent units with low\ncomputational complexity for cross-modal modeling. Ex-\nperimental results on the AVA-ActiveSpeaker dataset reveal\nthat the proposed framework achieves competitive mAP per-\nformance (94.1% vs. 94.2%), while the resource costs are\nsignificantly lower than the state-of-the-art method, partic-\nularly in model parameters (1.0M vs. 22.5M, approximately\n23×) and FLOPs (0.6G vs. 2.6G, approximately 4×). Ad-\nditionally, the proposed framework also performs well on\nthe Columbia dataset, thus demonstrating good robustness.\nThe code and model weights are available at https:\n//github.com/Junhua-Liao/Light-ASD.\n",
        "question": {
            "statement": "What is the primary goal of active speaker detection in audio-visual scenarios?",
            "options": [
                "To identify who is speaking",
                "To classify speech patterns",
                "To recognize facial expressions",
                "To analyze acoustic features"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Global Vision Transformer Pruning with Hessian-Aware Saliency\nHuanrui Yang1,2*\n, Hongxu Yin1, Maying Shen1, Pavlo Molchanov1, Hai Li3, and Jan Kautz1\n1NVIDIA, 2University of California, Berkeley, 3Duke University\nhuanrui@berkeley.edu, {dannyy, mshen, pmolchanov, jkautz}@nvidia.com, hai.li@duke.edu\nAbstract\nTransformers yield state-of-the-art results across many\ntasks. However, their heuristically designed architecture\nimpose huge computational costs during inference. This\nwork aims on challenging the common design philosophy of\nthe Vision Transformer (ViT) model with uniform dimension\nacross all the stacked blocks in a model stage, where we\nredistribute the parameters both across transformer blocks\nand between different structures within the block via the\nfirst systematic attempt on global structural pruning. Deal-\ning with diverse ViT structural components, we derive a\nnovel Hessian-based structural pruning criteria comparable\nacross all layers and structures, with latency-aware regu-\nlarization for direct latency reduction. Performing iterative\npruning on the DeiT-Base model leads to a new architec-\nture family called NViT (Novel ViT), with a novel parameter\nredistribution that utilizes parameters more efficiently. On\nImageNet-1K, NViT-Base achieves a 2.6× FLOPs reduction,\n5.1× parameter reduction, and 1.9× run-time speedup over\nthe DeiT-Base model in a near lossless manner. Smaller\nNViT variants achieve more than 1% accuracy gain at the\nsame throughput of the DeiT Small/Tiny variants, as well as\na lossless 3.3× parameter reduction over the SWIN-Small\nmodel. These results outperform prior art by a large margin.\nFurther analysis is provided on the parameter redistribution\ninsight of NViT, where we show the high prunability of ViT\nmodels, distinct sensitivity within ViT block, and unique\nparameter distribution trend across stacked ViT blocks. Our\ninsights provide viability for a simple yet effective parameter\nredistribution rule towards more efficient ViTs for off-the-\nshelf performance boost.\n",
        "question": {
            "statement": "What is a key advantage of the Novel ViT (NViT) architecture family compared to the DeiT-Base model?",
            "options": [
                "Requires more computational resources during inference",
                "Is less accurate than the DeiT-Base model",
                "Achieves a significant reduction in FLOPs and parameters while maintaining similar accuracy",
                "Has a more complex architecture with additional layers"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "HumanGen: Generating Human Radiance Fields with Explicit Priors\nSuyi Jiang1\nHaoran Jiang1\nZiyu Wang1\nHaimin Luo1,3\nWenzheng Chen2\nLan Xu1\n1ShanghaiTech University\n2University of Toronto\n3LumiAni Technology\nAbstract\nRecent years have witnessed the tremendous progress\nof 3D GANs for generating view-consistent radiance fields\nwith photo-realism.\nYet, high-quality generation of hu-\nman radiance fields remains challenging, partially due to\nthe limited human-related priors adopted in existing meth-\nods. We present HumanGen, a novel 3D human generation\nscheme with detailed geometry and 360◦realistic free-view\nrendering. It explicitly marries the 3D human generation\nwith various priors from the 2D generator and 3D recon-\nstructor of humans through the design of “anchor image”.\nWe introduce a hybrid feature representation using the an-\nchor image to bridge the latent space of HumanGen with\nthe existing 2D generator. We then adopt a pronged de-\nsign to disentangle the generation of geometry and appear-\nance. With the aid of the anchor image, we adapt a 3D re-\nconstructor for fine-grained details synthesis and propose\na two-stage blending scheme to boost appearance genera-\ntion. Extensive experiments demonstrate our effectiveness\nfor state-of-the-art 3D human generation regarding geome-\ntry details, texture quality, and free-view performance. No-\ntably, HumanGen can also incorporate various off-the-shelf\n2D latent editing methods, seamlessly lifting them into 3D.\n",
        "question": {
            "statement": "What is a key challenge in generating high-quality human radiance fields?",
            "options": [
                "Difficulty in achieving realistic textures",
                "Insufficient computing power",
                "Limited human-related priors in existing methods",
                "Inadequate training data"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "OneFormer: One Transformer to Rule Universal Image Segmentation\nJitesh Jain1,2, Jiachen Li1∗, MangTik Chiu1∗, Ali Hassani1, Nikita Orlov3, Humphrey Shi1,3\n1SHI Labs @ U of Oregon & UIUC, 2IIT Roorkee, 3Picsart AI Research (PAIR)\nhttps://github.com/SHI-Labs/OneFormer\nPanoptic Data\nSemantic Data\n3 architectures, 3 models & 3 datasets\nSemantic SOTA\nSemantic\nArchitecture\nInstance\nArchitecture\nPanoptic\nArchitecture\nSemantic Model\nInstance Model\nPanoptic Model\nInstance SOTA\nPanoptic SOTA\nPanoptic Data\nInstance Data\nSemantic Data\nSemantic SOTA\nSemantic Model\nInstance Model\nPanoptic Model\nInstance SOTA\nPanoptic SOTA\nUniversal Data\nSemantic\nSOTA\nUniversal Architecture\nInstance\nSOTA\nPanoptic\nSOTA\nUniversal Model\n(a) Specialized Architectures, Models & Datasets\n(b) Panoptic Architecture BUT Specialized Models & Datasets\n(c) Universal Architecture, Model and Dataset\n1 architecture, 3 models & 3 datasets\n1 architecture, 1 model & 1 dataset\nPanoptic\nArchitecture\nPanoptic\nArchitecture\nPanoptic\nArchitecture\nInstance Data\nFigure 1. A Path to Universal Image Segmentation. (a) Traditional segmentation methods developed specialized architectures and models\nfor each task to achieve top performance. (b) Recently, new panoptic/universal architectures [10,47] used the same architecture to achieve\ntop performance across different tasks. However, they still need to train different models for different tasks, resulting in a semi-universal\napproach. (c) We propose a unique multi-task universal architecture with a task-conditioned joint training strategy that sets new state-of-\nthe-arts across semantic, instance and panoptic segmentation tasks with a single model, unifying segmentation across architecture, model\nand dataset. Our work significantly reduces the underlying resource requirements, making segmentation more universal and accessible.\nAbstract\nUniversal Image Segmentation is not a new concept.\nPast attempts to unify image segmentation include scene\nparsing, panoptic segmentation, and, more recently, new\npanoptic architectures. However, such panoptic architec-\ntures do not truly unify image segmentation because they\nneed to be trained individually on the semantic, instance,\nor panoptic segmentation to achieve the best performance.\nIdeally, a truly universal framework should be trained only\nonce and achieve SOTA performance across all three image\nsegmentation tasks. To that end, we propose OneFormer, a\nuniversal image segmentation framework that unifies seg-\nmentation with a multi-task train-once design. We first pro-\npose a task-conditioned joint training strategy that enables\ntraining on ground truths of each domain (semantic, in-\nstance, and panoptic segmentation) within a single multi-\ntask training process. Secondly, we introduce a task token to\ncondition our model on the task at hand, making our model\ntask-dynamic to support multi-task training and inference.\nThirdly, we propose using a query-text contrastive loss dur-\ning training to establish better inter-task and inter-class\ndistinctions.\nNotably, our single OneFormer model out-\nperforms specialized Mask2Former models across all three\nsegmentation tasks on ADE20k, Cityscapes, and COCO, de-\nspite the latter being trained on each task individually. We\nbelieve OneFormer is a significant step towards making im-\nage segmentation more universal and accessible.\n",
        "question": {
            "statement": "What is the main limitation of recent panoptic/universal architectures in image segmentation?",
            "options": [
                "They require large amounts of annotated data",
                "They still require separate models for different tasks",
                "They are limited to specific types of images",
                "They are not able to achieve state-of-the-art performance"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Soft Augmentation for Image Classification\nYang Liu, Shen Yan, Laura Leal-Taixé, James Hays, Deva Ramanan\nArgo AI\nyoungleoel@gmail.com, shenyan@google.com, leal.taixe@tum.de, hays@gatech.edu, deva@cs.cmu.edu\nAbstract\nModern neural networks are over-parameterized and\nthus rely on strong regularization such as data augmenta-\ntion and weight decay to reduce overfitting and improve\ngeneralization. The dominant form of data augmentation\napplies invariant transforms, where the learning target of a\nsample is invariant to the transform applied to that sam-\nple.\nWe draw inspiration from human visual classifica-\ntion studies and propose generalizing augmentation with\ninvariant transforms to soft augmentation where the learn-\ning target softens non-linearly as a function of the de-\ngree of the transform applied to the sample: e.g., more ag-\ngressive image crop augmentations produce less confident\nlearning targets. We demonstrate that soft targets allow\nfor more aggressive data augmentation, offer more robust\nperformance boosts, work with other augmentation poli-\ncies, and interestingly, produce better calibrated models\n(since they are trained to be less confident on aggressively\ncropped/occluded examples). Combined with existing ag-\ngressive augmentation strategies, soft targets 1) double the\ntop-1 accuracy boost across Cifar-10, Cifar-100, ImageNet-\n1K, and ImageNet-V2, 2) improve model occlusion perfor-\nmance by up to 4×, and 3) half the expected calibration\nerror (ECE). Finally, we show that soft augmentation gen-\neralizes to self-supervised classification tasks. Code avail-\nable at https://github.com/youngleox/soft_\naugmentation\n",
        "question": {
            "statement": "What is the main idea behind'soft augmentation' in image classification?",
            "options": [
                "The learning target becomes less confident when the degree of transformation increases",
                "The learning target becomes more confident when the degree of transformation increases",
                "The learning target remains unchanged regardless of the transformation",
                "The learning target is randomly changed based on the transformation"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "9",
                "0",
                "0",
                "3"
            ]
        },
        "difference": 6,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "VIVE3D: Viewpoint-Independent Video Editing using 3D-Aware GANs\nAnna Fr¨\nuhst¨\nuck2*, Nikolaos Sarafianos1, Yuanlu Xu1, Peter Wonka2, Tony Tung1\n1 Meta Reality Labs Research, Sausalito\n2 KAUST\nafruehstueck.github.io/vive3D\nYAW = 0.3\nYAW = 0.0\nYAW = -0.3\n+GLASSES\n+AGE\n-SMILE\nFigure 1. We propose VIVE3D, a novel method that creates a powerful personalized 3D-aware generator using a low number of selected\nimages of a target person. Given a new video of that person, we can faithfully modify several facial attributes as well as the camera\nviewpoint of the head crop. Finally, we seamlessly composite the edited face with the source frame in a temporally and spatially consistent\nmanner, while retaining a plausible composition with the static components of the frame outside of the generator’s region. The dotted\nsquares in the center frame denote the reference regions for the three different camera poses in the column below.\nAbstract\nWe introduce VIVE3D, a novel approach that extends the\ncapabilities of image-based 3D GANs to video editing and\nis able to represent the input video in an identity-preserving\nand temporally consistent way. We propose two new build-\ning blocks. First, we introduce a novel GAN inversion tech-\nnique specifically tailored to 3D GANs by jointly embedding\nmultiple frames and optimizing for the camera parameters.\nSecond, besides traditional semantic face edits (e.g. for age\nand expression), we are the first to demonstrate edits that\nshow novel views of the head enabled by the inherent prop-\nerties of 3D GANs and our optical flow-guided compositing\ntechnique to combine the head with the background video.\nOur experiments demonstrate that VIVE3D generates high-\nfidelity face edits at consistent quality from a range of cam-\nera viewpoints which are composited with the original video\nin a temporally and spatially consistent manner.\n*This work was conducted during an internship at Meta RL Research.\n",
        "question": {
            "statement": "What is a key advantage of using 3D-aware GANs in video editing?",
            "options": [
                "Improved audio quality",
                "Increased computational efficiency",
                "Enhanced color correction capabilities",
                "Ability to edit facial attributes and change camera viewpoint"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Boosting Semi-Supervised Learning by Exploiting All Unlabeled Data\nYuhao Chen1 Xin Tan2* Borui Zhao1 Zhaowei Chen1 Renjie Song1 Jiajun Liang1 Xuequan Lu3\n1MEGVII Technology\n2East China Normal University\n3Deakin University\n{yhao.chen0617, zhaoborui.gm, chaowechan}@gmail.com, xtan@cs.ecnu.edu.cn\n{songrenjie, liangjiajun}@megvii.com, xuequan.lu@deakin.edu.au\nAbstract\nSemi-supervised learning (SSL) has attracted enormous\nattention due to its vast potential of mitigating the depen-\ndence on large labeled datasets. The latest methods (e.g.,\nFixMatch) use a combination of consistency regulariza-\ntion and pseudo-labeling to achieve remarkable successes.\nHowever, these methods all suffer from the waste of compli-\ncated examples since all pseudo-labels have to be selected\nby a high threshold to filter out noisy ones. Hence, the ex-\namples with ambiguous predictions will not contribute to\nthe training phase. For better leveraging all unlabeled ex-\namples, we propose two novel techniques: Entropy Mean-\ning Loss (EML) and Adaptive Negative Learning (ANL).\nEML incorporates the prediction distribution of non-target\nclasses into the optimization objective to avoid competition\nwith target class, and thus generating more high-confidence\npredictions for selecting pseudo-label.\nANL introduces\nthe additional negative pseudo-label for all unlabeled data\nto leverage low-confidence examples.\nIt adaptively allo-\ncates this label by dynamically evaluating the top-k per-\nformance of the model. EML and ANL do not introduce\nany additional parameter and hyperparameter.\nWe inte-\ngrate these techniques with FixMatch, and develop a sim-\nple yet powerful framework called FullMatch. Extensive\nexperiments on several common SSL benchmarks (CIFAR-\n10/100, SVHN, STL-10 and ImageNet) demonstrate that\nFullMatch exceeds FixMatch by a large margin. Integrated\nwith FlexMatch (an advanced FixMatch-based framework),\nwe achieve state-of-the-art performance.\nSource code is\navailable at https://github.com/megvii-research/FullMatch.\n",
        "question": {
            "statement": "What is a limitation of recent semi-supervised learning methods, such as FixMatch?",
            "options": [
                "They are only applicable to image classification tasks",
                "They waste complicated examples by filtering out noisy ones",
                "They require a large amount of labeled data",
                "They are computationally expensive"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "CREPE: Can Vision-Language Foundation Models Reason Compositionally?\nZixian Ma1*, Jerry Hong1*, Mustafa Omer Gul2*, Mona Gandhi3, Irena Gao1, Ranjay Krishna4\nStanford University1, Cornell University2, University of Pennsylvania3, University of Washington4\n{zixianma, jerryhong, irena}@cs.stanford.edu mog29@cornell.edu mona09@seas.upenn.edu\nranjay@cs.washington.edu\nAbstract\nA fundamental characteristic common to both human vi-\nsion and natural language is their compositional nature. Yet,\ndespite the performance gains contributed by large vision\nand language pretraining, we find that—across 7 architec-\ntures trained with 4 algorithms on massive datasets—they\nstruggle at compositionality. To arrive at this conclusion, we\nintroduce a new compositionality evaluation benchmark,\nCREPE, which measures two important aspects of compo-\nsitionality identified by cognitive science literature: system-\naticity and productivity. To measure systematicity, CREPE\nconsists of a test dataset containing over 370K image-text\npairs and three different seen-unseen splits. The three splits\nare designed to test models trained on three popular training\ndatasets: CC-12M, YFCC-15M, and LAION-400M. We also\ngenerate 325K, 316K, and 309K hard negative captions\nfor a subset of the pairs. To test productivity, CREPE con-\ntains 17K image-text pairs with nine different complexities\nplus 278K hard negative captions with atomic, swapping\nand negation foils. The datasets are generated by repurpos-\ning the Visual Genome scene graphs and region descriptions\nand applying handcrafted templates and GPT-3. For sys-\ntematicity, we find that model performance decreases con-\nsistently when novel compositions dominate the retrieval\nset, with Recall@1 dropping by up to 9%. For productivity,\nmodels’ retrieval success decays as complexity increases,\nfrequently nearing random chance at high complexity. These\nresults hold regardless of model and training dataset size.\n",
        "question": {
            "statement": "What is a key aspect of human vision and natural language that current large pre-trained models struggle with?",
            "options": [
                "interpretability",
                "compositionality",
                "scalability",
                "generalizability"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency\nPseudo Label Correction Module\nLinzhi Huang1, 2 *\nYulong Li2\nHongbo Tian1, 2*\nYue Yang2\nXiangang Li2\nWeihong Deng1 †\nJieping Ye2\n1Beijing University of Posts and Telecommunications, 2Beike\n{huanglinzhi, tianhongbo, whdeng}@bupt.edu.cn\n{liyulong008, yangyue092, lixiangang002, yejieping}@ke.com\nAbstract\nIn this paper, we delve into semi-supervised 2D human\npose estimation. The previous method ignored two prob-\nlems: (i) When conducting interactive training between\nlarge model and lightweight model, the pseudo label of\nlightweight model will be used to guide large models. (ii)\nThe negative impact of noise pseudo labels on training.\nMoreover, the labels used for 2D human pose estimation\nare relatively complex: keypoint category and keypoint po-\nsition.\nTo solve the problems mentioned above, we pro-\npose a semi-supervised 2D human pose estimation frame-\nwork driven by a position inconsistency pseudo label cor-\nrection module (SSPCM). We introduce an additional auxil-\niary teacher and use the pseudo labels generated by the two\nteacher model in different periods to calculate the inconsis-\ntency score and remove outliers. Then, the two teacher mod-\nels are updated through interactive training, and the student\nmodel is updated using the pseudo labels generated by two\nteachers. To further improve the performance of the student\nmodel, we use the semi-supervised Cut-Occlude based on\npseudo keypoint perception to generate more hard and ef-\nfective samples. In addition, we also proposed a new indoor\noverhead fisheye human keypoint dataset WEPDTOF-Pose.\nExtensive experiments demonstrate that our method outper-\nforms the previous best semi-supervised 2D human pose es-\ntimation method. We will release the code and dataset at\nhttps://github.com/hlz0606/SSPCM.\n*This work was done when the authors were visiting Beike as interns.\n†Corresponding author.\nSupervised\nDataDistill\nDUAL\nOurs\n1000\n5000\n10000\n31.5\n37.6\n44.6\n46.9\n46.4\n51.6\n55.6 57.5\n51.1\n56.6\n59.6 60.7\n0\n10\n20\n30\n40\n50\n60\n70\n80\nCOCO  AP(%)\nNumber of labeled person instances (Total Person: ≈150000)\nFigure 1. Performance comparison between our method SSPCM\nand SOTA method (DataDistill [33], DUAL [46]) on COCO [28]\ndataset.\nOn the COCO dataset, using 1000, 5000, and 10000\nlabeled person instances, our method has increased 2.3mAP,\n1.9mAP, and 1.1mAP compared with the previous method.\n",
        "question": {
            "statement": "What is the primary issue with using pseudo labels in semi-supervised 2D human pose estimation?",
            "options": [
                "Insufficient data for training",
                "Inability to generalize to new datasets",
                "Noise in pseudo labels",
                "Complexity of keypoint categories"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "9",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Learning Action Changes by Measuring Verb-Adverb Textual Relationships\nDavide Moltisanti, Frank Keller, Hakan Bilen, Laura Sevilla-Lara\nThe University of Edinburgh, United Kingdom\n{davide.moltisanti, frank.keller, h.bilen, l.sevilla}@ed.ac.uk\nAbstract\nThe goal of this work is to understand the way actions\nare performed in videos. That is, given a video, we aim to\npredict an adverb indicating a modification applied to the\naction (e.g. cut “finely”). We cast this problem as a regres-\nsion task. We measure textual relationships between verbs\nand adverbs to generate a regression target representing\nthe action change we aim to learn. We test our approach\non a range of datasets and achieve state-of-the-art results\non both adverb prediction and antonym classification. Fur-\nthermore, we outperform previous work when we lift two\ncommonly assumed conditions: the availability of action la-\nbels during testing and the pairing of adverbs as antonyms.\nExisting datasets for adverb recognition are either noisy,\nwhich makes learning difficult, or contain actions whose ap-\npearance is not influenced by adverbs, which makes evalu-\nation less reliable. To address this, we collect a new high\nquality dataset: Adverbs in Recipes (AIR). We focus on in-\nstructional recipes videos, curating a set of actions that\nexhibit meaningful visual changes when performed differ-\nently. Videos in AIR are more tightly trimmed and were\nmanually reviewed by multiple annotators to ensure high la-\nbelling quality. Results show that models learn better from\nAIR given its cleaner videos. At the same time, adverb pre-\ndiction on AIR is challenging, demonstrating that there is\nconsiderable room for improvement.\n",
        "question": {
            "statement": "What type of data does the authors' new dataset, Adverbs in Recipes (AIR), consist of?",
            "options": [
                "audio recordings of cooking instructions",
                "instructional recipe videos",
                "written recipes",
                "images of food"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions\nDale Decatur\nUniversity of Chicago\nddecatur@uchicago.edu\nItai Lang\nUniversity of Chicago\nitailang@uchicago.edu\nRana Hanocka\nUniversity of Chicago\nranahanocka@uchicago.edu\nHat\nNecklace\nHeadlights\nShoes\nEyeglasses\nFigure 1. 3D Highlighter localizes semantic regions on a shape using text as input. Our technique reasons about where to place seemingly\nunrelated concepts in semantically meaningful locations on the 3D shape, such as a ‘necklace’ on a horse or ‘shoes’ on an alien.\nAbstract\nWe present 3D Highlighter, a technique for localizing se-\nmantic regions on a mesh using text as input. A key feature\nof our system is the ability to interpret “out-of-domain”\nlocalizations. Our system demonstrates the ability to rea-\nson about where to place non-obviously related concepts\non an input 3D shape, such as adding clothing to a bare\n3D animal model. Our method contextualizes the text de-\nscription using a neural ﬁeld and colors the correspond-\ning region of the shape using a probability-weighted blend.\nOur neural optimization is guided by a pre-trained CLIP en-\ncoder, which bypasses the need for any 3D datasets or 3D\nannotations. Thus, 3D Highlighter is highly ﬂexible, gen-\neral, and capable of producing localizations on a myriad of\ninput shapes. Our code is publicly available at https:\n//github.com/threedle/3DHighlighter.\n",
        "question": {
            "statement": "What is a key advantage of the 3D Highlighter technique?",
            "options": [
                "Limited to localizing only functional parts of objects",
                "Ability to localize out-of-domain concepts",
                "Ability to only work with human-shaped models",
                "Requirement for large amounts of 3D annotated data"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Auto-CARD: Efficient and Robust Codec Avatar Driving\nfor Real-time Mobile Telepresence\nYonggan Fu1*\n, Yuecheng Li2, Chenghui Li2, Jason Saragih2,\nPeizhao Zhang2, Xiaoliang Dai2, Yingyan (Celine) Lin1\n1Georgia Institute of Technology\n2Meta\n{yfu314, celine.lin}@gatech.edu {yuecheng.li, leo.li, jsaragih, stzpz, xiaoliangdai}@meta.com\nAbstract\nReal-time and robust photorealistic avatars for telepres-\nence in AR/VR have been highly desired for enabling im-\nmersive photorealistic telepresence. However, there still\nexists one key bottleneck: the considerable computational\nexpense needed to accurately infer facial expressions cap-\ntured from headset-mounted cameras with a quality level that\ncan match the realism of the avatar’s human appearance.\nTo this end, we propose a framework called Auto-CARD,\nwhich for the first time enables real-time and robust driving\nof Codec Avatars when exclusively using merely on-device\ncomputing resources. This is achieved by minimizing two\nsources of redundancy. First, we develop a dedicated neural\narchitecture search technique called AVE-NAS for avatar\nencoding in AR/VR, which explicitly boosts both the searched\narchitectures’ robustness in the presence of extreme facial ex-\npressions and hardware friendliness on fast evolving AR/VR\nheadsets. Second, we leverage the temporal redundancy in\nconsecutively captured images during continuous rendering\nand develop a mechanism dubbed LATEX to skip the com-\nputation of redundant frames. Specifically, we first identify\nan opportunity from the linearity of the latent space derived\nby the avatar decoder and then propose to perform adaptive\nlatent extrapolation for redundant frames. For evaluation,\nwe demonstrate the efficacy of our Auto-CARD framework in\nreal-time Codec Avatar driving settings, where we achieve a\n5.05× speed-up on Meta Quest 2 while maintaining a compa-\nrable or even better animation quality than state-of-the-art\navatar encoder designs.\n",
        "question": {
            "statement": "What is the primary goal of the Auto-CARD framework?",
            "options": [
                "Reduce the cost of developing avatar encoder designs",
                "Enable real-time and robust driving of Codec Avatars using only on-device computing resources",
                "Improve the visual quality of avatars in AR/VR",
                "Increase the storage capacity of AR/VR headsets"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "10",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Common Pets in 3D:\nDynamic New-View Synthesis of Real-Life Deformable Categories\nSamarth Sinha\nUniversity of Toronto\nsamarth.sinha@mail.utoronto.ca\nRoman Shapovalov\nMeta AI\nromansh@meta.com\nJeremy Reizenstein\nMeta AI\nreizenstein@meta.com\nIgnacio Rocco\nMeta AI\nirocco@meta.com\nNatalia Neverova\nMeta AI\nnneverova@meta.com\nAndrea Vedaldi\nMeta AI\nvedaldi@meta.com\nDavid Novotny\nMeta AI\ndnovotny@meta.com\nReconstruct unseen videos at test-time\nTrain a category-level model on videos of non-rigid objects\nTrackeRF\nTrackeRF\nFigure 1. We tackle the problem of synthesising new views of deformable objects given only a small number of views taken at different\ntimes. We introduce new benchmark data for this task: Common Pets in 3D (CoP3D), containing 4,200 smartphone videos of cats and\ndogs collected ‘in the wild’. We also propose a new method, Tracker-NeRF, a deformable new-view synthesis algorithm which learns a\ncategory-level reconstruction prior from videos and applies it to reconstruct new objects at test time.\nAbstract\nObtaining photorealistic reconstructions of objects from\nsparse views is inherently ambiguous and can only be\nachieved by learning suitable reconstruction priors. Ear-\nlier works on sparse rigid object reconstruction successfully\nlearned such priors from large datasets such as CO3D. In\nthis paper, we extend this approach to dynamic objects. We\nuse cats and dogs as a representative example and introduce\nCommon Pets in 3D (CoP3D), a collection of crowd-sourced\nvideos of approximately 4,200 distinct pets. CoP3D is one\nof the first large-scale datasets for benchmarking non-rigid\n3D reconstruction “in the wild”. We also propose Tracker-\nNeRF, a method for learning 4D reconstruction from our\ndataset. At test time, given a small number of video frames\nof an unseen object, Tracker-NeRF predicts the trajecto-\nries of its 3D points and generates new views, interpolating\nviewpoint and time. Results on CoP3D reveal significantly\nbetter non-rigid new-view synthesis performance than ex-\nisting baselines. The data will be available on the project\nwebpage: https://cop3d.github.io/.\n",
        "question": {
            "statement": "What type of objects does the proposed method, Tracker-NeRF, aim to reconstruct?",
            "options": [
                "non-rigid objects",
                "synthetic characters",
                "static scenes",
                "rigid objects"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Relational Space-Time Query in Long-Form Videos\nXitong Yang1\nFu-Jen Chu1\nMatt Feiszli1\nRaghav Goyal1,2\nLorenzo Torresani1\nDu Tran1\n1 Meta AI\n2 University of British Columbia\n00:00\n30:00\n05:00\n10:00\n20:00\n25:00\n15:00\nObject-Query Example:\nQ: What objects did I interacted with when I was \n“washing dishes” during [20:00 -- 30:00] ?\nA:\n…\n…\n…\n…\nActivity-Query Example:\nQ: What activities did I do with {                                   }\nduring [00:00 -- 15:00] ?\nA: “Making tea”\nTime-Query Example:\nQ: When did I “take pills” \nFeafefaefefefef.     with {                                       } ? \nA: [18:31 – 20:15]\nActivity-Query Template:\nQ: What activities did I do when I interacted with object 𝑜\nfefduring [𝑠, 𝑒] ?\nA: A subset of 𝐶pre-defined activities 𝒜⊆1, … , 𝐶\nObject-Query Template:\nQ: What objects did I interact with when I was doing activity \nfef𝑎during [𝑠, 𝑒] ?\nA: Spatio-temporal locations of the interacted objects                   \nfe 𝑡!, 𝑥!, 𝑦!, 𝑤!, ℎ! !\"#\n$\nTime-Query Template:\nQ: When did I do activity 𝑎that involves interaction with \nfefobject 𝑜?\nA: Temporal locations of the corresponding activity \nfef [𝑠!, 𝑒!] !\"#\n%\nFigure 1. Illustration of the three types of queries in our Relational Space-Time Query (ReST) framework. Given a long video spanning\nup to 30 minutes, a set of queries are provided to assess a model’s ability to understand activities, objects, and their interactions in the\nvideo. All queries and answers are generated in the form of pre-defined templates (top-left) to avoid the ambiguity and bias introduced\nby language input / output. Note that ReST is a holistic framework that supports constructing queries with different levels of complexity\nbeyond the three basic types described in this paper.\nAbstract\nEgocentric videos are often available in the form of un-\ninterrupted, uncurated long videos capturing the camera\nwearers’ daily life activities.Understanding these videos re-\nquires models to be able to reason about activities, objects,\nand their interactions. However, current video benchmarks\nstudy these problems independently and under short, cu-\nrated clips. In contrast, real-world applications, e.g. AR\nassistants, require bundling these problems for both model\ndevelopment and evaluation. In this paper, we propose to\nstudy these problems in a joint framework for long video\nunderstanding.\nOur contributions are three-fold.\nFirst,\nwe propose an integrated framework, namely Relational\nSpace-Time Query (ReST), for evaluating video under-\nstanding models via templated spatiotemporal queries. Sec-\nond, we introduce two new benchmarks, ReST-ADL and\nReST-Ego4D 1, which augment the existing egocentric video\ndatasets with abundant query annotations generated by the\nReST framework. Finally, we present a set of baselines and\n1The latest version of our benchmark and models will be available here.\nin-depth analysis on the two benchmarks and provide in-\nsights about the query tasks. We view our integrated frame-\nwork and benchmarks as a step towards comprehensive,\nmulti-step reasoning in long videos, and believe it will fa-\ncilitate the development of next generations of video under-\nstanding models.\n",
        "question": {
            "statement": "What type of data does the Relational Space-Time Query framework aim to analyze?",
            "options": [
                "Short curated clips",
                "Egocentric videos",
                "Static images",
                "Audio recordings"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "High-frequency Stereo Matching Network\nHaoliang Zhao1,4†, Huizhou Zhou2,4†, Yongjun Zhang1*, Jie Chen3, Yitong Yang1 and Yong Zhao3,4**\n1Text Computing & Cognitive Intelligence Engineering Research Center of National Education\nMinistry, State Key Laboratory of Public Big Data, College of Computer Science and Technology,\nInstitute of Artificial Intelligence, Guizhou University, Guiyang 550025, Guizhou, China\n2School of Physics and Optoelectronic Engineering, Guangdong University of Technology, Guangzhou\n510006, China\n3The Key Laboratory of Integrated Microsystems, Shenzhen Graduate School, Peking University, China\n4Ghost-Valley AI Technology, Shenzhen, Guangdong, China\nAbstract\nIn the field of binocular stereo matching, remarkable\nprogress has been made by iterative methods like RAFT-\nStereo and CREStereo. However, most of these methods\nlose information during the iterative process, making it\ndifficult to generate more detailed difference maps that take\nfull advantage of high-frequency information. We propose\nthe Decouple module to alleviate the problem of data\ncoupling and allow features containing subtle details to\ntransfer across the iterations which proves to alleviate the\nproblem significantly in the ablations. To further capture\nhigh-frequency details, we propose a Normalization Refine-\nment module that unifies the disparities as a proportion of\nthe disparities over the width of the image, which address\nthe problem of module failure in cross-domain scenarios.\nFurther, with the above improvements, the ResNet-like\nfeature extractor that has not been changed for years\nbecomes a bottleneck. Towards this end, we proposed a\nmulti-scale and multi-stage feature extractor that intro-\nduces the channel-wise self-attention mechanism which\ngreatly addresses this bottleneck.\nOur method (DLNR)\nranks 1st on the Middlebury leaderboard, significantly\noutperforming the next best method by 13.04%.\nOur\nmethod also achieves SOTA performance on the KITTI-\n2015 benchmark for D1-fg. Code and demos are available\nat: https://github.com/David-Zhao-1997/\nHigh-frequency-Stereo-Matching-Network.\n† These authors contributed equally.\n⋆Corresponding author.\nEmail: zyj6667@126.com.\n⋆⋆Second Corresponding author.\nEmail: zhaoyong@pkusz.edu.cn\n",
        "question": {
            "statement": "What is a common limitation of iterative methods in binocular stereo matching?",
            "options": [
                "Loss of high-frequency information",
                "Inability to handle large datasets",
                "Insufficient computational resources",
                "Lack of robustness to noise"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Are We Ready for Vision-Centric Driving Streaming Perception?\nThe ASAP Benchmark\nXiaofeng Wang1,2 Zheng Zhu3 Yunpeng Zhang3 Guan Huang3\nYun Ye3 Wenbo Xu3 Ziwei Chen4 Xingang Wang1*\n1Institute of Automation, Chinese Academy of Sciences\n2University of Chinese Academy of Sciences\n3PhiGent Robotics\n4Southeast University\nAbstract\nIn recent years, vision-centric perception has flourished\nin various autonomous driving tasks, including 3D detec-\ntion, semantic map construction, motion forecasting, and\ndepth estimation. Nevertheless, the latency of vision-centric\napproaches is too high for practical deployment (e.g., most\ncamera-based 3D detectors have a runtime greater than\n300ms). To bridge the gap between ideal researches and\nreal-world applications, it is necessary to quantify the\ntrade-off between performance and efficiency. Tradition-\nally, autonomous-driving perception benchmarks perform\nthe offline evaluation, neglecting the inference time de-\nlay. To mitigate the problem, we propose the Autonomous-\ndriving StreAming Perception (ASAP) benchmark, which\nis the first benchmark to evaluate the online performance\nof vision-centric perception in autonomous driving.\nOn\nthe basis of the 2Hz annotated nuScenes dataset, we first\npropose an annotation-extending pipeline to generate high-\nframe-rate labels for the 12Hz raw images. Referring to\nthe practical deployment, the Streaming Perception Under\nconstRained-computation (SPUR) evaluation protocol is\nfurther constructed, where the 12Hz inputs are utilized\nfor streaming evaluation under the constraints of differ-\nent computational resources.\nIn the ASAP benchmark,\ncomprehensive experiment results reveal that the model\nrank alters under different constraints, suggesting that the\nmodel latency and computation budget should be consid-\nered as design choices to optimize the practical deployment.\nTo facilitate further research, we establish baselines for\ncamera-based streaming 3D detection, which consistently\nenhance the streaming performance across various hard-\nware.\nASAP project page: https://github.com/\nJeffWang987/ASAP.\n",
        "question": {
            "statement": "What is a major limitation of current vision-centric approaches in autonomous driving?",
            "options": [
                "High latency",
                "Lack of data",
                "Insufficient computing power",
                "Limited camera resolution"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Contrastive Mean Teacher for Domain Adaptive Object Detectors\nShengcao Cao1\nDhiraj Joshi2\nLiang-Yan Gui1\nYu-Xiong Wang1\n1University of Illinois at Urbana-Champaign\n2IBM Research\n1{cao44,lgui,yxw}@illinois.edu\n2djoshi@us.ibm.com\nAbstract\nObject detectors often suffer from the domain gap be-\ntween training (source domain) and real-world applications\n(target domain). Mean-teacher self-training is a powerful\nparadigm in unsupervised domain adaptation for object de-\ntection, but it struggles with low-quality pseudo-labels. In\nthis work, we identify the intriguing alignment and syn-\nergy between mean-teacher self-training and contrastive\nlearning. Motivated by this, we propose Contrastive Mean\nTeacher (CMT) – a unified, general-purpose framework\nwith the two paradigms naturally integrated to maximize\nbeneficial learning signals. Instead of using pseudo-labels\nsolely for final predictions, our strategy extracts object-\nlevel features using pseudo-labels and optimizes them via\ncontrastive learning, without requiring labels in the target\ndomain.\nWhen combined with recent mean-teacher self-\ntraining methods, CMT leads to new state-of-the-art target-\ndomain performance: 51.9% mAP on Foggy Cityscapes,\noutperforming the previously best by 2.1% mAP. Notably,\nCMT can stabilize performance and provide more signifi-\ncant gains as pseudo-label noise increases.\n",
        "question": {
            "statement": "What is a major challenge faced by mean-teacher self-training in unsupervised domain adaptation for object detection?",
            "options": [
                "Low-quality pseudo-labels",
                "Insufficient training data",
                "Domain gap between source and target domains",
                " Limited computational resources"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Visual Programming: Compositional visual reasoning without training\nTanmay Gupta, Aniruddha Kembhavi\nPRIOR @ Allen Institute for AI\nhttps://prior.allenai.org/projects/visprog\nQuestion: Are there both ties and glasses in the picture?\nProgram:\nBOX0=Loc(image=IMAGE, object=‘ties’)\nANSWER0=Count(box=BOX0)\nBOX1=Loc(image=IMAGE, object=‘glasses’)\nANSWER1=Count(box=BOX1)\nANSWER2=Eval(“‘yes’ if {ANSWER0} > 0 and {ANSWER1} > 0 else ‘no’”)\nRESULT=ANSWER2\nPrediction: no\nIMAGE:\nCompositional Visual Question Answering\nLEFT:\nRIGHT:\nStatement: The left and right image contains a total of six people and two boats.\nProgram:\nANSWER0=Vqa(image=LEFT, question=‘How many people are in the image?’)\nANSWER1=Vqa(image=RIGHT, question=‘How many people are in the image?’)\nANSWER2=Vqa(image=LEFT, question=‘How many boats are in the image?’)\nANSWER3=Vqa(image=RIGHT, question=‘How many boats are in the image?’)\nANSWER4=Eval(‘{ANSWER0} + {ANSWER1} == 6 and {ANSWER2} + {ANSWER3} == 2’)\nRESULT=ANSWER4\nPrediction: False\nNatural Language Visual Reasoning\nIMAGE:\nPrediction: IMAGE0\nInstruction: Tag the 7 main characters on the TV show Big Bang Theory\nProgram:\nOBJ0=FaceDet(image=IMAGE)\nLIST0=List(query=‘main characters on the TV show Big Bang Theory’, max=7)\nOBJ1=Classify(image=IMAGE, object=OBJ0, categories=LIST0)\nIMAGE0=Tag(image=IMAGE, object=OBJ1)\nRESULT=IMAGE0\nFactual Knowledge Object Tagging\nIMAGE:\nPrediction: IMAGE1\nInstruction: Hide Daniel Craig with 8) and Sean Connery with ;)\nProgram:\nOBJ0=FaceDet(image=IMAGE)\nOBJ1=Select(image=IMAGE, object=OBJ0, query=‘Daniel Craig’, category=None)\nIMAGE0=Emoji(image=IMAGE, object=OBJ1, emoji=‘smiling_face_with_sunglasses’)\nOBJ2=Select(image=IMAGE, object=OBJ0, query=‘Sean Connery’, category: None)\nIMAGE1=Emoji(image=IMAGE0, object=OBJ2, emoji=‘winking_face’)\nRESULT=IMAGE1\nNatural Language Image Editing\nIMAGE:\nPrediction: IMAGE0\nInstruction: Replace desert with lush green grass\nProgram:\nOBJ0=Seg(image=IMAGE)\nOBJ1=Select(image=IMAGE, object=OBJ0, query=‘desert’, category=None)\nIMAGE0=Replace(image=IMAGE, object=OBJ1, prompt=‘lush green grass’)\nRESULT=IMAGE0\nIMAGE:\nPrediction: IMAGE0\nInstruction: Create a color pop of Barack Obama (person)\nProgram:\nOBJ0=Seg(image=IMAGE)\nOBJ1=Select(image=IMAGE, object=OBJ0, query=‘Barack Obama’, category=‘person’)\nIMAGE0=ColorPop(image=IMAGE, object=OBJ1)\nRESULT=IMAGE0\nHigh-level \nProgram\nNatural Language \nInstruction\nInput\nImage(s)\nIn-context\ninstruction-program\npairs\nProgram \nInterpreter\nVISPROG\nProgram\nGenerator\nVISPROG\nPrediction\nVisual\nRationale\nVisual Programming\nFigure 1. VISPROG is a modular and interpretable neuro-symbolic system for compositional visual reasoning. Given a few examples\nof natural language instructions and the desired high-level programs, VISPROG generates a program for any new instruction using in-\ncontext learning in GPT-3 and then executes the program on the input image(s) to obtain the prediction. VISPROG also summarizes the\nintermediate outputs into an interpretable visual rationale (Fig. 4). We demonstrate VISPROG on tasks that require composing a diverse\nset of modules for image understanding and manipulation, knowledge retrieval, and arithmetic and logical operations.\nAbstract\nWe present VISPROG, a neuro-symbolic approach to\nsolving complex and compositional visual tasks given nat-\nural language instructions.\nVISPROG avoids the need\nfor any task-specific training.\nInstead, it uses the in-\ncontext learning ability of large language models to gener-\nate python-like modular programs, which are then executed\nto get both the solution and a comprehensive and inter-\npretable rationale. Each line of the generated program may\ninvoke one of several off-the-shelf computer vision models,\nimage processing subroutines, or python functions to pro-\nduce intermediate outputs that may be consumed by subse-\nquent parts of the program. We demonstrate the flexibility\nof VISPROG on 4 diverse tasks - compositional visual ques-\ntion answering, zero-shot reasoning on image pairs, factual\nknowledge object tagging, and language-guided image edit-\ning. We believe neuro-symbolic approaches like VISPROG",
        "question": {
            "statement": "What is the primary advantage of the VISPROG system?",
            "options": [
                "It does not require task-specific training",
                "It requires a large amount of annotated data",
                "It can only be used with simple, non-compositional tasks",
                "It can only be used for image editing tasks"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "High-level \nProgram\nNatural Language \nInstruction\nInput\nImage(s)\nIn-context\ninstruction-program\npairs\nProgram \nInterpreter\nVISPROG\nProgram\nGenerator\nVISPROG\nPrediction\nVisual\nRationale\nVisual Programming\nFigure 1. VISPROG is a modular and interpretable neuro-symbolic system for compositional visual reasoning. Given a few examples\nof natural language instructions and the desired high-level programs, VISPROG generates a program for any new instruction using in-\ncontext learning in GPT-3 and then executes the program on the input image(s) to obtain the prediction. VISPROG also summarizes the\nintermediate outputs into an interpretable visual rationale (Fig. 4). We demonstrate VISPROG on tasks that require composing a diverse\nset of modules for image understanding and manipulation, knowledge retrieval, and arithmetic and logical operations.\nAbstract\nWe present VISPROG, a neuro-symbolic approach to\nsolving complex and compositional visual tasks given nat-\nural language instructions.\nVISPROG avoids the need\nfor any task-specific training.\nInstead, it uses the in-\ncontext learning ability of large language models to gener-\nate python-like modular programs, which are then executed\nto get both the solution and a comprehensive and inter-\npretable rationale. Each line of the generated program may\ninvoke one of several off-the-shelf computer vision models,\nimage processing subroutines, or python functions to pro-\nduce intermediate outputs that may be consumed by subse-\nquent parts of the program. We demonstrate the flexibility\nof VISPROG on 4 diverse tasks - compositional visual ques-\ntion answering, zero-shot reasoning on image pairs, factual\nknowledge object tagging, and language-guided image edit-\ning. We believe neuro-symbolic approaches like VISPROG\nare an exciting avenue to easily and effectively expand the\nscope of AI systems to serve the long tail of complex tasks\nthat people may wish to perform.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14953\n",
        "question": {
            "statement": "What is the primary advantage of the VISPROG system?",
            "options": [
                "It requires extensive manual programming",
                "It only works with simple visual tasks",
                "It does not require task-specific training",
                "It is limited to only four specific tasks"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Non-Contrastive Unsupervised Learning of Physiological Signals from Video\nJeremy Speth, Nathan Vance, Patrick Flynn, Adam Czajka\nUniversity of Notre Dame\n{jspeth,nvance1,flynn,aczajka}@nd.edu\nAbstract\nSubtle periodic signals such as blood volume pulse and\nrespiration can be extracted from RGB video, enabling non-\ncontact health monitoring at low cost. Advancements in\nremote pulse estimation – or remote photoplethysmogra-\nphy (rPPG) – are currently driven by deep learning solu-\ntions. However, modern approaches are trained and evalu-\nated on benchmark datasets with ground truth from contact-\nPPG sensors. We present the first non-contrastive unsuper-\nvised learning framework for signal regression to mitigate\nthe need for labelled video data. With minimal assumptions\nof periodicity and finite bandwidth, our approach discov-\ners the blood volume pulse directly from unlabelled videos.\nWe find that encouraging sparse power spectra within nor-\nmal physiological bandlimits and variance over batches of\npower spectra is sufficient for learning visual features of\nperiodic signals. We perform the first experiments utilizing\nunlabelled video data not specifically created for rPPG to\ntrain robust pulse rate estimators. Given the limited induc-\ntive biases and impressive empirical results, the approach is\ntheoretically capable of discovering other periodic signals\nfrom video, enabling multiple physiological measurements\nwithout the need for ground truth signals.\n",
        "question": {
            "statement": "What is a major limitation of current approaches to remote pulse estimation?",
            "options": [
                "Insufficient computational power",
                "The need for labelled video data",
                "Inability to detect respiration signals",
                "Limited camera resolution"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Deep Polarization Reconstruction with PDAVIS Events\nHaiyang Mei1,2\nZuowen Wang2\nXin Yang1 Xiaopeng Wei1\nTobi Delbruck2\n1Dalian University of Technology, Dalian, China\n2Institute of Neuroinformatics, University of Z¨\nurich and ETH Z¨\nurich, Z¨\nurich, Switzerland\nhaiyang.mei@outlook.com, {xinyang, xpwei}@dlut.edu.cn, {zuowen, tobi}@ini.uzh.ch\nIntensity\nAngle of\nLinear Polarization\nDegree of\nLinear Polarization\nPolarization\nFireNet\nE2P\n(Ours)\nSaturated\nBlur\nPolarization Event Camera \n(PDAVIS)\nFrame\nreadout\nLn(I)\nθon \nθoff\nON\nOFF\nEvents\nSubpixel Circuit\nPolarizer Array\n0\n1\n0\nπ/2 \n+\n-\n+\n-\n+\n-\nEquation\nFigure 1. The polarization dynamic and active pixel vision sensor (PDAVIS) [15] is integrated with a nanowire polarizer array. It con-\ncurrently outputs conventional polarization intensity frames and asynchronous polarization brightness change events with submillisecond\nlatency over a million-fold illumination range. Taking polarization events as input, the existing state of the art (SOA) method Polarization\nFireNet [15,41] can reconstruct the intensity video with high dynamic range (HDR) and less motion blur, but fails to correctly reconstruct\nthe angle and degree of linear polarization. Our Events to Polarization (E2P) achieves sharp output and HDR with a more accurate polar-\nization reconstruction.\nAbstract\nThe polarization event camera PDAVIS is a novel bio-\ninspired neuromorphic vision sensor that reports both con-\nventional polarization frames and asynchronous, continu-\nously per-pixel polarization brightness changes (polariza-\ntion events) with fast temporal resolution and large dy-\nnamic range. A deep neural network method (Polariza-\ntion FireNet) was previously developed to reconstruct the\npolarization angle and degree from polarization events for\nbridging the gap between the polarization event camera\nand mainstream computer vision. However, Polarization\nFireNet applies a network pre-trained for normal event-\nbased frame reconstruction independently on each of four\nchannels of polarization events from four linear polariza-\ntion angles, which ignores the correlations between chan-\nnels and inevitably introduces content inconsistency be-\ntween the four reconstructed frames, resulting in unsatisfac-\ntory polarization reconstruction performance. In this work,\nwe strive to train an effective, yet efficient, DNN model that\ndirectly outputs polarization from the input raw polariza-\ntion events.\nTo this end, we constructed the first large-\nscale event-to-polarization dataset, which we subsequently\nemployed to train our events-to-polarization network E2P.\nE2P extracts rich polarization patterns from input polariza-\ntion events and enhances features through cross-modality\ncontext integration. We demonstrate that E2P outperforms\nPolarization FireNet by a significant margin with no addi-\ntional computing cost. Experimental results also show that\nE2P produces more accurate measurement of polarization\nthan the PDAVIS frames in challenging fast and high dy-\nnamic range scenes. Code and data are publicly available\nat: https://github.com/SensorsINI/e2p.\n",
        "question": {
            "statement": "What is a limitation of the previous state-of-the-art method, Polarization FireNet, when reconstructing the angle and degree of linear polarization?",
            "options": [
                "ignoring correlations between channels",
                "having low dynamic range",
                "producing blurry output",
                "requiring additional computing cost"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "8",
                "0",
                "3",
                "0"
            ]
        },
        "difference": 5,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Jedi: Entropy-based Localization and Removal of Adversarial Patches\nBilel Tarchoun ∗, Anouar Ben Khalifa ∗, ⊥, Mohamed Ali Mahjoub ∗, Nael Abu-Ghazaleh †\nIhsen Alouani ‡, Υ\n‡ CSIT, Queen’s University Belfast, UK\n∗Universit´\ne de Sousse, Ecole Nationale d’Ing´\nenieurs de Sousse, LATIS, Sousse, Tunisia;\nΥIEMN CNRS 8520, Universit´\ne Polytechnique Hauts-de-France\n⊥Universit´\ne de Jendouba, Institut National des Technologies et des Sciences du Kef, Tunisia;\n† University of California Riverside, CA, USA\nbilel.tarchoun@eniso.u-sousse.tn,i.alouani@qub.ac.uk\nAbstract\nReal-world adversarial physical patches were shown to\nbe successful in compromising state-of-the-art models in a\nvariety of computer vision applications. Existing defenses\nthat are based on either input gradient or features analy-\nsis have been compromised by recent GAN-based attacks\nthat generate naturalistic patches. In this paper, we pro-\npose Jedi, a new defense against adversarial patches that\nis resilient to realistic patch attacks. Jedi tackles the patch\nlocalization problem from an information theory perspec-\ntive; leverages two new ideas: (1) it improves the identi-\nfication of potential patch regions using entropy analysis:\nwe show that the entropy of adversarial patches is high,\neven in naturalistic patches; and (2) it improves the local-\nization of adversarial patches, using an autoencoder that\nis able to complete patch regions from high entropy ker-\nnels. Jedi achieves high-precision adversarial patch local-\nization, which we show is critical to successfully repair the\nimages. Since Jedi relies on an input entropy analysis, it is\nmodel-agnostic, and can be applied on pre-trained off-the-\nshelf models without changes to the training or inference of\nthe protected models. Jedi detects on average 90% of ad-\nversarial patches across different benchmarks and recovers\nup to 94% of successful patch attacks (Compared to 75%\nand 65% for LGS and Jujutsu, respectively).\n",
        "question": {
            "statement": "What approach does the Jedi defense mechanism use to identify potential patch regions?",
            "options": [
                "Gradient analysis",
                "Feature extraction",
                "Entropy analysis",
                "Convolutional neural networks"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "PREIM3D: 3D Consistent Precise Image Attribute Editing from a Single Image\nJianhui Li1,2, Jianmin Li1 ∗, Haoji Zhang1, Shilong Liu1, Zhengyi Wang1,\nZihao Xiao3, Kaiwen Zheng1, Jun Zhu1 ∗.\n1 Department of Computer Science and Technology, Institute for AI, BNRist, Tsinghua University\n2 State Key Laboratory of Astronautic dynamics, Xi’an Satellite Control Center\n3 RealAI\nblack hair\ninversion\nsmile\nage\ninput\neyeglasses\nshape\nFigure 1. 3D consistent precise inversion and editing. Our method enables reconstructing texture and geometry from a single real image and\nallows one to perform a list of attributes editing sequentially. The yaw angles of the second to sixth columns are [−30◦, −20◦, 0◦, 20◦, 30◦].\nThe last column is the shape of the sixth column.\nAbstract\nWe study the 3D-aware image attribute editing problem\nin this paper, which has wide applications in practice. Re-\ncent methods solved the problem by training a shared en-\ncoder to map images into a 3D generator’s latent space or\nby per-image latent code optimization and then edited im-\nages in the latent space. Despite their promising results\nnear the input view, they still suffer from the 3D inconsis-\ntency of produced images at large camera poses and im-\nprecise image attribute editing, like affecting unspecified\nattributes during editing. For more efficient image inver-\nsion, we train a shared encoder for all images. To alle-\nviate 3D inconsistency at large camera poses, we propose\ntwo novel methods, an alternating training scheme and a\nmulti-view identity loss, to maintain 3D consistency and\nsubject identity.\nAs for imprecise image editing, we at-\ntribute the problem to the gap between the latent space of\nreal images and that of generated images.\nWe compare\nthe latent space and inversion manifold of GAN models\nand demonstrate that editing in the inversion manifold can\nachieve better results in both quantitative and qualitative\n* Corresponding authors.\nevaluations. Extensive experiments show that our method\nproduces more 3D consistent images and achieves more\nprecise image editing than previous work.\nSource code\nand pretrained models can be found on our project page:\nhttps://mybabyyh.github.io/Preim3D/.\n",
        "question": {
            "statement": "What is a common issue with existing image attribute editing methods?",
            "options": [
                "Requiring multiple input images",
                "Having fast processing speeds",
                "Producing high-quality images",
                "Affecting unspecified attributes during editing"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "2",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Resource-Efficient RGBD Aerial Tracking\nJinyu Yang1,2,†, Shang Gao1,†, Zhe Li1,†, Feng Zheng1,3∗, Aleˇ\ns Leonardis2\n1Southern University of Science and Technology 2University of Birmingham 3Peng Cheng Laboratory\njinyu.yang96@outlook.com\ngaos2021@mail.sustech.edu.cn\nzhe.li.cs@outlook.com\nf.zheng@ieee.org\na.leonardis@cs.bham.ac.uk\nAbstract\nAerial robots are now able to fly in complex environ-\nments, and drone-captured data gains lots of attention in\nobject tracking. However, current research on aerial per-\nception has mainly focused on limited categories, such as\npedestrian or vehicle, and most scenes are captured in ur-\nban environments from a birds-eye view. Recently, UAVs\nequipped with depth cameras have been also deployed for\nmore complex applications, while RGBD aerial tracking\nis still unexplored.\nCompared with traditional RGB ob-\nject tracking, adding depth information can more effec-\ntively deal with more challenging scenes such as target\nand background interference. To this end, in this paper,\nwe explore RGBD aerial tracking in an overhead space,\nwhich can greatly enlarge the development of drone-based\nvisual perception. To boost the research, we first propose a\nlarge-scale benchmark for RGBD aerial tracking, contain-\ning 1,000 drone-captured RGBD videos with dense annota-\ntions. Then, as drone-based applications require for real-\ntime processing with limited computational resources, we\nalso propose an efficient RGBD tracker named EMT. Our\ntracker runs at over 100 fps on GPU, and 25 fps on the edge\nplatform of NVidia Jetson NX Xavier, benefiting from its ef-\nficient multimodal fusion and feature matching. Extensive\nexperiments show that our EMT achieves promising track-\ning performance. All resources are available at https://\ngithub.com/yjybuaa/RGBDAerialTracking.\n",
        "question": {
            "statement": "What is a key advantage of using RGBD (Red, Green, Blue, Depth) object tracking compared to traditional RGB object tracking?",
            "options": [
                "Increased robustness to weather conditions",
                "Dealing more effectively with target and background interference",
                "Improved performance in low-light conditions",
                "Enhanced ability to track fast-moving objects"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "GKEAL: Gaussian Kernel Embedded Analytic Learning for Few-shot Class\nIncremental Task\nHuiping Zhuang1∗, Zhenyu Weng2, Run He1, Zhiping Lin2, Ziqian Zeng1\n1Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China\n2School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore\n∗corresponding:\nhpzhuang@scut.edu.cn\nAbstract\nFew-shot class incremental learning (FSCIL) aims to\naddress catastrophic forgetting during class incremental\nlearning in a few-shot learning setting. In this paper, we\napproach the FSCIL by adopting analytic learning, a tech-\nnique that converts network training into linear problems.\nThis is inspired by the fact that the recursive implemen-\ntation (batch-by-batch learning) of analytic learning gives\nidentical weights to that produced by training on the en-\ntire dataset at once. The recursive implementation and the\nweight-identical property highly resemble the FSCIL setting\n(phase-by-phase learning) and its goal of avoiding catas-\ntrophic forgetting. By bridging the FSCIL with the ana-\nlytic learning, we propose a Gaussian kernel embedded an-\nalytic learning (GKEAL) for FSCIL. The key components\nof GKEAL include the kernel analytic module which allows\nthe GKEAL to conduct FSCIL in a recursive manner, and\nthe augmented feature concatenation module that balances\nthe preference between old and new tasks especially effec-\ntively under the few-shot setting. Our experiments show that\nthe GKEAL gives state-of-the-art performance on several\nbenchmark datasets.\n",
        "question": {
            "statement": "What is the main goal of few-shot class incremental learning?",
            "options": [
                "To increase the number of classes in a classification task",
                "To improve the accuracy of classification models",
                "To avoid catastrophic forgetting during class incremental learning",
                "To reduce the computational cost of model training"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Color Backdoor: A Robust Poisoning Attack in Color Space\nWenbo Jiang1* Hongwei Li1† Guowen Xu2 Tianwei Zhang2\n1University of Electronic Science and Technology of China\n2Nanyang Technological University\nAbstract\nBackdoor attacks against neural networks have been in-\ntensively investigated, where the adversary compromises\nthe integrity of the victim model, causing it to make wrong\npredictions for inference samples containing a specific trig-\nger. To make the trigger more imperceptible and human-\nunnoticeable, a variety of stealthy backdoor attacks have\nbeen proposed, some works employ imperceptible pertur-\nbations as the backdoor triggers, which restrict the pixel\ndifferences of the triggered image and clean image. Some\nworks use special image styles (e.g., reflection, Instagram\nfilter) as the backdoor triggers. However, these attacks sac-\nrifice the robustness, and can be easily defeated by common\npreprocessing-based defenses.\nThis paper presents a novel color backdoor attack,\nwhich can exhibit robustness and stealthiness at the same\ntime. The key insight of our attack is to apply a uniform\ncolor space shift for all pixels as the trigger. This global\nfeature is robust to image transformation operations and the\ntriggered samples maintain natural-looking. To find the op-\ntimal trigger, we first define naturalness restrictions through\nthe metrics of PSNR, SSIM and LPIPS. Then we employ the\nParticle Swarm Optimization (PSO) algorithm to search for\nthe optimal trigger that can achieve high attack effective-\nness and robustness while satisfying the restrictions. Exten-\nsive experiments demonstrate the superiority of PSO and\nthe robustness of color backdoor against different main-\nstream backdoor defenses.\n",
        "question": {
            "statement": "What is a characteristic of the 'color backdoor' attack on neural networks?",
            "options": [
                "It applies a uniform color space shift to all pixels",
                "It requires significant changes to the original image",
                "It relies on adding a visible watermark to the input image",
                "It uses a local feature that is sensitive to image transformations"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "1",
                "0",
                "0"
            ]
        },
        "difference": 9,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Optimal Proposal Learning for Deployable End-to-End Pedestrian Detection\nXiaolin Song1\nBinghui Chen\nPengyu Li\nJun-Yan He\nBiao Wang\nYifeng Geng\nXuansong Xie\nHonggang Zhang1\n1Beijing University of Posts and Telecommunications, Beijing, China\n{sxlshirley, zhhg}@bupt.edu.cn, chenbinghui@bupt.cn, lipengyu007@gmail.com,\njunyanhe1989@gmail.com, wangbiao225@foxmail.com, gengyifeng@gmail.com\nAbstract\nEnd-to-end pedestrian detection focuses on training\na pedestrian detection model via discarding the Non-\nMaximum Suppression (NMS) post-processing. Though a\nfew methods have been explored, most of them still suffer\nfrom longer training time and more complex deployment,\nwhich cannot be deployed in the actual industrial applica-\ntions. In this paper, we intend to bridge this gap and pro-\npose an Optimal Proposal Learning (OPL) framework for\ndeployable end-to-end pedestrian detection.\nSpecifically,\nwe achieve this goal by using CNN-based light detector and\nintroducing two novel modules, including a Coarse-to-Fine\n(C2F) learning strategy for proposing precise positive pro-\nposals for the Ground-Truth (GT) instances by reducing the\nambiguity of sample assignment/output in training/testing\nrespectively, and a Completed Proposal Network (CPN) for\nproducing extra information compensation to further recall\nthe hard pedestrian samples.\nExtensive experiments are\nconducted on CrowdHuman, TJU-Ped and Caltech, and the\nresults show that our proposed OPL method significantly\noutperforms the competing methods.\n",
        "question": {
            "statement": "What is the primary goal of the Optimal Proposal Learning (OPL) framework in pedestrian detection?",
            "options": [
                "To increase the accuracy of pedestrian detection models at the cost of longer training times",
                "To focus solely on detecting pedestrians in crowded scenes",
                "To reduce training time and complexity for deployable end-to-end pedestrian detection",
                "To eliminate the need for Non-Maximum Suppression (NMS) post-processing"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "9",
                "3"
            ]
        },
        "difference": 6,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "RIFormer: Keep Your Vision Backbone Effective But Removing Token Mixer\nJiahao Wang1,2\nSongyang Zhang1∗\nYong Liu3\nTaiqiang Wu3\nYujiu Yang3\nXihui Liu2\nKai Chen1∗\nPing Luo2\nDahua Lin1\n1Shanghai AI Laboratory\n2The University of HongKong\n3Tsinghua Shenzhen International Graduate School\nwang-jh19@tsinghua.org.cn\n{zhangsongyang,chenkai,lindahua}@pjlab.org.cn\nyang.yujiu@sz.tsinghua.edu.cn\nxihuiliu@eee.hku.hk\npluo@cs.hku.hk\nAbstract\nThis paper studies how to keep a vision backbone ef-\nfective while removing token mixers in its basic building\nblocks. Token mixers, as self-attention for vision transform-\ners (ViTs), are intended to perform information communica-\ntion between different spatial tokens but suffer from consid-\nerable computational cost and latency. However, directly\nremoving them will lead to an incomplete model structure\nprior, and thus brings a significant accuracy drop. To this\nend, we first develop an RepIdentityFormer base on the re-\nparameterizing idea, to study the token mixer free model\narchitecture.\nAnd we then explore the improved learn-\ning paradigm to break the limitation of simple token mixer\nfree backbone, and summarize the empirical practice into 5\nguidelines. Equipped with the proposed optimization strat-\negy, we are able to build an extremely simple vision back-\nbone with encouraging performance, while enjoying the\nhigh efficiency during inference. Extensive experiments and\nablative analysis also demonstrate that the inductive bias of\nnetwork architecture, can be incorporated into simple net-\nwork structure with appropriate optimization strategy. We\nhope this work can serve as a starting point for the explo-\nration of optimization-driven efficient network design.\n",
        "question": {
            "statement": "What is a major drawback of token mixers in vision transformers?",
            "options": [
                "incomplete model structure",
                "insufficient learning capacity",
                "inefficient communication between tokens",
                "considerable computational cost and latency"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "2",
                "2",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Camouflaged Instance Segmentation via Explicit De-camouflaging\nNaisong Luo1*, Yuwen Pan1∗, Rui Sun1, Tianzhu Zhang1,2,3†, Zhiwei Xiong1,2, Feng Wu1,2\n1University of Science and Technology of China\n2Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\n3Deep Space Exploration Lab\n{lns6, panyw, issunrui}@mail.ustc.edu.cn, {tzzhang, zwxiong, fengwu}@ustc.edu.cn\nAbstract\nCamouflaged Instance Segmentation (CIS) aims at pre-\ndicting the instance-level masks of camouflaged objects,\nwhich are usually the animals in the wild adapting their ap-\npearance to match the surroundings. Previous instance seg-\nmentation methods perform poorly on this task as they are\neasily disturbed by the deceptive camouflage. To address\nthese challenges, we propose a novel De-camouflaging Net-\nwork (DCNet) including a pixel-level camouflage decou-\npling module and an instance-level camouflage suppression\nmodule. The proposed DCNet enjoys several merits. First,\nthe pixel-level camouflage decoupling module can extract\ncamouflage characteristics based on the Fourier transfor-\nmation. Then a difference attention mechanism is proposed\nto eliminate the camouflage characteristics while reserv-\ning target object characteristics in the pixel feature. Sec-\nond, the instance-level camouflage suppression module can\naggregate rich instance information from pixels by use of\ninstance prototypes. To mitigate the effect of background\nnoise during segmentation, we introduce some reliable ref-\nerence points to build a more robust similarity measure-\nment. With the aid of these two modules, our DCNet can ef-\nfectively model de-camouflaging and achieve accurate seg-\nmentation for camouflaged instances. Extensive experimen-\ntal results on two benchmarks demonstrate that our DCNet\nperforms favorably against state-of-the-art CIS methods,\ne.g., with more than 5% performance gains on COD10K\nand NC4K datasets in average precision.\n",
        "question": {
            "statement": "What is the main challenge faced by traditional instance segmentation methods when dealing with camouflaged objects?",
            "options": [
                "The objects' appearance blends in with the surroundings",
                "The objects are too small to detect",
                "The objects have complex shapes",
                "The objects move quickly in the scene"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "DINER: Depth-aware Image-based NEural Radiance ﬁelds\nMalte Prinzler1,3\nmalte.prinzler@tuebingen.mpg.de\nOtmar Hilliges2\notmar.hilliges@inf.ethz.ch\nJustus Thies1\njustus.thies@tuebingen.mpg.de\n1Max Planck Institute for Intelligent Systems, T¨\nubingen, Germany\n2ETH Z¨\nurich\n3Max Planck ETH Center for Learning Systems\n4 Input Views\nDepth + Features\nVolumetric Reconstruction\nFigure 1. Based on sparse input views, we predict depth and feature maps to infer a volumetric scene representation in terms of a radiance\nﬁeld which enables novel viewpoint synthesis. The depth information allows us to use input views with high relative distance such that the\nscene can be captured more completely and with higher synthesis quality compared to previous state-of-the-art methods.\nAbstract\nWe present Depth-aware Image-based NEural Radiance\nﬁelds (DINER). Given a sparse set of RGB input views, we\npredict depth and feature maps to guide the reconstruction\nof a volumetric scene representation that allows us to ren-\nder 3D objects under novel views. Speciﬁcally, we propose\nnovel techniques to incorporate depth information into fea-\nture fusion and efﬁcient scene sampling. In comparison to\nthe previous state of the art, DINER achieves higher synthe-\nsis quality and can process input views with greater dispar-\nity. This allows us to capture scenes more completely with-\nout changing capturing hardware requirements and ulti-\nmately enables larger viewpoint changes during novel view\nsynthesis. We evaluate our method by synthesizing novel\nviews, both for human heads and for general objects, and\nobserve signiﬁcantly improved qualitative results and in-\ncreased perceptual metrics compared to the previous state\nof the art. The code is publicly available through the Project\nWebpage.\n",
        "question": {
            "statement": "What is the main advantage of using depth information in image-based neural radiance fields?",
            "options": [
                "It limits the amount of viewpoint change possible during novel view synthesis",
                "It allows for the capture of scenes more completely without changing capturing hardware requirements",
                "It reduces the computational cost of novel view synthesis",
                "It eliminates the need for feature maps in scene reconstruction"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "2",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Learning Audio-Visual Source Localization via\nFalse Negative Aware Contrastive Learning\nWeixuan Sun1,5 * , Jiayi Zhang2 ∗, Jianyuan Wang3, Zheyuan Liu1, Yiran Zhong4,\nTianpeng Feng5, Yandong Guo5, Yanhao Zhang5, Nick Barnes1\n1Australian National University, 2Beihang University, 3The University of Oxford,\n4Shanghai AI Lab, 5OPPO Research Institute.\nAbstract\nSelf-supervised audio-visual source localization aims to\nlocate sound-source objects in video frames without extra\nannotations. Recent methods often approach this goal with\nthe help of contrastive learning, which assumes only the au-\ndio and visual contents from the same video are positive\nsamples for each other. However, this assumption would\nsuffer from false negative samples in real-world training.\nFor example, for an audio sample, treating the frames from\nthe same audio class as negative samples may mislead the\nmodel and therefore harm the learned representations (e.g.,\nthe audio of a siren wailing may reasonably correspond to\nthe ambulances in multiple images). Based on this obser-\nvation, we propose a new learning strategy named False\nNegative Aware Contrastive (FNAC) to mitigate the prob-\nlem of misleading the training with such false negative sam-\nples.\nSpecifically, we utilize the intra-modal similarities\nto identify potentially similar samples and construct corre-\nsponding adjacency matrices to guide contrastive learning.\nFurther, we propose to strengthen the role of true negative\nsamples by explicitly leveraging the visual features of sound\nsources to facilitate the differentiation of authentic sound-\ning source regions. FNAC achieves state-of-the-art perfor-\nmances on Flickr-SoundNet, VGG-Sound, and AVSBench,\nwhich demonstrates the effectiveness of our method in mit-\nigating the false negative issue. The code is available at\nhttps://github.com/OpenNLPLab/FNAC_AVL.\n",
        "question": {
            "statement": "What is a common problem in self-supervised audio-visual source localization approaches that assume audio and visual contents from the same video are positive samples for each other?",
            "options": [
                "Inability to handle multi-modal inputs",
                "Treating frames from the same audio class as negative samples",
                "Insufficient training data",
                "High computational requirements"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "8",
                "0",
                "0"
            ]
        },
        "difference": 6,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "MagicNet: Semi-Supervised Multi-Organ Segmentation via Magic-Cube\nPartition and Recovery\nDuowen Chen1\nYunhao Bai1\nWei Shen2\nQingli Li1\nLequan Yu3\nYan Wang1*\n1Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University\n2MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n3The University of Hong Kong\nduowen chen@hotmail.com, yhbai@stu.ecnu.edu.cn, wei.shen@sjtu.edu.cn,\nqlli@cs.ecnu.edu.cn, lqyu@hku.hk, ywang@cee.ecnu.edu.cn\nAbstract\nWe propose a novel teacher-student model for semi-\nsupervised multi-organ segmentation.\nIn teacher-student\nmodel, data augmentation is usually adopted on unlabeled\ndata to regularize the consistent training between teacher\nand student. We start from a key perspective that fixed rela-\ntive locations and variable sizes of different organs can pro-\nvide distribution information where a multi-organ CT scan\nis drawn. Thus, we treat the prior anatomy as a strong tool\nto guide the data augmentation and reduce the mismatch\nbetween labeled and unlabeled images for semi-supervised\nlearning. More specifically, we propose a data augmen-\ntation strategy based on partition-and-recovery N3 cubes\ncross- and within- labeled and unlabeled images. Our strat-\negy encourages unlabeled images to learn organ semantics\nin relative locations from the labeled images (cross-branch)\nand enhances the learning ability for small organs (within-\nbranch). For within-branch, we further propose to refine the\nquality of pseudo labels by blending the learned representa-\ntions from small cubes to incorporate local attributes. Our\nmethod is termed as MagicNet, since it treats the CT vol-\nume as a magic-cube and N3-cube partition-and-recovery\nprocess matches with the rule of playing a magic-cube. Ex-\ntensive experiments on two public CT multi-organ datasets\ndemonstrate the effectiveness of MagicNet, and noticeably\noutperforms state-of-the-art semi-supervised medical im-\nage segmentation approaches, with +7% DSC improve-\nment on MACT dataset with 10% labeled images. Code is\navaiable at https://github.com/DeepMed-Lab-\nECNU/MagicNet.\n",
        "question": {
            "statement": "What is the main idea behind the proposed data augmentation strategy in MagicNet?",
            "options": [
                "Using prior anatomical knowledge to guide data augmentation",
                "Ignoring the size and location of organs during data augmentation",
                "Randomly applying transformations to both labeled and unlabeled images",
                "Only using labeled images for data augmentation"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "9",
                "2",
                "2",
                "1"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "PEAL: Prior-embedded Explicit Attention Learning for Low-overlap Point\nCloud Registration\nJunle Yu1\nLuwei Ren1\nWenhui Zhou1∗\nYu Zhang2∗\nLili Lin3\nGuojun Dai1\n1Hangzhou Dianzi University\n2Shanghai Jiaotong University\n3Zhejiang Gongshang University\nAbstract\nLearning distinctive point-wise features is critical for\nlow-overlap point cloud registration.\nRecently, it has\nachieved huge success in incorporating Transformer into\npoint cloud feature representation, which usually adopts\na self-attention module to learn intra-point-cloud features\nfirst, then utilizes a cross-attention module to perform fea-\nture exchange between input point clouds.\nThe advan-\ntage of Transformer models mainly benefits from the use\nof self-attention to capture the global correlations in fea-\nture space.\nHowever, these global correlations may in-\nvolve ambiguity for point cloud registration task, espe-\ncially in indoor low-overlap scenarios, because the corre-\nlations with an extensive range of non-overlapping points\nmay degrade the feature distinctiveness. To address this is-\nsue, we present PEAL, a Prior-embedded Explicit Attention\nLearning model. By incorporating prior knowledge into\nthe learning process, the points are divided into two parts.\nOne includes points lying in the putative overlapping region\nand the other includes points located in the putative non-\noverlapping region. Then PEAL explicitly learns one-way\nattention with the putative overlapping points. This simplis-\ntic design attains surprising performance, significantly re-\nlieving the aforementioned feature ambiguity. Our method\nimproves the Registration Recall by 6+% on the challenging\n3DLoMatch benchmark and achieves state-of-the-art per-\nformance on Feature Matching Recall, Inlier Ratio, and\nRegistration Recall on both 3DMatch and 3DLoMatch.\n",
        "question": {
            "statement": "What is a limitation of using self-attention modules in Transformer models for point cloud registration tasks?",
            "options": [
                "They are computationally expensive",
                "They require large amounts of training data",
                "They may involve ambiguity due to correlations with non-overlapping points",
                "They are not suitable for outdoor scenarios"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Behavioral Analysis of Vision-and-Language Navigation Agents\nZijiao Yang\nOregon State University\nyangziji@oregonstate.edu\nArjun Majumdar\nGeorgia Institute of Technology\narjun.majumdar@gatech.edu\nStefan Lee\nOregon State University\nleestef@oregonstate.edu\nAbstract\nTo be successful,\nVision-and-Language Navigation\n(VLN) agents must be able to ground instructions to ac-\ntions based on their surroundings. In this work, we develop\na methodology to study agent behavior on a skill-specific\nbasis – examining how well existing agents ground instruc-\ntions about stopping, turning, and moving towards speci-\nfied objects or rooms. Our approach is based on gener-\nating skill-specific interventions and measuring changes in\nagent predictions. We present a detailed case study analyz-\ning the behavior of a recent agent and then compare multi-\nple agents in terms of skill-specific competency scores. This\nanalysis suggests that biases from training have lasting ef-\nfects on agent behavior and that existing models are able to\nground simple referring expressions. Our comparisons be-\ntween models show that skill-specific scores correlate with\nimprovements in overall VLN task performance.\n",
        "question": {
            "statement": "What is a key challenge for Vision-and-Language Navigation (VLN) agents to be successful?",
            "options": [
                "Grounding instructions to actions based on their surroundings",
                "Understanding natural language instructions",
                "Navigating through unfamiliar environments",
                "Recognizing visual objects in the environment"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "2",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "COT: Unsupervised Domain Adaptation with Clustering and Optimal Transport\nYang Liu 1 *\nZhipeng Zhou 1 *\nBaigui Sun 1 †\n1Alibaba Group\nAbstract\nUnsupervised domain adaptation (UDA) aims to trans-\nfer the knowledge from a labeled source domain to an\nunlabeled target domain.\nTypically, to guarantee desir-\nable knowledge transfer, aligning the distribution between\nsource and target domain from a global perspective is\nwidely adopted in UDA. Recent researchers further point\nout the importance of local-level alignment and propose to\nconstruct instance-pair alignment by leveraging on Optimal\nTransport (OT) theory. However, existing OT-based UDA\napproaches are limited to handling class imbalance chal-\nlenges and introduce a heavy computation overhead when\nconsidering a large-scale training situation. To cope with\ntwo aforementioned issues, we propose a Clustering-based\nOptimal Transport (COT) algorithm, which formulates the\nalignment procedure as an Optimal Transport problem and\nconstructs a mapping between clustering centers in the\nsource and target domain via an end-to-end manner. With\nthis alignment on clustering centers, our COT eliminates\nthe negative effect caused by class imbalance and reduces\nthe computation cost simultaneously. Empirically, our COT\nachieves state-of-the-art performance on several authorita-\ntive benchmark datasets.\n",
        "question": {
            "statement": "What is a common approach in unsupervised domain adaptation to ensure desirable knowledge transfer?",
            "options": [
                "Using only labeled data from the target domain",
                "Ignoring the difference between source and target domains",
                "Applying the same model to both source and target domains",
                "Aligning the distribution between source and target domains"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Mutual Information-Based Temporal Difference Learning for\nHuman Pose Estimation in Video\nRunyang Feng1,2, Yixing Gao1,2*\n, Xueqing Ma1,2, Tze Ho Elden Tse3, Hyung Jin Chang3\n1 School of Artificial Intelligence, Jilin University,\n2 Engineering Research Center of Knowledge-Driven Human-Machine Intelligence,\nMinistry of Education, China, 3School of Computer Science, University of Birmingham\n{fengry22, maxq21}@mails.jlu.edu.cn, gaoyixing@jlu.edu.cn,\ntxt994@student.bham.ac.uk, h.j.chang@bham.ac.uk\nAbstract\nTemporal modeling is crucial for multi-frame human\npose estimation.\nMost existing methods directly employ\noptical flow or deformable convolution to predict full-\nspectrum motion fields, which might incur numerous irrele-\nvant cues, such as a nearby person or background. Without\nfurther efforts to excavate meaningful motion priors, their\nresults are suboptimal, especially in complicated spatio-\ntemporal interactions.\nOn the other hand, the temporal\ndifference has the ability to encode representative motion\ninformation which can potentially be valuable for pose es-\ntimation but has not been fully exploited. In this paper, we\npresent a novel multi-frame human pose estimation frame-\nwork, which employs temporal differences across frames to\nmodel dynamic contexts and engages mutual information\nobjectively to facilitate useful motion information disen-\ntanglement. To be specific, we design a multi-stage Tem-\nporal Difference Encoder that performs incremental cas-\ncaded learning conditioned on multi-stage feature differ-\nence sequences to derive informative motion representa-\ntion. We further propose a Representation Disentanglement\nmodule from the mutual information perspective, which can\ngrasp discriminative task-relevant motion signals by explic-\nitly defining useful and noisy constituents of the raw motion\nfeatures and minimizing their mutual information. These\nplace us to rank No.1 in the Crowd Pose Estimation in Com-\nplex Events Challenge on benchmark dataset HiEve, and\nachieve state-of-the-art performance on three benchmarks\nPoseTrack2017, PoseTrack2018, and PoseTrack21.\n",
        "question": {
            "statement": "What is a limitation of using optical flow or deformable convolution for multi-frame human pose estimation?",
            "options": [
                "They are unable to model dynamic contexts",
                "They may capture irrelevant cues",
                "They are computationally expensive",
                "They require manual annotation"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "StyleSync: High-Fidelity Generalized and Personalized Lip Sync\nin Style-based Generator\nJiazhi Guan1,2*\nZhanwang Zhang1*\nHang Zhou1†\nTianshu Hu1†\nKaisiyuan Wang3\nDongliang He1\nHaocheng Feng1\nJingtuo Liu1\nErrui Ding1\nZiwei Liu4\nJingdong Wang1\n1Department of Computer Vision Technology (VIS), Baidu Inc.\n2Tsinghua University\n3The University of Sydney\n4S-Lab, Nanyang Technological University\nguanjz20@mails.tsinghua.edu.cn {zhangzhanwang,zhouhang09,hutianshu01}@baidu.com\nand\nTarget\nTemplate\nLip-Synced \nVideo\nPersonalized \nLip-Sync \nResults\nConditional \nAudio\nFigure 1. Personalized lip-sync results generated by our StyleSync framework. Our method not only supports high-fidelity modification\nto any target template video according to conditional audio but can further adapt to specific styles with personalized optimization. In this\nfigure, our lip-sync results should have the same mouth shapes as the lip-synced video of the conditional audio.\nAbstract\nDespite recent advances in syncing lip movements with\nany audio waves, current methods still struggle to balance\ngeneration quality and the model’s generalization ability.\nPrevious studies either require long-term data for training\nor produce a similar movement pattern on all subjects with\nlow quality. In this paper, we propose StyleSync, an effec-\ntive framework that enables high-fidelity lip synchroniza-\ntion. We identify that a style-based generator would suffi-\nciently enable such a charming property on both one-shot\n*Equal contribution.\n†Corresponding authors.\nand few-shot scenarios.\nSpecifically, we design a mask-\nguided spatial information encoding module that preserves\nthe details of the given face. The mouth shapes are accu-\nrately modified by audio through modulated convolutions.\nMoreover, our design also enables personalized lip-sync by\nintroducing style space and generator refinement on only\nlimited frames. Thus the identity and talking style of a tar-\nget person could be accurately preserved. Extensive ex-\nperiments demonstrate the effectiveness of our method in\nproducing high-fidelity results on a variety of scenes. Re-\nsources can be found at https://hangz-nju-cuhk.\ngithub.io/projects/StyleSync.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1505\n",
        "question": {
            "statement": "What is the main advantage of using a style-based generator in lip synchronization?",
            "options": [
                "It requires a large amount of training data",
                "It produces similar movement patterns on all subjects",
                "It only works well in one-shot scenarios",
                "It enables high-fidelity lip synchronization while preserving the identity and talking style of a target person"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Rethinking Few-Shot Medical Segmentation:\nA Vector Quantization View\nShiqi Huang, Tingfa Xu*\n, Ning Shen, Feng Mu, Jianan Li*\nBeijing Institute of Technology\n{bitsqhuang,lijianan15}@gmail.com, {ciom xtf1, bitmufeng}@bit.edu.cn, shennbit@163.com\nAbstract\nThe existing few-shot medical segmentation networks\nshare the same practice that the more prototypes, the bet-\nter performance. This phenomenon can be theoretically in-\nterpreted in Vector Quantization (VQ) view: the more pro-\ntotypes, the more clusters are separated from pixel-wise\nfeature points distributed over the full space.\nHowever,\nas we further think about few-shot segmentation with this\nperspective, it is found that the clusterization of feature\npoints and the adaptation to unseen tasks have not received\nenough attention. Motivated by the observation, we propose\na learning VQ mechanism consisting of grid-format VQ\n(GFVQ), self-organized VQ (SOVQ) and residual oriented\nVQ (ROVQ). To be specific, GFVQ generates the prototype\nmatrix by averaging square grids over the spatial extent,\nwhich uniformly quantizes the local details; SOVQ adap-\ntively assigns the feature points to different local classes\nand creates a new representation space where the learn-\nable local prototypes are updated with a global view; ROVQ\nintroduces residual information to fine-tune the aforemen-\ntioned learned local prototypes without re-training, which\nbenefits the generalization performance for the irrelevance\nto the training task.\nWe empirically show that our VQ\nframework yields the state-of-the-art performance over ab-\ndomen, cardiac and prostate MRI datasets and expect this\nwork will provoke a rethink of the current few-shot medical\nsegmentation model design. Our code will soon be publicly\navailable.\n",
        "question": {
            "statement": "What is the main limitation of traditional few-shot medical segmentation networks?",
            "options": [
                "They focus on increasing the number of prototypes without considering clusterization and adaptation",
                "They require extensive computational resources",
                "They rely heavily on large amounts of labeled data",
                "They are limited to segmenting specific organs or tissues"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "On the Benefits of 3D Pose and Tracking for Human Action Recognition\nJathushan Rajasegaran1,2, Georgios Pavlakos1, Angjoo Kanazawa1, Christoph Feichtenhofer2, Jitendra Malik1,2\n1UC Berkeley, 2Meta AI, FAIR\nAbstract\nIn this work we study the benefits of using tracking and\n3D poses for action recognition. To achieve this, we take the\nLagrangian view on analysing actions over a trajectory of\nhuman motion rather than at a fixed point in space. Taking\nthis stand allows us to use the tracklets of people to pre-\ndict their actions. In this spirit, first we show the benefits of\nusing 3D pose to infer actions, and study person-person in-\nteractions. Subsequently, we propose a Lagrangian Action\nRecognition model by fusing 3D pose and contextualized\nappearance over tracklets. To this end, our method achieves\nstate-of-the-art performance on the AVA v2.2 dataset on\nboth pose only settings and on standard benchmark settings.\nWhen reasoning about the action using only pose cues, our\npose model achieves +10.0 mAP gain over the correspond-\ning state-of-the-art while our fused model has a gain of +2.8\nmAP over the best state-of-the-art model. Code and results\nare available at: https://brjathu.github.io/LART\n",
        "question": {
            "statement": "What approach to analyzing human motion is taken in the Lagrangian view?",
            "options": [
                "Analyzing only the starting and ending points of motion",
                "Analyzing actions over a trajectory of motion rather than at a fixed point in space",
                "Analyzing actions at a fixed point in space rather than over a trajectory of motion",
                "Analyzing individual body parts separately rather than together"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Flow supervision for Deformable NeRF\nChaoyang Wang1\nLachlan Ewen MacDonald2\nLaszlo A. Jeni1\nSimon Lucey12\n1Carnegie Mellon University\n2University of Adelaide\n{chaoyanw,laszlojeni}@cs.cmu.edu\n{lachlan.macdonald, simon.lucey}@adelaide.edu.au\nhttps://mightychaos.github.io/projects/fsdnerf\nAbstract\nIn this paper we present a new method for deformable\nNeRF that can directly use optical flow as supervision. We\novercome the major challenge with respect to the compu-\ntationally inefficiency of enforcing the flow constraints to\nthe backward deformation field, used by deformable NeRFs.\nSpecifically, we show that inverting the backward deforma-\ntion function is actually not needed for computing scene\nflows between frames. This insight dramatically simplifies\nthe problem, as one is no longer constrained to deformation\nfunctions that can be analytically inverted. Instead, thanks\nto the weak assumptions required by our derivation based\non the inverse function theorem, our approach can be ex-\ntended to a broad class of commonly used backward defor-\nmation field. We present results on monocular novel view\nsynthesis with rapid object motion, and demonstrate signifi-\ncant improvements over baselines without flow supervision.\n",
        "question": {
            "statement": "What is a major challenge in using optical flow as supervision for deformable NeRF?",
            "options": [
                "Insufficient training data",
                "Inability to handle complex scenes",
                "Limited hardware capabilities",
                "Computational inefficiency of enforcing flow constraints"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Learnable Skeleton-Aware 3D Point Cloud Sampling\nCheng Wen*\nBaosheng Yu*\nDacheng Tao\nSchool of Computer Science, The University of Sydney, NSW 2008, Australia\ncwen6671@uni.sydney.edu.au; baosheng.yu@sydney.edu.au; dacheng.tao@gmail.com\nAbstract\nPoint cloud sampling is crucial for efficient large-scale\npoint cloud analysis, where learning-to-sample methods\nhave recently received increasing attention from the com-\nmunity for jointly training with downstream tasks. However,\nthe above-mentioned task-specific sampling methods usu-\nally fail to explore the geometries of objects in an explicit\nmanner. In this paper, we introduce a new skeleton-aware\nlearning-to-sample method by learning object skeletons as\nthe prior knowledge to preserve the object geometry and\ntopology information during sampling. Specifically, with-\nout labor-intensive annotations per object category, we first\nlearn category-agnostic object skeletons via the medial axis\ntransform definition in an unsupervised manner. With ob-\nject skeleton, we then evaluate the histogram of the local\nfeature size as the prior knowledge to formulate skeleton-\naware sampling from a probabilistic perspective. Addition-\nally, the proposed skeleton-aware sampling pipeline with\nthe task network is thus end-to-end trainable by explor-\ning the reparameterization trick. Extensive experiments on\nthree popular downstream tasks, point cloud classification,\nretrieval, and reconstruction, demonstrate the effectiveness\nof the proposed method for efficient point cloud analysis.\n",
        "question": {
            "statement": "What is a limitation of traditional task-specific point cloud sampling methods?",
            "options": [
                "They require a large amount of annotated data",
                "They are limited to specific object categories",
                "They are not suitable for real-time applications",
                "They fail to explicitly consider the geometry and topology of objects"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Learning Multi-Modal Class-Specific Tokens for Weakly Supervised Dense\nObject Localization\nLian Xu1, Wanli Ouyang2, Mohammed Bennamoun1, Farid Boussaid1, and Dan Xu3\n1The University of Western Australia\n2Shanghai AI Laboratory\n3Hong Kong University of Science and Technology\n{lian.xu,mohammed.bennamoun,farid.boussaid}@uwa.edu.au,\nwanli.ouyang@sydney.edu.au, danxu@cse.ust.hk\nAbstract\nWeakly supervised dense object localization (WSDOL)\nrelies generally on Class Activation Mapping (CAM), which\nexploits the correlation between the class weights of the im-\nage classifier and the pixel-level features. Due to the lim-\nited ability to address intra-class variations, the image clas-\nsifier cannot properly associate the pixel features, leading\nto inaccurate dense localization maps. In this paper, we\npropose to explicitly construct multi-modal class represen-\ntations by leveraging the Contrastive Language-Image Pre-\ntraining (CLIP), to guide dense localization. More specifi-\ncally, we propose a unified transformer framework to learn\ntwo-modalities of class-specific tokens, i.e., class-specific\nvisual and textual tokens. The former captures semantics\nfrom the target visual data while the latter exploits the class-\nrelated language priors from CLIP, providing complemen-\ntary information to better perceive the intra-class diversi-\nties.\nIn addition, we propose to enrich the multi-modal\nclass-specific tokens with sample-specific contexts compris-\ning visual context and image-language context. This en-\nables more adaptive class representation learning, which\nfurther facilitates dense localization. Extensive experiments\nshow the superiority of the proposed method for WSDOL on\ntwo multi-label datasets, i.e., PASCAL VOC and MS COCO,\nand one single-label dataset, i.e., OpenImages. Our dense\nlocalization maps also lead to the state-of-the-art weakly\nsupervised semantic segmentation (WSSS) results on PAS-\nCAL VOC and MS COCO. 1\n",
        "question": {
            "statement": "What is a limitation of traditional Class Activation Mapping (CAM) approaches in weakly supervised dense object localization?",
            "options": [
                "insufficient training data",
                " inability to address intra-class variations",
                "requirement for manual annotation",
                "high computational complexity"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Histopathology Whole Slide Image Analysis with Heterogeneous Graph\nRepresentation Learning\nTsai Hor Chan1,∗, Fernando Julio Cendra1,2∗, Lan Ma2, Guosheng Yin1,3, Lequan Yu1\n1Department of Statistics and Actuarial Science, The University of Hong Kong\n2TCL Corporate Research Hong Kong\n3Department of Mathematics, Imperial College London\n{hchanth, fcendra}@connect.hku.hk, rubyma@tcl.com, guosheng.yin@imperial.ac.uk, lqyu@hku.hk\nAbstract\nGraph-based methods have been extensively applied to\nwhole slide histopathology image (WSI) analysis due to the\nadvantage of modeling the spatial relationships among dif-\nferent entities. However, most of the existing methods fo-\ncus on modeling WSIs with homogeneous graphs (e.g., with\nhomogeneous node type).\nDespite their successes, these\nworks are incapable of mining the complex structural re-\nlations between biological entities (e.g., the diverse inter-\naction among different cell types) in the WSI. We propose\na novel heterogeneous graph-based framework to lever-\nage the inter-relationships among different types of nu-\nclei for WSI analysis. Specifically, we formulate the WSI\nas a heterogeneous graph with “nucleus-type” attribute to\neach node and a semantic similarity attribute to each edge.\nWe then present a new heterogeneous-graph edge attribute\ntransformer (HEAT) to take advantage of the edge and\nnode heterogeneity during massage aggregating. Further,\nwe design a new pseudo-label-based semantic-consistent\npooling mechanism to obtain graph-level features, which\ncan mitigate the over-parameterization issue of conven-\ntional cluster-based pooling. Additionally, observing the\nlimitations of existing association-based localization meth-\nods, we propose a causal-driven approach attributing the\ncontribution of each node to improve the interpretability\nof our framework.\nExtensive experiments on three pub-\nlic TCGA benchmark datasets demonstrate that our frame-\nwork outperforms the state-of-the-art methods with consid-\nerable margins on various tasks. Our codes are available\nat https://github.com/HKU-MedAI/WSI-HGNN.\n",
        "question": {
            "statement": "What is a limitation of conventional cluster-based pooling methods in whole slide image analysis?",
            "options": [
                "interpretability of results",
                "data quality issues",
                "over-parameterization",
                "computational complexity"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Revisiting Prototypical Network for Cross Domain Few-Shot Learning\nFei Zhou1\nPeng Wang2*\nLei Zhang1†\nWei Wei1\nYanning Zhang1\n1 Northwestern Polytechnical University\n2 University of Wollonggong\nzhoufei@mail.nwpu.edu.cn\npengw@uow.edu.au\n{nwpuzhanglei,weiweinwpu,ynzhang}@nwpu.edu.cn\nAbstract\nPrototypical Network is a popular few-shot solver that\naims at establishing a feature metric generalizable to novel\nfew-shot classiﬁcation (FSC) tasks using deep neural net-\nworks. However, its performance drops dramatically when\ngeneralizing to the FSC tasks in new domains. In this study,\nwe revisit this problem and argue that the devil lies in the\nsimplicity bias pitfall in neural networks. In speciﬁc, the\nnetwork tends to focus on some biased shortcut features\n(e.g., color, shape, etc.) that are exclusively sufﬁcient to dis-\ntinguish very few classes in the meta-training tasks within\na pre-deﬁned domain, but fail to generalize across domains\nas some desirable semantic features. To mitigate this prob-\nlem, we propose a Local-global Distillation Prototypical\nNetwork (LDP-net). Different from the standard Prototypi-\ncal Network, we establish a two-branch network to classify\nthe query image and its random local crops, respectively.\nThen, knowledge distillation is conducted among these two\nbranches to enforce their class afﬁliation consistency. The\nrationale behind is that since such global-local semantic re-\nlationship is expected to hold regardless of data domains,\nthe local-global distillation is beneﬁcial to exploit some\ncross-domain transferable semantic features for feature\nmetric establishment. Moreover, such local-global seman-\ntic consistency is further enforced among different images of\nthe same class to reduce the intra-class semantic variation\nof the resultant feature. In addition, we propose to update\nthe local branch as Exponential Moving Average (EMA)\nover training episodes, which makes it possible to better\ndistill cross-episode knowledge and further enhance the\ngeneralization performance. Experiments on eight cross-\ndomain FSC benchmarks empirically clarify our argument\nand show the state-of-the-art results of LDP-net. Code is\navailable in https://github.com/NWPUZhoufei/LDP-Net\n*F. Zhou and P. Wang contributed equally in this work.\n†Corresponding author.\n",
        "question": {
            "statement": "What is a major limitation of prototypical networks in few-shot learning?",
            "options": [
                "They are limited to classification tasks only",
                "They are not suitable for real-time applications",
                "They require large amounts of labeled data",
                "They tend to focus on biased shortcut features that do not generalize well across domains"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Aligning Bag of Regions for Open-Vocabulary Object Detection\nSize Wu1\nWenwei Zhang1\nSheng Jin2,3\nWentao Liu3,4\nChen Change Loy1*\n1S-Lab, Nanyang Technological University\n2 The University of Hong Kong\n3 SenseTime Research and Tetras.AI\n4 Shanghai AI Laboratory\n{size001, wenwei001, ccloy}@ntu.edu.sg\n{jinsheng, liuwentao}@sensetime.com\nAbstract\nPre-trained vision-language models (VLMs) learn to\nalign vision and language representations on large-scale\ndatasets, where each image-text pair usually contains a bag\nof semantic concepts. However, existing open-vocabulary\nobject detectors only align region embeddings individually\nwith the corresponding features extracted from the VLMs.\nSuch a design leaves the compositional structure of semantic\nconcepts in a scene under-exploited, although the structure\nmay be implicitly learned by the VLMs. In this work, we\npropose to align the embedding of bag of regions beyond in-\ndividual regions. The proposed method groups contextually\ninterrelated regions as a bag. The embeddings of regions\nin a bag are treated as embeddings of words in a sentence,\nand they are sent to the text encoder of a VLM to obtain the\nbag-of-regions embedding, which is learned to be aligned\nto the corresponding features extracted by a frozen VLM.\nApplied to the commonly used Faster R-CNN, our approach\nsurpasses the previous best results by 4.6 box AP50 and 2.8\nmask AP on novel categories of open-vocabulary COCO\nand LVIS benchmarks, respectively. Code and models are\navailable at https://github.com/wusize/ovdet.\n",
        "question": {
            "statement": "What is a limitation of existing open-vocabulary object detectors when using pre-trained vision-language models?",
            "options": [
                "They only align individual region embeddings with corresponding features",
                "They are not compatible with Faster R-CNN architecture",
                "They require additional training data",
                "They cannot handle novel categories"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "9",
                "0",
                "0",
                "3"
            ]
        },
        "difference": 6,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Integral Neural Networks\nKirill Solodskikh*†\nAzim Kurbanov*†\nRuslan Aydarkhanov†\nIrina Zhelavskaya\nYury Parfenov\nDehua Song\nStamatios Lefkimmiatis\nHuawei Noah’s Ark Lab\n{kirillceo, azimcto, ruslancto}@garch.me\n{zhelavskaya.irina1, parfenov.yury, dehua.song, stamatios.lefkimmiatis}@huawei.com\nAbstract\nWe introduce a new family of deep neural networks,\nwhere instead of the conventional representation of net-\nwork layers as N-dimensional weight tensors, we use a\ncontinuous layer representation along the filter and chan-\nnel dimensions. We call such networks Integral Neural Net-\nworks (INNs). In particular, the weights of INNs are rep-\nresented as continuous functions defined on N-dimensional\nhypercubes, and the discrete transformations of inputs to\nthe layers are replaced by continuous integration opera-\ntions, accordingly. During the inference stage, our con-\ntinuous layers can be converted into the traditional tensor\nrepresentation via numerical integral quadratures.\nSuch\nkind of representation allows the discretization of a net-\nwork to an arbitrary size with various discretization in-\ntervals for the integral kernels. This approach can be ap-\nplied to prune the model directly on an edge device while\nsuffering only a small performance loss at high rates of\nstructural pruning without any fine-tuning. To evaluate the\npractical benefits of our proposed approach, we have con-\nducted experiments using various neural network architec-\ntures on multiple tasks. Our reported results show that the\nproposed INNs achieve the same performance with their\nconventional discrete counterparts, while being able to pre-\nserve approximately the same performance (2% accuracy\nloss for ResNet18 on Imagenet) at a high rate (up to 30%)\nof structural pruning without fine-tuning, compared to 65%\naccuracy loss of the conventional pruning methods under\nthe same conditions. Code is available at gitee.\n",
        "question": {
            "statement": "What is the main advantage of using Integral Neural Networks (INNs) over traditional neural networks?",
            "options": [
                "They are more interpretable than traditional neural networks",
                "They allow for efficient pruning of the model without significant loss of accuracy",
                "They are only applicable to specific domains such as computer vision",
                "They require less computational resources for training"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Structured Kernel Estimation for Photon-Limited Deconvolution\nYash Sanghvi, Zhiyuan Mao, Stanley H. Chan\nSchool of Electrical and Computer Engineering, Purdue University\n{ysanghvi, mao114, stanchan}@purdue.edu\nAbstract\nImages taken in a low light condition with the pres-\nence of camera shake suffer from motion blur and photon\nshot noise.\nWhile state-of-the-art image restoration net-\nworks show promising results, they are largely limited to\nwell-illuminated scenes and their performance drops sig-\nnificantly when photon shot noise is strong.\nIn this paper, we propose a new blur estimation tech-\nnique customized for photon-limited conditions. The pro-\nposed method employs a gradient-based backpropagation\nmethod to estimate the blur kernel. By modeling the blur\nkernel using a low-dimensional representation with the\nkey points on the motion trajectory, we significantly re-\nduce the search space and improve the regularity of the\nkernel estimation problem.\nWhen plugged into an iter-\native framework, our novel low-dimensional representa-\ntion provides improved kernel estimates and hence signifi-\ncantly better deconvolution performance when compared to\nend-to-end trained neural networks. The source code and\npretrained mdoels are available at https://github.\ncom/sanghviyashiitb/structured- kernel-\ncvpr23\n",
        "question": {
            "statement": "What is a major limitation of current state-of-the-art image restoration networks?",
            "options": [
                "They are unable to handle complex motion trajectories",
                "They are sensitive to camera type",
                "They are largely limited to well-illuminated scenes",
                "They require a large amount of computational resources"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Where is My Spot? Few-shot Image Generation via\nLatent Subspace Optimization\nChenxi Zheng 1* Bangzhen Liu 1* Huaidong Zhang 1† Xuemiao Xu1,2,3,4† Shengfeng He 5\n1South China University of Technology\n2State Key Laboratory of Subtropical Building Science\n3Ministry of Education Key Laboratory of Big Data and Intelligent Robot\n4Guangdong Provincial Key Lab of Computational Intelligence and Cyberspace Information\n5Singapore Management University\n{cszcx, cs liubz}@mail.scut.edu.cn, {huaidongz, xuemx}@scut.edu.cn, shengfenghe@smu.edu.sg\n(a) 3-shot Input\n(b) AGE [7]\n(c) WaveGAN [44]\n(d) Ours\nFigure 1. We propose a new image synthesis approach that allows generating diverse unseen results with only 1- or 3-shot samples. Our\nkey idea is to exploit the continuity of the StyleGAN latent space, and further empower it to generate unseen categories via latent subspace\noptimization.\nAbstract\nImage generation relies on massive training data that\ncan hardly produce diverse images of an unseen category\naccording to a few examples.\nIn this paper, we address\nthis dilemma by projecting sparse few-shot samples into a\ncontinuous latent space that can potentially generate in-\nfinite unseen samples.\nThe rationale behind is that we\naim to locate a centroid latent position in a conditional\nStyleGAN, where the corresponding output image on that\ncentroid can maximize the similarity with the given sam-\nples. Although the given samples are unseen for the con-\nditional StyleGAN, we assume the neighboring latent sub-\nspace around the centroid belongs to the novel category,\nand therefore introduce two latent subspace optimization\nobjectives. In the first one we use few-shot samples as pos-\nitive anchors of the novel class, and adjust the StyleGAN to\nproduce the corresponding results with the new class label\ncondition. The second objective is to govern the genera-\ntion process from the other way around, by altering the cen-\n*Equal Contributions.\n†Corresponding authors.\ntroid and its surrounding latent subspace for a more pre-\ncise generation of the novel class. These reciprocal opti-\nmization objectives inject a novel class into the StyleGAN\nlatent subspace, and therefore new unseen samples can be\neasily produced by sampling images from it. Extensive ex-\nperiments demonstrate superior few-shot generation perfor-\nmances compared with state-of-the-art methods, especially\nin terms of diversity and generation quality. Code is avail-\nable at https://github.com/chansey0529/LSO.\n",
        "question": {
            "statement": "What is the main goal of the proposed method in image generation?",
            "options": [
                "To develop a new architecture for image generation models",
                "To generate diverse unseen images with only a few samples",
                "To reduce the computational cost of image generation models",
                "To improve the quality of generated images using large datasets"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Revisiting Rolling Shutter Bundle Adjustment:\nToward Accurate and Fast Solution\nBangyan Liao1,2,∗\nDelin Qu1,3,∗\nYifei Xue4\nHuiqing Zhang1\nYizhen Lao1,†\n1College of Computer Science and Electronic Engineering, Hunan University\n2College of Electrical and Information Engineering, Hunan University\n3Shanghai AI Laboratory\n4Jiangxi Provincial Natural Resources Cause Development Center\nAbstract\nWe propose an accurate and fast bundle adjustment (BA)\nsolution that estimates the 6-DoF pose with an independent\nRS model of the camera and the geometry of the environ-\nment based on measurements from a rolling shutter (RS)\ncamera. This tackles the challenges in the existing works,\nnamely, relying on high frame rate video as input, restric-\ntive assumptions on camera motion and poor efﬁciency. To\nthis end, we ﬁrst verify the positive inﬂuence of the image\npoint normalization to RSBA. Then we present a novel vi-\nsual residual covariance model to standardize the repro-\njection error during RSBA, which consequently improves\nthe overall accuracy. Besides, we demonstrate the com-\nbination of Normalization and covariance standardization\nWeighting in RSBA (NW-RSBA) can avoid common planar\ndegeneracy without the need to constrain the ﬁlming man-\nner. Finally, we propose an acceleration strategy for NW-\nRSBA based on the sparsity of its Jacobian matrix and Schur\ncomplement. The extensive synthetic and real data experi-\nments verify the effectiveness and efﬁciency of the proposed\nsolution over the state-of-the-art works.\n",
        "question": {
            "statement": "What is a key challenge in traditional rolling shutter bundle adjustment methods?",
            "options": [
                "Inadequate storage capacity",
                "Limited camera resolution",
                "Insufficient lighting conditions",
                "Relying on high frame rate video as input"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Class-Incremental Exemplar Compression for Class-Incremental Learning\nZilin Luo1\nYaoyao Liu2\nBernt Schiele2\nQianru Sun1\n1Singapore Management University\n2Max Planck Institute for Informatics, Saarland Informatics Campus\nzilin.luo.2021@phdcs.smu.edu.sg\n{yaoyao.liu, schiele}@mpi-inf.mpg.de\nqianrusun@smu.edu.sg\nAbstract\nExemplar-based class-incremental learning (CIL) [36]\nfinetunes the model with all samples of new classes but few-\nshot exemplars of old classes in each incremental phase,\nwhere the “few-shot” abides by the limited memory bud-\nget. In this paper, we break this “few-shot” limit based\non a simple yet surprisingly effective idea: compressing\nexemplars by downsampling non-discriminative pixels and\nsaving “many-shot” compressed exemplars in the mem-\nory. Without needing any manual annotation, we achieve\nthis compression by generating 0-1 masks on discrimina-\ntive pixels from class activation maps (CAM) [49].\nWe\npropose an adaptive mask generation model called class-\nincremental masking (CIM) to explicitly resolve two dif-\nficulties of using CAM: 1) transforming the heatmaps of\nCAM to 0-1 masks with an arbitrary threshold leads to\na trade-off between the coverage on discriminative pix-\nels and the quantity of exemplars, as the total memory is\nfixed; and 2) optimal thresholds vary for different object\nclasses, which is particularly obvious in the dynamic envi-\nronment of CIL. We optimize the CIM model alternatively\nwith the conventional CIL model through a bilevel opti-\nmization problem [40]. We conduct extensive experiments\non high-resolution CIL benchmarks including Food-101,\nImageNet-100, and ImageNet-1000, and show that using\nthe compressed exemplars by CIM can achieve a new state-\nof-the-art CIL accuracy, e.g., 4.8 percentage points higher\nthan FOSTER [42] on 10-Phase ImageNet-1000. Our code\nis available at https://github.com/xfflzl/CIM-CIL.\n",
        "question": {
            "statement": "What is the main advantage of using compressed exemplars in class-incremental learning?",
            "options": [
                "It allows for saving more exemplars in the fixed memory budget",
                "It reduces the importance of manual annotation",
                "It eliminates the need for fine-tuning the model",
                "It increases the number of new classes in each incremental phase"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "2",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Learning 3D-aware Image Synthesis with Unknown Pose Distribution\nZifan Shi†*1\nYujun Shen†2\nYinghao Xu*3\nSida Peng4\nYiyi Liao4\nSheng Guo2\nQifeng Chen1\nDit-Yan Yeung1\n1HKUST\n2Ant Group\n3CUHK\n4Zhejiang University\nAbstract\nExisting methods for 3D-aware image synthesis largely\ndepend on the 3D pose distribution pre-estimated on the\ntraining set.\nAn inaccurate estimation may mislead the\nmodel into learning faulty geometry. This work proposes\nPoF3D that frees generative radiance fields from the re-\nquirements of 3D pose priors. We first equip the generator\nwith an efficient pose learner, which is able to infer a pose\nfrom a latent code, to approximate the underlying true pose\ndistribution automatically. We then assign the discriminator\na task to learn pose distribution under the supervision\nof the generator and to differentiate real and synthesized\nimages with the predicted pose as the condition.\nThe\npose-free generator and the pose-aware discriminator are\njointly trained in an adversarial manner. Extensive results\non a couple of datasets confirm that the performance of\nour approach, regarding both image quality and geometry\nquality, is on par with state of the art.\nTo our best\nknowledge, PoF3D demonstrates the feasibility of learning\nhigh-quality 3D-aware image synthesis without using 3D\npose priors for the first time. Project page can be found\nhere.\n",
        "question": {
            "statement": "What is a limitation of existing methods for 3D-aware image synthesis?",
            "options": [
                "They can only synthesize low-resolution images",
                "They require large amounts of computational resources",
                "They rely on pre-estimated 3D pose distributions",
                "They are limited to specific object categories"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "CORA: Adapting CLIP for Open-Vocabulary Detection with\nRegion Prompting and Anchor Pre-Matching\nXiaoshi Wu1, Feng Zhu2, Rui Zhao2,3, Hongsheng Li1,4\n1Multimedia Laboratory, The Chinese University of Hong Kong\n2SenseTime Research\n3Qing Yuan Research Institute, Shanghai Jiao Tong University\n4Centre for Perceptual and Interactive Intelligence (CPII)\n{wuxiaoshi@link, hsli@ee}.cuhk.edu.hk, {zhufeng, zhaorui}@sensetime.com\nAbstract\nOpen-vocabulary detection (OVD) is an object detection\ntask aiming at detecting objects from novel categories be-\nyond the base categories on which the detector is trained.\nRecent OVD methods rely on large-scale visual-language\npre-trained models, such as CLIP, for recognizing novel\nobjects. We identify the two core obstacles that need to\nbe tackled when incorporating these models into detec-\ntor training: (1) the distribution mismatch that happens\nwhen applying a VL-model trained on whole images to\nregion recognition tasks; (2) the difficulty of localizing\nobjects of unseen classes.\nTo overcome these obstacles,\nwe propose CORA, a DETR-style framework that adapts\nCLIP for Open-vocabulary detection by Region prompt-\ning and Anchor pre-matching. Region prompting mitigates\nthe whole-to-region distribution gap by prompting the re-\ngion features of the CLIP-based region classifier. Anchor\npre-matching helps learning generalizable object localiza-\ntion by a class-aware matching mechanism. We evaluate\nCORA on the COCO OVD benchmark, where we achieve\n41.7 AP50 on novel classes, which outperforms the pre-\nvious SOTA by 2.4 AP50 even without resorting to extra\ntraining data. When extra training data is available, we\ntrain CORA+ on both ground-truth base-category annota-\ntions and additional pseudo bounding box labels computed\nby CORA. CORA+ achieves 43.1 AP50 on the COCO OVD\nbenchmark and 28.1 box APr on the LVIS OVD benchmark.\nThe code is available at https://github.com/tgxs002/CORA.\n",
        "question": {
            "statement": "What is the main objective of Open-Vocabulary Detection (OVD) in object detection tasks?",
            "options": [
                "Detecting objects from novel categories beyond the base categories on which the detector is trained",
                "Segmenting objects from the background image",
                "Classifying objects into predefined categories",
                "Detecting objects within the base categories on which the detector is trained"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Hierarchical Video-Moment Retrieval and Step-Captioning\nAbhay Zala* 1\nJaemin Cho* 1\nSatwik Kottur2\nXilun Chen2\nBarlas Oguz2\nYashar Mehdad2\nMohit Bansal1\nUNC Chapel Hill1\nMeta AI2\n{jmincho, aszala, mbansal}@cs.unc.edu\n{skottur, xilun, barlaso, mehdad}@fb.com\nAbstract\nThere is growing interest in searching for information\nfrom large video corpora. Prior works have studied rele-\nvant tasks, such as text-based video retrieval, moment re-\ntrieval, video summarization, and video captioning in iso-\nlation, without an end-to-end setup that can jointly search\nfrom video corpora and generate summaries. Such an end-\nto-end setup would allow for many interesting applications,\ne.g., a text-based search that finds a relevant video from\na video corpus, extracts the most relevant moment from\nthat video, and segments the moment into important steps\nwith captions. To address this, we present the HIREST\n(HIerarchical REtrieval and STep-captioning) dataset and\npropose a new benchmark that covers hierarchical infor-\nmation retrieval and visual/textual stepwise summarization\nfrom an instructional video corpus. HIREST consists of\n3.4K text-video pairs from an instructional video dataset,\nwhere 1.1K videos have annotations of moment spans rel-\nevant to text query and breakdown of each moment into\nkey instruction steps with caption and timestamps (totaling\n8.6K step captions). Our hierarchical benchmark consists\nof video retrieval, moment retrieval, and two novel moment\nsegmentation and step captioning tasks. In moment segmen-\ntation, models break down a video moment into instruction\nsteps and identify start-end boundaries. In step caption-\ning, models generate a textual summary for each step. We\nalso present starting point task-specific and end-to-end joint\nbaseline models for our new benchmark. While the baseline\nmodels show some promising results, there still exists large\nroom for future improvement by the community.1\n",
        "question": {
            "statement": "What is the primary goal of the HIREST dataset and benchmark?",
            "options": [
                "To develop more accurate facial recognition systems",
                "To improve video compression algorithms",
                "To analyze viewer engagement metrics",
                "To enable joint video retrieval, moment segmentation, and step-wise summarization from instructional videos"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries\nYuanwen Yue1\nTheodora Kontogianni2\nKonrad Schindler1,2\nFrancis Engelmann2\n1Photogrammetry and Remote Sensing, ETH Zurich\n2ETH AI Center, ETH Zurich\nAbstract\nWe address 2D floorplan reconstruction from 3D scans.\nExisting approaches typically employ heuristically de-\nsigned multi-stage pipelines. Instead, we formulate floor-\nplan reconstruction as a single-stage structured predic-\ntion task: find a variable-size set of polygons, which in\nturn are variable-length sequences of ordered vertices.\nTo solve it we develop a novel Transformer architec-\nture that generates polygons of multiple rooms in paral-\nlel, in a holistic manner without hand-crafted intermedi-\nate stages. The model features two-level queries for poly-\ngons and corners, and includes polygon matching to make\nthe network end-to-end trainable.\nOur method achieves\na new state-of-the-art for two challenging datasets, Struc-\ntured3D and SceneCAD, along with significantly faster in-\nference than previous methods.\nMoreover, it can read-\nily be extended to predict additional information, i.e., se-\nmantic room types and architectural elements like doors\nand windows.\nOur code and models are available at:\nhttps://github.com/ywyue/RoomFormer.\n",
        "question": {
            "statement": "What is the main advantage of the proposed approach to 2D floorplan reconstruction from 3D scans?",
            "options": [
                "It does not require hand-crafted intermediate stages.",
                "It requires prior knowledge of the number of rooms",
                "It is limited to predicting room shapes only",
                "It only works on simple floorplans"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "PEFAT: Boosting Semi-supervised Medical Image Classification via Pseudo-loss\nEstimation and Feature Adversarial Training\nQingjie Zeng1∗\nYutong Xie2∗\nZilin Lu1 Yong Xia1†\n1 School of Computer Science and Engineering, Northwestern Polytechnical University, China\n2 The University of Adelaide, Australia\nmaxwell@mail.nwpu.edu.cn, yutong.xie678@gmail.com, luzl@mail.nwpu.edu.cn, yxia@nwpu.edu.cn\nAbstract\nPseudo-labeling approaches have been proven beneficial\nfor semi-supervised learning (SSL) schemes in computer vi-\nsion and medical imaging. Most works are dedicated to\nfinding samples with high-confidence pseudo-labels from\nthe perspective of model predicted probability.\nWhereas\nthis way may lead to the inclusion of incorrectly pseudo-\nlabeled data if the threshold is not carefully adjusted. In\naddition, low-confidence probability samples are frequently\ndisregarded and not employed to their full potential.\nIn\nthis paper, we propose a novel Pseudo-loss Estimation\nand Feature Adversarial Training semi-supervised frame-\nwork, termed as PEFAT, to boost the performance of multi-\nclass and multi-label medical image classification from the\npoint of loss distribution modeling and adversarial train-\ning.\nSpecifically, we develop a trustworthy data selec-\ntion scheme to split a high-quality pseudo-labeled set, in-\nspired by the dividable pseudo-loss assumption that clean\ndata tend to show lower loss while noise data is the oppo-\nsite. Instead of directly discarding these samples with low-\nquality pseudo-labels, we present a novel regularization\napproach to learn discriminate information from them via\ninjecting adversarial noises at the feature-level to smooth\nthe decision boundary. Experimental results on three med-\nical and two natural image benchmarks validate that our\nPEFAT can achieve a promising performance and surpass\nother state-of-the-art methods.\nThe code is available at\nhttps://github.com/maxwell0027/PEFAT.\n",
        "question": {
            "statement": "What is a limitation of traditional pseudo-labeling approaches in semi-supervised learning?",
            "options": [
                "They require large amounts of labeled data",
                "They may include incorrectly labeled data if the confidence threshold is not carefully adjusted",
                "They are only effective for binary classification tasks",
                "They are not applicable to medical image classification"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Unified Pose Sequence Modeling\nLin Geng Foo1*\nTianjiao Li1*\nHossein Rahmani2\nQiuhong Ke3\nJun Liu1†\n1Singapore University of Technology & Design\n2Lancaster University\n3Monash University\n{lingeng foo,tianjiao li}@mymail.sutd.edu.sg, h.rahmani@lancaster.ac.uk,\nqiuhong.ke@monash.edu, jun liu@sutd.edu.sg\nAbstract\nWe propose a Unified Pose Sequence Modeling ap-\nproach to unify heterogeneous human behavior understand-\ning tasks based on pose data, e.g., action recognition, 3D\npose estimation and 3D early action prediction. A major\nobstacle is that different pose-based tasks require different\noutput data formats. Specifically, the action recognition and\nprediction tasks require class predictions as outputs, while\n3D pose estimation requires a human pose output, which\nlimits existing methods to leverage task-specific network ar-\nchitectures for each task. Hence, in this paper, we propose\na novel Unified Pose Sequence (UPS) model to unify het-\nerogeneous output formats for the aforementioned tasks by\nconsidering text-based action labels and coordinate-based\nhuman poses as language sequences.\nThen, by optimiz-\ning a single auto-regressive transformer, we can obtain a\nunified output sequence that can handle all the aforemen-\ntioned tasks. Moreover, to avoid the interference brought by\nthe heterogeneity between different tasks, a dynamic rout-\ning mechanism is also proposed to empower our UPS with\nthe ability to learn which subsets of parameters should be\nshared among different tasks. To evaluate the efficacy of\nthe proposed UPS, extensive experiments are conducted on\nfour different tasks with four popular behavior understand-\ning benchmarks.\n",
        "question": {
            "statement": "What is a major obstacle in unifying heterogeneous human behavior understanding tasks based on pose data?",
            "options": [
                "The lack of large-scale datasets for training",
                "The need for real-time processing",
                "Different pose-based tasks require different output data formats",
                "The complexity of human behavior understanding tasks"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "MIC: Masked Image Consistency for Context-Enhanced Domain Adaptation\nLukas Hoyer 1\nDengxin Dai 2\nHaoran Wang 2\nLuc Van Gool 1,3\n1 ETH Zurich\n2 Max Planck Institute for Informatics, Saarland Informatics Campus\n3 KU Leuven\n{lhoyer,vangool}@vision.ee.ethz.ch, {ddai,hawang}@mpi-inf.mpg.de\nAbstract\nIn unsupervised domain adaptation (UDA), a model\ntrained on source data (e.g. synthetic) is adapted to tar-\nget data (e.g. real-world) without access to target anno-\ntation. Most previous UDA methods struggle with classes\nthat have a similar visual appearance on the target domain\nas no ground truth is available to learn the slight appear-\nance differences. To address this problem, we propose a\nMasked Image Consistency (MIC) module to enhance UDA\nby learning spatial context relations of the target domain\nas additional clues for robust visual recognition. MIC en-\nforces the consistency between predictions of masked target\nimages, where random patches are withheld, and pseudo-\nlabels that are generated based on the complete image by\nan exponential moving average teacher. To minimize the\nconsistency loss, the network has to learn to infer the pre-\ndictions of the masked regions from their context. Due to\nits simple and universal concept, MIC can be integrated\ninto various UDA methods across different visual recogni-\ntion tasks such as image classification, semantic segmenta-\ntion, and object detection. MIC significantly improves the\nstate-of-the-art performance across the different recognition\ntasks for synthetic-to-real, day-to-nighttime, and clear-to-\nadverse-weather UDA. For instance, MIC achieves an un-\nprecedented UDA performance of 75.9 mIoU and 92.8%\non GTA→Cityscapes and VisDA-2017, respectively, which\ncorresponds to an improvement of +2.1 and +3.0 percent\npoints over the previous state of the art. The implementation\nis available at https://github.com/lhoyer/MIC.\n",
        "question": {
            "statement": "What is the main challenge faced by most previous Unsupervised Domain Adaptation (UDA) methods?",
            "options": [
                "classes that have a distinct visual appearance on the source domain",
                "classes that require manual annotation",
                "classes that have a similar visual appearance on the target domain",
                "classes that are not present in the target domain"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "FREDOM: Fairness Domain Adaptation Approach to\nSemantic Scene Understanding\nThanh-Dat Truong1, Ngan Le1, Bhiksha Raj2, Jackson Cothren3, Khoa Luu1\n1CVIU Lab, University of Arkansas, USA\n2Carnegie Mellon University, USA\n3Dep. of Geosciences, University of Arkansas, USA\n{tt032, thile, jcothre, khoaluu}@uark.edu, bhiksha@cs.cmu.edu\nAbstract\nAlthough Domain Adaptation in Semantic Scene Seg-\nmentation has shown impressive improvement in recent\nyears, the fairness concerns in the domain adaptation have\nyet to be well defined and addressed. In addition, fairness is\none of the most critical aspects when deploying the segmen-\ntation models into human-related real-world applications,\ne.g., autonomous driving, as any unfair predictions could\ninfluence human safety. In this paper, we propose a novel\nFairness Domain Adaptation (FREDOM) approach to se-\nmantic scene segmentation. In particular, from the proposed\nformulated fairness objective, a new adaptation framework\nwill be introduced based on the fair treatment of class distri-\nbutions. Moreover, to generally model the context of struc-\ntural dependency, a new conditional structural constraint is\nintroduced to impose the consistency of predicted segmen-\ntation. Thanks to the proposed Conditional Structure Net-\nwork, the self-attention mechanism has sufficiently modeled\nthe structural information of segmentation. Through the ab-\nlation studies, the proposed method has shown the perfor-\nmance improvement of the segmentation models and pro-\nmoted fairness in the model predictions. The experimental\nresults on the two standard benchmarks, i.e., SYNTHIA →\nCityscapes and GTA5 →Cityscapes, have shown that our\nmethod achieved State-of-the-Art (SOTA) performance1.\n",
        "question": {
            "statement": "What is a crucial aspect to consider when deploying semantic scene segmentation models in real-world applications such as autonomous driving?",
            "options": [
                "speed",
                "fairness",
                "complexity",
                "accuracy"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "VoxelNeXt: Fully Sparse VoxelNet for 3D Object Detection and Tracking\nYukang Chen1\nJianhui Liu2\nXiangyu Zhang3\nXiaojuan Qi2\nJiaya Jia1,4\n1The Chinese University of Hong Kong 2The University of Hong Kong 3MEGVII 4SmartMore\nAbstract\n3D object detectors usually rely on hand-crafted prox-\nies, e.g., anchors or centers, and translate well-studied 2D\nframeworks to 3D. Thus, sparse voxel features need to be\ndensified and processed by dense prediction heads, which\ninevitably costs extra computation. In this paper, we in-\nstead propose VoxelNext for fully sparse 3D object detec-\ntion. Our core insight is to predict objects directly based\non sparse voxel features, without relying on hand-crafted\nproxies.\nOur strong sparse convolutional network Vox-\nelNeXt detects and tracks 3D objects through voxel fea-\ntures entirely.\nIt is an elegant and efficient framework,\nwith no need for sparse-to-dense conversion or NMS post-\nprocessing. Our method achieves a better speed-accuracy\ntrade-off than other mainframe detectors on the nuScenes\ndataset. For the first time, we show that a fully sparse voxel-\nbased representation works decently for LIDAR 3D object\ndetection and tracking. Extensive experiments on nuScenes,\nWaymo, and Argoverse2 benchmarks validate the effective-\nness of our approach. Without bells and whistles, our model\noutperforms all existing LIDAR methods on the nuScenes\ntracking test benchmark. Code and models are available at\ngithub.com/dvlab-research/VoxelNeXt.\n",
        "question": {
            "statement": "What is the main advantage of using a fully sparse voxel-based representation in 3D object detection and tracking?",
            "options": [
                "Improved accuracy in low-light conditions",
                "Reduced computational cost",
                "Increased robustness to noise and outliers",
                "Enhanced interpretability of results"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "9",
                "2",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Solving relaxations of MAP-MRF problems:\nCombinatorial in-face Frank-Wolfe directions\nVladimir Kolmogorov\nInstitute of Science and Technology Austria (ISTA)\nAm Campus 1, Klosterneuburg 3400, Austria\nvnk@ist.ac.at\nAbstract\nWe consider the problem of solving LP relaxations\nof MAP-MRF inference problems, and in particular the\nmethod proposed recently in [16, 35]. As a key computa-\ntional subroutine, it uses a variant of the Frank-Wolfe (FW)\nmethod to minimize a smooth convex function over a combi-\nnatorial polytope. We propose an efﬁcient implementation\nof this subroutine based on in-face Frank-Wolfe directions,\nintroduced in [4] in a different context. More generally, we\ndeﬁne an abstract data structure for a combinatorial sub-\nproblem that enables in-face FW directions, and describe\nits specialization for tree-structured MAP-MRF inference\nsubproblems. Experimental results indicate that the result-\ning method is the current state-of-art LP solver for some\nclasses of problems. Our code is available at pub.ist.\nac.at/˜vnk/papers/IN-FACE-FW.html.\n",
        "question": {
            "statement": "What type of optimization method is used as a key computational subroutine in solving LP relaxations of MAP-MRF inference problems?",
            "options": [
                "Frank-Wolfe method",
                "Quadratic Programming",
                "Dynamic Programming",
                "Gradient Descent"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Reconstructing Animatable Categories from Videos\nGengshan Yang Chaoyang Wang N Dinesh Reddy Deva Ramanan\nCarnegie Mellon University\nInternet Videos of a Category\nCanonical Space\nArticulations & Deformations\nDifferentiable \nRendering\nt=0s\nt=2s\nsphynx cat\ncheetah\nSkeleton\nColor: Skinning weights\nMorphology\nMotion Transfer\nFigure 1. Given videos of a deformable category and a skeleton, we reconstruct an animatable 3D model that factorizes variations across\ninstances (e.g., cheetah’s and sphynx’s are both cats but with different shape morphology, skeleton dimensions, and texture) from\ntime-speciﬁc variations within an instance (e.g., skeleton articulations and elastic shape deformation). Left: Input videos; Middle-left: 3D\nshape, skeleton, and skinning weights (visualized as surface colors) in the canonical space; Middle-right: Disentangled between-instance\nand within-instance variations over time. Right: Morphology and motion transferred across the two instances.\nAbstract\nBuilding animatable 3D models is challenging due to the\nneed for 3D scans, laborious registration, and rigging. Re-\ncently, differentiable rendering provides a pathway to ob-\ntain high-quality 3D models from monocular videos, but\nthese are limited to rigid categories or single instances. We\npresent RAC, a method to build category-level 3D models\nfrom monocular videos, disentangling variations over in-\nstances and motion over time. Three key ideas are intro-\nduced to solve this problem: (1) specializing a category-\nlevel skeleton to instances, (2) a method for latent space\nregularization that encourages shared structure across a\ncategory while maintaining instance details, and (3) us-\ning 3D background models to disentangle objects from the\nbackground. We build 3D models for humans, cats and dogs\ngiven monocular videos. Project page: https://gengshan-\ny.github.io/rac-www/.\n",
        "question": {
            "statement": "What is a major challenge when building animatable 3D models?",
            "options": [
                "The need for 3D scans, laborious registration, and rigging",
                "Inability to capture complex motions",
                "Limited availability of video data",
                "Insufficient computational power"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "3",
                "0",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "PIVOT: Prompting for Video Continual Learning\nAndrés Villa1,2, Juan León Alcázar2, Motasem Alfarra2, Kumail Alhamoud2, Julio Hurtado3,\nFabian Caba Heilbron4, Alvaro Soto1, Bernard Ghanem2\n1Pontificia Universidad Católica de Chile, 2King Abdullah University of Science and Technology (KAUST),\n3University of Pisa, 4Adobe Research\nafvilla@uc.cl, {juancarlo.alcazar,motasem.alfarra,kumail.hamoud}@kaust.edu.sa\njulio.hurtado@di.unipi.it, caba@adobe.com, asoto@ing.puc.cl, bernard.ghanem@kaust.edu.sa\nAbstract\nModern machine learning pipelines are limited due to\ndata availability, storage quotas, privacy regulations, and\nexpensive annotation processes. These constraints make it\ndifficult or impossible to train and update large-scale mod-\nels on such dynamic annotated sets. Continual learning di-\nrectly approaches this problem, with the ultimate goal of\ndevising methods where a deep neural network effectively\nlearns relevant patterns for new (unseen) classes, without\nsignificantly altering its performance on previously learned\nones.\nIn this paper, we address the problem of contin-\nual learning for video data. We introduce PIVOT, a novel\nmethod that leverages extensive knowledge in pre-trained\nmodels from the image domain, thereby reducing the num-\nber of trainable parameters and the associated forgetting.\nUnlike previous methods, ours is the first approach that ef-\nfectively uses prompting mechanisms for continual learning\nwithout any in-domain pre-training. Our experiments show\nthat PIVOT improves state-of-the-art methods by a signifi-\ncant 27% on the 20-task ActivityNet setup.\n",
        "question": {
            "statement": "What is the main objective of continual learning in machine learning?",
            "options": [
                "To enable a model to learn new patterns without significantly affecting its performance on previously learned ones",
                "To reduce the number of trainable parameters in a model",
                "To improve the accuracy of a model on a specific task",
                "To increase the size of a model's training dataset"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Randomized Adversarial Training via Taylor Expansion\nGaojie Jin1,2, Xinping Yi2B, Dengyu Wu2, Ronghui Mu3, Xiaowei Huang2B\n1State Key Laboratory of Computer Science, Institute of Software, CAS, Beijing, China\n2University of Liverpool, Liverpool, UK\n3Lancaster University, Lancaster, UK\n{g.jin3, xinping.yi, xiaowei.huang}@liverpool.ac.uk\nB Corresponding Author\nAbstract\nIn recent years, there has been an explosion of research\ninto developing more robust deep neural networks against\nadversarial examples. Adversarial training appears as one\nof the most successful methods. To deal with both the ro-\nbustness against adversarial examples and the accuracy\nover clean examples, many works develop enhanced ad-\nversarial training methods to achieve various trade-offs be-\ntween them [19,38,80]. Leveraging over the studies [8,32]\nthat smoothed update on weights during training may help\nfind flat minima and improve generalization, we suggest\nreconciling the robustness-accuracy trade-off from another\nperspective, i.e., by adding random noise into determinis-\ntic weights. The randomized weights enable our design of a\nnovel adversarial training method via Taylor expansion of a\nsmall Gaussian noise, and we show that the new adversar-\nial training method can flatten loss landscape and find flat\nminima. With PGD, CW, and Auto Attacks, an extensive set\nof experiments demonstrate that our method enhances the\nstate-of-the-art adversarial training methods, boosting both\nrobustness and clean accuracy. The code is available at\nhttps://github.com/Alexkael/Randomized-\nAdversarial-Training.\n",
        "question": {
            "statement": "What approach is proposed to reconcile the robustness-accuracy trade-off in deep neural networks?",
            "options": [
                "Using larger neural networks",
                "Increasing the number of epochs during training",
                "Adding random noise to deterministic weights",
                "Implementing early stopping during training"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "BEV-LaneDet: An Efﬁcient 3D Lane Detection Based on Virtual Camera via\nKey-Points\nRuihao Wang1,2, Jian Qin*1, Kaiying Li1, Yaochen Li2, Dong Cao1, Jintao Xu1\n1HAOMO.AI Technology Co., Ltd.\n2Xi’an Jiaotong University\n{wangruihao, qinjian, likaiying, caodong, xujintao}@haomo.ai, yaochenli@mail.xjtu.edu.cn\nAbstract\n3D lane detection which plays a crucial role in vehicle\nrouting, has recently been a rapidly developing topic in au-\ntonomous driving. Previous works struggle with practical-\nity due to their complicated spatial transformations and in-\nﬂexible representations of 3D lanes. Faced with the issues,\nour work proposes an efﬁcient and robust monocular 3D\nlane detection called BEV-LaneDet with three main contri-\nbutions. First, we introduce the Virtual Camera that uniﬁes\nthe in/extrinsic parameters of cameras mounted on differ-\nent vehicles to guarantee the consistency of the spatial re-\nlationship among cameras. It can effectively promote the\nlearning procedure due to the uniﬁed visual space. We sec-\nondly propose a simple but efﬁcient 3D lane representation\ncalled Key-Points Representation. This module is more suit-\nable to represent the complicated and diverse 3D lane struc-\ntures. At last, we present a light-weight and chip-friendly\nspatial transformation module named Spatial Transforma-\ntion Pyramid to transform multiscale front-view features\ninto BEV features. Experimental results demonstrate that\nour work outperforms the state-of-the-art approaches in\nterms of F-Score, being 10.6% higher on the OpenLane\ndataset and 4.0% higher on the Apollo 3D synthetic dataset,\nwith a speed of 185 FPS. Code is released at https:\n//github.com/gigo-team/bev_lane_det.\n",
        "question": {
            "statement": "What is the primary advantage of using a 'Virtual Camera' in 3D lane detection systems?",
            "options": [
                "It unifies the intrinsic and extrinsic parameters of cameras mounted on different vehicles",
                "It is a type of camera specifically designed for autonomous vehicles",
                "It provides a wider field of view than traditional cameras",
                "It reduces the amount of data required for 3D lane detection"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "AssemblyHands:\nTowards Egocentric Activity Understanding via 3D Hand Pose Estimation\nTakehiko Ohkawa1,2* Kun He1\nFadime Sener1\nTomas Hodan1\nLuan Tran1\nCem Keskin1\n1Meta Reality Labs\n2The University of Tokyo\nhttps://assemblyhands.github.io\nAbstract\nWe present AssemblyHands, a large-scale benchmark\ndataset with accurate 3D hand pose annotations, to facili-\ntate the study of egocentric activities with challenging hand-\nobject interactions. The dataset includes synchronized ego-\ncentric and exocentric images sampled from the recent As-\nsembly101 dataset, in which participants assemble and dis-\nassemble take-apart toys. To obtain high-quality 3D hand\npose annotations for the egocentric images, we develop an\nefficient pipeline, where we use an initial set of manual an-\nnotations to train a model to automatically annotate a much\nlarger dataset. Our annotation model uses multi-view fea-\nture fusion and an iterative refinement scheme, and achieves\nan average keypoint error of 4.20 mm, which is 85% lower\nthan the error of the original annotations in Assembly101.\nAssemblyHands provides 3.0M annotated images, includ-\ning 490K egocentric images, making it the largest existing\nbenchmark dataset for egocentric 3D hand pose estimation.\nUsing this data, we develop a strong single-view baseline of\n3D hand pose estimation from egocentric images. Further-\nmore, we design a novel action classification task to evalu-\nate predicted 3D hand poses. Our study shows that having\nhigher-quality hand poses directly improves the ability to\nrecognize actions.\n",
        "question": {
            "statement": "What is the primary goal of the AssemblyHands project?",
            "options": [
                "To investigate the psychology of hand-object interactions",
                "To develop a new type of take-apart toy",
                "To create a dataset for facial recognition",
                "To improve the accuracy of 3D hand pose estimation in egocentric activity understanding"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Multi-Mode Online Knowledge Distillation for Self-Supervised Visual\nRepresentation Learning\nKaiyou Song* Jin Xie\nShan Zhang\nZimeng Luo\nMegvii Technology\n{songkaiyou, xiejin, zhangshan, luozimeng}@megvii.com\nAbstract\nSelf-supervised learning (SSL) has made remarkable\nprogress in visual representation learning. Some studies\ncombine SSL with knowledge distillation (SSL-KD) to boost\nthe representation learning performance of small models. In\nthis study, we propose a Multi-mode Online Knowledge Dis-\ntillation method (MOKD) to boost self-supervised visual rep-\nresentation learning. Different from existing SSL-KD meth-\nods that transfer knowledge from a static pre-trained teacher\nto a student, in MOKD, two different models learn collab-\noratively in a self-supervised manner. Specifically, MOKD\nconsists of two distillation modes: self-distillation and cross-\ndistillation modes. Among them, self-distillation performs\nself-supervised learning for each model independently, while\ncross-distillation realizes knowledge interaction between dif-\nferent models. In cross-distillation, a cross-attention feature\nsearch strategy is proposed to enhance the semantic feature\nalignment between different models. As a result, the two\nmodels can absorb knowledge from each other to boost their\nrepresentation learning performance. Extensive experimen-\ntal results on different backbones and datasets demonstrate\nthat two heterogeneous models can benefit from MOKD and\noutperform their independently trained baseline. In addition,\nMOKD also outperforms existing SSL-KD methods for both\nthe student and teacher models.\n",
        "question": {
            "statement": "What is the main difference between traditional knowledge distillation methods and the Multi-mode Online Knowledge Distillation (MOKD) approach?",
            "options": [
                "One model learns from another fixed model",
                "Two models learn collaboratively and interact with each other",
                "Multiple models learn separately without interaction",
                "A single pre-trained teacher model transfers knowledge to a student model"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "8",
                "2",
                "2"
            ]
        },
        "difference": 6,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Spherical Transformer for LiDAR-based 3D Recognition\nXin Lai1\nYukang Chen1\nFanbin Lu1\nJianhui Liu2\nJiaya Jia1,3\n1The Chinese University of Hong Kong\n2The University of Hong Kong\n3SmartMore\nAbstract\nLiDAR-based 3D point cloud recognition has benefited\nvarious applications. Without specially considering the Li-\nDAR point distribution, most current methods suffer from\ninformation disconnection and limited receptive field, es-\npecially for the sparse distant points.\nIn this work, we\nstudy the varying-sparsity distribution of LiDAR points and\npresent SphereFormer to directly aggregate information\nfrom dense close points to the sparse distant ones. We de-\nsign radial window self-attention that partitions the space\ninto multiple non-overlapping narrow and long windows.\nIt overcomes the disconnection issue and enlarges the re-\nceptive field smoothly and dramatically, which significantly\nboosts the performance of sparse distant points. Moreover,\nto fit the narrow and long windows, we propose exponen-\ntial splitting to yield fine-grained position encoding and\ndynamic feature selection to increase model representation\nability. Notably, our method ranks 1st on both nuScenes and\nSemanticKITTI semantic segmentation benchmarks with\n81.9% and 74.8% mIoU, respectively.\nAlso, we achieve\nthe 3rd place on nuScenes object detection benchmark with\n72.8% NDS and 68.5% mAP. Code is available at https:\n//github.com/dvlab-research/SphereFormer.git.\n",
        "question": {
            "statement": "What is a common limitation of current methods in LiDAR-based 3D point cloud recognition?",
            "options": [
                "Information disconnection and limited receptive field",
                "Insufficient training data",
                " Limited sensor resolution",
                "High computational complexity"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Learning Rotation-Equivariant Features for Visual Correspondence\nJongmin Lee\nByungjin Kim\nSeungwook Kim\nMinsu Cho\nPohang University of Science and Technology (POSTECH), South Korea\nhttp://cvlab.postech.ac.kr/research/RELF\nAbstract\nExtracting discriminative local features that are invari-\nant to imaging variations is an integral part of establish-\ning correspondences between images.\nIn this work, we\nintroduce a self-supervised learning framework to extract\ndiscriminative rotation-invariant descriptors using group-\nequivariant CNNs. Thanks to employing group-equivariant\nCNNs, our method effectively learns to obtain rotation-\nequivariant features and their orientations explicitly, with-\nout having to perform sophisticated data augmentations.\nThe resultant features and their orientations are further pro-\ncessed by group aligning, a novel invariant mapping tech-\nnique that shifts the group-equivariant features by their ori-\nentations along the group dimension.\nOur group align-\ning technique achieves rotation-invariance without any col-\nlapse of the group dimension and thus eschews loss of dis-\ncriminability. The proposed method is trained end-to-end\nin a self-supervised manner, where we use an orientation\nalignment loss for the orientation estimation and a con-\ntrastive descriptor loss for robust local descriptors to ge-\nometric/photometric variations. Our method demonstrates\nstate-of-the-art matching accuracy among existing rotation-\ninvariant descriptors under varying rotation and also shows\ncompetitive results when transferred to the task of keypoint\nmatching and camera pose estimation.\n",
        "question": {
            "statement": "What is a key advantage of using group-equivariant CNNs in the context of visual correspondence?",
            "options": [
                "They require manual annotation of keypoints in images.",
                "They enable the direct estimation of camera poses from images.",
                "They are limited to processing grayscale images only.",
                "They allow for effective learning of rotation-equivariant features without requiring sophisticated data augmentations."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "RenderDiffusion: Image Diffusion for 3D Reconstruction,\nInpainting and Generation\nTitas Anciukeviˇ\ncius1,2, Zexiang Xu2, Matthew Fisher2,\nPaul Henderson3, Hakan Bilen1, Niloy J. Mitra2,4, Paul Guerrero2\n1University of Edinburgh, 2Adobe Research, 3University of Glasglow, 4UCL\nMonocular 3D Reconstruction\n3D-Aware Inpainting\nUnconditional 3D Generation\ninput\nest. depth\nnovel view\ninput & mask\ninpainted\nnovel view\nFigure 1. We propose a 3D-aware image diffusion model that can be used for monocular 3D reconstruction, 3D-aware inpainting, and\nunconditional generation, while being trained with only monocular 2D supervision. Here we show results on ShapeNet and FFHQ.\nAbstract\nDiffusion models currently achieve state-of-the-art per-\nformance for both conditional and unconditional image\ngeneration. However, so far, image diffusion models do not\nsupport tasks required for 3D understanding, such as view-\nconsistent 3D generation or single-view object reconstruc-\ntion. In this paper, we present RenderDiffusion, the first dif-\nfusion model for 3D generation and inference, trained using\nonly monocular 2D supervision. Central to our method is a\nnovel image denoising architecture that generates and ren-\nders an intermediate three-dimensional representation of a\nscene in each denoising step. This enforces a strong induc-\ntive structure within the diffusion process, providing a 3D\nconsistent representation while only requiring 2D supervi-\nsion. The resulting 3D representation can be rendered from\nany view. We evaluate RenderDiffusion on FFHQ, AFHQ,\nShapeNet and CLEVR datasets, showing competitive per-\nformance for generation of 3D scenes and inference of 3D\nscenes from 2D images. Additionally, our diffusion-based\napproach allows us to use 2D inpainting to edit 3D scenes.\nProject page:\nhttps://github.com/Anciukevicius/\nRenderDiffusion\n",
        "question": {
            "statement": "What is a key advantage of the RenderDiffusion model compared to other diffusion models?",
            "options": [
                "It requires 3D supervision during training",
                "It is limited to generating 2D images",
                "It can generate and infer 3D scenes from 2D images",
                "It can only perform unconditional image generation"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Nighttime Smartphone Reflective Flare Removal\nUsing Optical Center Symmetry Prior\nYuekun Dai\nYihang Luo\nShangchen Zhou Chongyi Li* Chen Change Loy\nS-Lab, Nanyang Technological University\n{YDAI005, c200211, s200094, chongyi.li, ccloy}@ntu.edu.sg\nhttps://ykdai.github.io/projects/BracketFlare\nAbstract\nReflective flare is a phenomenon that occurs when light\nreflects inside lenses, causing bright spots or a “ghosting\neffect” in photos, which can impact their quality. Elimi-\nnating reflective flare is highly desirable but challenging.\nMany existing methods rely on manually designed features\nto detect these bright spots, but they often fail to identify\nreflective flares created by various types of light and may\neven mistakenly remove the light sources in scenarios with\nmultiple light sources. To address these challenges, we pro-\npose an optical center symmetry prior, which suggests that\nthe reflective flare and light source are always symmetrical\naround the lens’s optical center. This prior helps to locate\nthe reflective flare’s proposal region more accurately and\ncan be applied to most smartphone cameras. Building on\nthis prior, we create the first reflective flare removal dataset\ncalled BracketFlare, which contains diverse and realistic\nreflective flare patterns. We use continuous bracketing to\ncapture the reflective flare pattern in the underexposed im-\nage and combine it with a normally exposed image to syn-\nthesize a pair of flare-corrupted and flare-free images. With\nthe dataset, neural networks can be trained to remove the\nreflective flares effectively. Extensive experiments demon-\nstrate the effectiveness of our method on both synthetic and\nreal-world datasets.\n",
        "question": {
            "statement": "What is the main challenge in eliminating reflective flare from photos?",
            "options": [
                "The lack of symmetry around the lens's optical center",
                "The camera's inability to capture bright spots",
                "The need for manual feature design",
                "The difficulty in identifying reflective flares created by various types of light"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "2",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "EvShutter: Transforming Events for Unconstrained Rolling Shutter Correction\nJulius Erbach\nStepan Tulyakov\nPatricia Vitoria\nAlfredo Bochicchio\nYuanyou Li\nHuawei Technologies, Zurich Research Center\nFigure 1. Comparison of EvShutter to state-of-the-art event-based RS correction method [28].\nOur method introduces a novel event\ntransformation, which allows us to correct RS artifacts in the presence of non-linear motion or large displacements.\nAbstract\nWidely used Rolling Shutter (RS) CMOS sensors capture\nhigh resolution images at the expense of introducing dis-\ntortions and artifacts in the presence of motion. In such\nsituations, RS distortion correction algorithms are critical.\nRecent methods rely on a constant velocity assumption and\nrequire multiple frames to predict the dense displacement\nfield.\nIn this work, we introduce a new method, called\nEventful Shutter (EvShutter)1, that corrects RS artifacts us-\ning a single RGB image and event information with high\ntemporal resolution. The method firstly removes blur us-\ning a novel flow-based deblurring module and then com-\npensates RS using a double encoder hourglass network. In\ncontrast to previous methods, it does not rely on a constant\nvelocity assumption and uses a simple architecture thanks\nto an event transformation dedicated to RS, called Filter\nand Flip (FnF), that transforms input events to encode only\n1The evaluation code and the dataset can be found here https://\ngithub.com/juliuserbach/EvShutter\nthe changes between GS and RS images. To evaluate the\nproposed method and facilitate future research, we collect\nthe first dataset with real events and high-quality RS im-\nages with optional blur, called RS-ERGB. We generate the\nRS images from GS images using a newly proposed simula-\ntor based on adaptive interpolation. The simulator permits\nthe use of inexpensive cameras with long exposure to cap-\nture high-quality GS images. We show that on this realistic\ndataset the proposed method outperforms the state-of-the-\nart image- and event-based methods by 9.16 dB and 0.75 dB\nrespectively in terms of PSNR and an improvement of 23 %\nand 21 % in LPIPS.\n",
        "question": {
            "statement": "What is a key advantage of the Eventful Shutter (EvShutter) method over previous rolling shutter correction algorithms?",
            "options": [
                "It does not rely on a constant velocity assumption",
                "It can only be used with low-resolution images",
                "It is limited to correcting small displacements",
                "It requires multiple frames to predict the dense displacement field"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Network Expansion For Practical Training Acceleration\nNing Ding1,2, Yehui Tang1,2, Kai Han2⋆, Chao Xu1, Yunhe Wang2⋆\n1 National Key Lab of General AI, School of Intelligence Science and Technology, Peking University\n2 Huawei Noah’s Ark Lab\ndingning@stu.pku.edu.cn, yhtang@pku.edu.cn, kai.han@huawei.com,\nxuchao@cis.pku.edu.cn, yunhe.wang@huawei.com\nAbstract\nRecently, the sizes of deep neural networks and train-\ning datasets both increase drastically to pursue better\nperformance in a practical sense.\nWith the prevalence\nof transformer-based models in vision tasks, even more\npressure is laid on the GPU platforms to train these heavy\nmodels, which consumes a large amount of time and\ncomputing resources as well.\nTherefore, it’s crucial to\naccelerate the training process of deep neural networks.\nIn this paper, we propose a general network expansion\nmethod to reduce the practical time cost of the model\ntraining process. Specifically, we utilize both width- and\ndepth-level sparsity of dense models to accelerate the\ntraining of deep neural networks. Firstly, we pick a sparse\nsub-network from the original dense model by reducing\nthe number of parameters as the starting point of training.\nThen the sparse architecture will gradually expand during\nthe training procedure and finally grow into a dense one.\nWe design different expanding strategies to grow CNNs\nand ViTs respectively, due to the great heterogeneity in\nbetween the two architectures. Our method can be easily\nintegrated into popular deep learning frameworks, which\nsaves considerable training time and hardware resources.\nExtensive experiments show that our acceleration method\ncan significantly speed up the training process of modern\nvision models on general GPU devices with negligible\nperformance drop (e.g. 1.42× faster for ResNet-101 and\n1.34× faster for DeiT-base on ImageNet-1k).\nThe code\nis\navailable\nat\nhttps : / / github . com / huawei -\nnoah / Efficient - Computing / tree / master /\nTrainingAcceleration / NetworkExpansion\nand\nhttps : / / gitee . com / mindspore / hub / blob /\nmaster / mshub _ res / assets / noah - cvlab / gpu /\n1.8/networkexpansion_v1.0_imagenet2012.md.\n",
        "question": {
            "statement": "What is the primary goal of the proposed network expansion method in deep neural network training?",
            "options": [
                "to change the architecture of the model",
                "to improve the accuracy of the model",
                "to accelerate the training process",
                "to reduce the size of the training dataset"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Ego-Body Pose Estimation via Ego-Head Pose Estimation\nJiaman Li\nC. Karen Liu†\nJiajun Wu†\nStanford University\n{jiamanli,karenliu,jiajunwu}@cs.stanford.edu\n(a) Egocentric Video\n(b) Head Pose Estimation\n(c) Full-Body Human Motion Generation\nTimestamp\nTimestamp\nGenerated Motion 1\nGenerated Motion 2\nx\ny\nz\nFigure 1. Taking egocentric video (a) as input, our approach first predicts head pose (b); it then estimates multiple plausible full-body\nhuman motions (c) from the predicted head pose. This motion sequence shows a human kicking and jumping in place. Please refer to the\nsupplementary video on our project page for a complete motion sequence visualization.\nAbstract\nEstimating 3D human motion from an egocentric video\nsequence plays a critical role in human behavior understand-\ning and has various applications in VR/AR. However, naively\nlearning a mapping between egocentric videos and human\nmotions is challenging, because the user’s body is often un-\nobserved by the front-facing camera placed on the head of\nthe user. In addition, collecting large-scale, high-quality\ndatasets with paired egocentric videos and 3D human mo-\ntions requires accurate motion capture devices, which often\nlimit the variety of scenes in the videos to lab-like environ-\nments. To eliminate the need for paired egocentric video and\nhuman motions, we propose a new method, Ego-Body Pose\nEstimation via Ego-Head Pose Estimation (EgoEgo), which\ndecomposes the problem into two stages, connected by the\nhead motion as an intermediate representation. EgoEgo first\nintegrates SLAM and a learning approach to estimate accu-\nrate head motion. Subsequently, leveraging the estimated\nhead pose as input, EgoEgo utilizes conditional diffusion\n† indicates equal contribution.\nto generate multiple plausible full-body motions. This dis-\nentanglement of head and body pose eliminates the need\nfor training datasets with paired egocentric videos and 3D\nhuman motion, enabling us to leverage large-scale egocen-\ntric video datasets and motion capture datasets separately.\nMoreover, for systematic benchmarking, we develop a syn-\nthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired\negocentric videos and human motion. On both ARES and\nreal data, our EgoEgo model performs significantly better\nthan the current state-of-the-art methods.\n",
        "question": {
            "statement": "What is a major challenge in estimating 3D human motion from egocentric video sequences?",
            "options": [
                "The video sequence is too short",
                "The user's body is often unobserved by the front-facing camera",
                "The lighting conditions are poor",
                "The camera is not able to capture 3D motion"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Neural Dependencies Emerging from Learning Massive Categories\nRuili Feng1,3, Kecheng Zheng2,1, Kai Zhu1, Yujun Shen2, Jian Zhao 1, Yukun Huang1, Deli Zhao3,\nJingren Zhou3, Michael Jordan4, Zheng-Jun Zha1\n1University of Science and Technology of China, Hefei, China\n2Ant Group, 3Alibaba Group, Hangzhou, China\n4University of California, Berkeley\nruilifengustc@gmail.com,{zkcys001,kaizhu}@mail.ustc.edu.cn,\nshenyujun0302@gmail.com, {zj140,kevinh}@mail.ustc.edu.cn,\nzhaodeli@gmail.com,jingren.zhou@alibaba-inc.com,\njordan@cs.berkeley.edu,zhazj@ustc.edu.cn.\n× 𝜽𝒊𝟏\n𝒄𝑵\nResNet-50\nResNet-50\nSwin-T\nSwin-T\nResNet-50\nResNet-50\n𝒙~𝑝𝑑𝑎𝑡𝑎\n𝒙~𝑝𝑑𝑎𝑡𝑎\n(b) Neural Dependency between ResNet-50 and Swin-T\n(a) Neural Dependency within a ResNet-50\n𝒄𝒊𝟏\n𝒄𝒊𝒌\n𝒄𝒊𝟐\n× 𝜽𝒊𝟐\n× 𝜽𝒊𝒌\n𝒄𝟏\n𝒄𝟐\n𝒄𝟑\n𝒄𝑵−𝟏\n…\n𝒄𝑵\n𝒄𝟏\n𝒄𝟐\n𝒄𝟑\n𝒄𝑵−𝟏\n…\n𝒄𝟏\n𝒄𝟐\n𝒄𝟑\n𝒄𝑵−𝟏\n…\n…\n𝒄𝑵\n𝒄𝟏\n𝒄𝟐\n𝒄𝟑\n𝒄𝑵−𝟏\n…\n𝒄𝟏\n𝒄𝟐\n𝒄𝟑\n𝒄𝑵−𝟏\n…\n𝒄𝑵\n× 𝜽𝒊𝟏\n𝒄𝒊𝟏\n𝒄𝒊𝒌\n𝒄𝒊𝟐\n× 𝜽𝒊𝟐\n× 𝜽𝒊𝒌\n…\nOriginal Acc. ≈Acc. \nOriginal Acc. ≈Acc. \nFigure 1. Illustration of neural dependencies that emerge (a) within a single network and (b) between two independently learned\nnetworks. Taking the intra-network dependency as an instance, the logits predicted for the category “macaw” can be safely replaced by a\nlinear combination of the logits predicted for a few other categories, barely scarifying the accuracy.\nAbstract\nThis work presents two astonishing findings on neural\nnetworks learned for large-scale image classification. 1)\nGiven a well-trained model, the logits predicted for some\ncategory can be directly obtained by linearly combining\nthe predictions of a few other categories, which we call\nneural dependency. 2) Neural dependencies exist not only\nwithin a single model, but even between two independently\nlearned models, regardless of their architectures. Towards\na theoretical analysis of such phenomena, we demon-\nstrate that identifying neural dependencies is equivalent\nto solving the Covariance Lasso (CovLasso) regression\nproblem proposed in this paper.\nThrough investigating\nthe properties of the problem solution, we confirm that\nneural dependency is guaranteed by a redundant logit\ncovariance matrix, which condition is easily met given\nmassive categories, and that neural dependency is highly\nsparse, implying that one category correlates to only a few\nothers. We further empirically show the potential of neural\ndependencies in understanding internal data correlations,\ngeneralizing models to unseen categories, and improving\nmodel robustness with a dependency-derived regularizer.\nCode to reproduce the results in this paper is available at\nhttps://github.com/RuiLiFeng/Neural-Dependencies.\n",
        "question": {
            "statement": "What is the main characteristic of neural dependencies found in large-scale image classification models?",
            "options": [
                "They only exist within a single model architecture",
                "They are dense and exist across all categories",
                "They require manual feature engineering",
                "They are sparse and exist between a few correlated categories"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Unsupervised space-time network\nfor temporally-consistent segmentation of multiple motions\nEtienne Meunier\nInria, Rennes, France\netienne.meunier@inria.fr\nPatrick Bouthemy\nInria, Rennes, France\npatrick.bouthemy@inria.fr\nAbstract\nMotion segmentation is one of the main tasks in com-\nputer vision and is relevant for many applications. The op-\ntical flow (OF) is the input generally used to segment every\nframe of a video sequence into regions of coherent motion.\nTemporal consistency is a key feature of motion segmenta-\ntion, but it is often neglected. In this paper, we propose an\noriginal unsupervised spatio-temporal framework for mo-\ntion segmentation from optical flow that fully investigates\nthe temporal dimension of the problem. More specifically,\nwe have defined a 3D network for multiple motion segmen-\ntation that takes as input a sub-volume of successive opti-\ncal flows and delivers accordingly a sub-volume of coher-\nent segmentation maps. Our network is trained in a fully\nunsupervised way, and the loss function combines a flow\nreconstruction term involving spatio-temporal parametric\nmotion models, and a regularization term enforcing tempo-\nral consistency on the masks. We have specified an easy\ntemporal linkage of the predicted segments. Besides, we\nhave proposed a flexible and efficient way of coding U-nets.\nWe report experiments on several VOS benchmarks with\nconvincing quantitative results, while not using appearance\nand not training with any ground-truth data. We also high-\nlight through visual results the distinctive contribution of\nthe short- and long-term temporal consistency brought by\nour OF segmentation method.\n",
        "question": {
            "statement": "What is a common input used in computer vision to segment frames of a video sequence into regions of coherent motion?",
            "options": [
                "Audio signals",
                "Depth maps",
                "Appearance features",
                "Optical flow"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "NIKI: Neural Inverse Kinematics with Invertible Neural Networks\nfor 3D Human Pose and Shape Estimation\nJiefeng Li1∗\nSiyuan Bian1∗\nQi Liu1\nJiasheng Tang3\nFan Wang3\nCewu Lu12†\n1Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n3Alibaba Group\nAbstract\nWith the progress of 3D human pose and shape estima-\ntion, state-of-the-art methods can either be robust to oc-\nclusions or obtain pixel-aligned accuracy in non-occlusion\ncases. However, they cannot obtain robustness and mesh-\nimage alignment at the same time. In this work, we present\nNIKI (Neural Inverse Kinematics with Invertible Neural\nNetwork), which models bi-directional errors to improve\nthe robustness to occlusions and obtain pixel-aligned ac-\ncuracy. NIKI can learn from both the forward and inverse\nprocesses with invertible networks. In the inverse process,\nthe model separates the error from the plausible 3D pose\nmanifold for a robust 3D human pose estimation. In the\nforward process, we enforce the zero-error boundary condi-\ntions to improve the sensitivity to reliable joint positions for\nbetter mesh-image alignment. Furthermore, NIKI emulates\nthe analytical inverse kinematics algorithms with the twist-\nand-swing decomposition for better interpretability.\nEx-\nperiments on standard and occlusion-specific benchmarks\ndemonstrate the effectiveness of NIKI, where we exhibit ro-\nbust and well-aligned results simultaneously. Code is avail-\nable at https://github.com/Jeff-sjtu/NIKI.\n",
        "question": {
            "statement": "What is the main advantage of using invertible neural networks in 3D human pose and shape estimation?",
            "options": [
                "They enable real-time processing of high-resolution images",
                "They eliminate the need for training data",
                "They allow for bi-directional error modeling, improving robustness to occlusions and pixel-aligned accuracy",
                "They reduce the computational complexity of the model"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Probabilistic Knowledge Distillation of Face Ensembles\nJianqing Xu*\nShen Li*\nAilin Deng2\nMiao Xiong2\nJiaying Wu2\nJiaxiang Wu1\nShouhong Ding1\nBryan Hooi2\n1Youtu Lab, Tencent.\n2IDS and SoC, National University of Singapore.\n{joejqxu, willjxwu, ericshding}@tencent.com\n{shen.li, ailin, miao.xiong, jiayingwu}@u.nus.edu\nbhooi@comp.nus.edu.sg\nAbstract\nMean ensemble (i.e. averaging predictions from multiple\nmodels) is a commonly-used technique in machine learning\nthat improves the performance of each individual model. We\nformalize it as feature alignment for ensemble in open-set\nface recognition and generalize it into Bayesian Ensemble\nAveraging (BEA) through the lens of probabilistic modeling.\nThis generalization brings up two practical benefits that ex-\nisting methods could not provide: (1) the uncertainty of a\nface image can be evaluated and further decomposed into\naleatoric uncertainty and epistemic uncertainty, the latter\nof which can be used as a measure for out-of-distribution\ndetection of faceness; (2) a BEA statistic provably reflects\nthe aleatoric uncertainty of a face image, acting as a mea-\nsure for face image quality to improve recognition perfor-\nmance. To inherit the uncertainty estimation capability from\nBEA without the loss of inference efficiency, we propose\nBEA-KD, a student model to distill knowledge from BEA.\nBEA-KD mimics the overall behavior of ensemble members\nand consistently outperforms SOTA knowledge distillation\nmethods on various challenging benchmarks.\n",
        "question": {
            "statement": "What is the main advantage of using Bayesian Ensemble Averaging (BEA) over traditional mean ensemble method?",
            "options": [
                "simplified model architecture",
                "improved computational efficiency",
                "ability to evaluate and decompose uncertainty into aleatoric and epistemic components",
                "increased model capacity"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Meta Compositional Referring Expression Segmentation\nLi Xu1, Mark He Huang1,3, Xindi Shang2, Zehuan Yuan2, Ying Sun3, Jun Liu 1*\n1Singapore University of Technology and Design, Singapore\n2ByteDance\n3Institute for Infocomm Research (I2R) & Centre for Frontier AI Research (CFAR), A*STAR, Singapore\n{li xu, he huang}@mymail.sutd.edu.sg\n{shangxindi, yuanzehuan}@bytedance.com\nsuny@i2r.a-star.edu.sg, jun liu@sutd.edu.sg\nAbstract\nReferring expression segmentation aims to segment an\nobject described by a language expression from an image.\nDespite the recent progress on this task, existing models\ntackling this task may not be able to fully capture semantics\nand visual representations of individual concepts, which\nlimits their generalization capability, especially when han-\ndling novel compositions of learned concepts. In this work,\nthrough the lens of meta learning, we propose a Meta Com-\npositional Referring Expression Segmentation (MCRES)\nframework to enhance model compositional generalization\nperformance. Specifically, to handle various levels of novel\ncompositions, our framework first uses training data to con-\nstruct a virtual training set and multiple virtual testing sets,\nwhere data samples in each virtual testing set contain a\nlevel of novel compositions w.r.t. the virtual training set.\nThen, following a novel meta optimization scheme to opti-\nmize the model to obtain good testing performance on the\nvirtual testing sets after training on the virtual training set,\nour framework can effectively drive the model to better cap-\nture semantics and visual representations of individual con-\ncepts, and thus obtain robust generalization performance\neven when handling novel compositions. Extensive experi-\nments on three benchmark datasets demonstrate the effec-\ntiveness of our framework.\n",
        "question": {
            "statement": "What is the primary goal of the Meta Compositional Referring Expression Segmentation (MCRES) framework?",
            "options": [
                "To increase the complexity of the referring expressions",
                "To focus on single-concept objects rather than multi-concept objects",
                "To improve model generalization performance when handling novel compositions of learned concepts",
                "To reduce the size of the training dataset"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition\nXiao Yang1, Chang Liu2, Longlong Xu1, Yikai Wang1, Yinpeng Dong1,3†,\nNing Chen1, Hang Su1,4, Jun Zhu1,3,4†\n1 Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, Tsinghua-Bosch Joint ML Center,\nTHBI Lab, Tsinghua-China Mobile Communications Group Co., Ltd. Joint Institute, Tsinghua University\n2 Peking University\n3 RealAI\n4 Zhongguancun Laboratory\n{yangxiao19, xu-ll18}@mails.tsinghua.edu.cn\nchang.liu@stu.pku.edu.cn\nyikaiw@outlook.com\n{dongyinpeng, ningchen, suhangss, dcszj}@tsinghua.edu.cn\nAbstract\nFace recognition is a prevailing authentication solution\nin numerous biometric applications. Physical adversarial\nattacks, as an important surrogate, can identify the weak-\nnesses of face recognition systems and evaluate their ro-\nbustness before deployed. However, most existing physical\nattacks are either detectable readily or ineffective against\ncommercial recognition systems. The goal of this work is to\ndevelop a more reliable technique that can carry out an end-\nto-end evaluation of adversarial robustness for commercial\nsystems. It requires that this technique can simultaneously\ndeceive black-box recognition models and evade defensive\nmechanisms. To fulfill this, we design adversarial textured\n3D meshes (AT3D) with an elaborate topology on a human\nface, which can be 3D-printed and pasted on the attacker’s\nface to evade the defenses. However, the mesh-based op-\ntimization regime calculates gradients in high-dimensional\nmesh space, and can be trapped into local optima with un-\nsatisfactory transferability. To deviate from the mesh-based\nspace, we propose to perturb the low-dimensional coeffi-\ncient space based on 3D Morphable Model, which signifi-\ncantly improves black-box transferability meanwhile enjoy-\ning faster search efficiency and better visual quality. Exten-\nsive experiments in digital and physical scenarios show that\nour method effectively explores the security vulnerabilities\nof multiple popular commercial services, including three\nrecognition APIs, four anti-spoofing APIs, two prevailing\nmobile phones and two automated access control systems.\n",
        "question": {
            "statement": "What is the main goal of developing adversarial textured 3D meshes in the context of physical face recognition?",
            "options": [
                "To enhance the user experience of face recognition systems",
                "To improve the accuracy of face recognition models",
                "To develop new face recognition algorithms",
                "To evaluate the robustness of commercial face recognition systems"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "9"
            ]
        },
        "difference": 9,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Query-Centric Trajectory Prediction\nZikang Zhou1,2\nJianping Wang1,2\nYung-Hui Li3\nYu-Kai Huang4\n1City University of Hong Kong\n2City University of Hong Kong Shenzhen Research Institute\n3Hon Hai Research Institute\n4Carnegie Mellon University\nzikanzhou2-c@my.cityu.edu.hk\nAbstract\nPredicting the future trajectories of surrounding agents\nis essential for autonomous vehicles to operate safely. This\npaper presents QCNet, a modeling framework toward push-\ning the boundaries of trajectory prediction. First, we iden-\ntify that the agent-centric modeling scheme used by existing\napproaches requires re-normalizing and re-encoding the in-\nput whenever the observation window slides forward, lead-\ning to redundant computations during online prediction.\nTo overcome this limitation and achieve faster inference,\nwe introduce a query-centric paradigm for scene encoding,\nwhich enables the reuse of past computations by learning\nrepresentations independent of the global spacetime coordi-\nnate system. Sharing the invariant scene features among all\ntarget agents further allows the parallelism of multi-agent\ntrajectory decoding. Second, even given rich encodings of\nthe scene, existing decoding strategies struggle to capture\nthe multimodality inherent in agents’ future behavior, espe-\ncially when the prediction horizon is long. To tackle this\nchallenge, we first employ anchor-free queries to generate\ntrajectory proposals in a recurrent fashion, which allows\nthe model to utilize different scene contexts when decod-\ning waypoints at different horizons. A refinement module\nthen takes the trajectory proposals as anchors and leverages\nanchor-based queries to refine the trajectories further. By\nsupplying adaptive and high-quality anchors to the refine-\nment module, our query-based decoder can better deal with\nthe multimodality in the output of trajectory prediction. Our\napproach ranks 1st on Argoverse 1 and Argoverse 2 motion\nforecasting benchmarks, outperforming all methods on all\nmain metrics by a large margin. Meanwhile, our model can\nachieve streaming scene encoding and parallel multi-agent\ndecoding thanks to the query-centric design ethos.\n",
        "question": {
            "statement": "What is a major limitation of traditional agent-centric modeling schemes in trajectory prediction?",
            "options": [
                "Difficulty in capturing multimodal behavior",
                "Requiring re-normalization and re-encoding of input data whenever the observation window slides forward",
                "Lack of parallelism in multi-agent decoding",
                "Inability to handle long prediction horizons"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "10",
                "3",
                "2"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Implicit Surface Contrastive Clustering for LiDAR Point Clouds\nZaiwei Zhang*\nNuro Inc\nzaizhang@nuro.ai\nMin Bai\nAWS AI\nbaimin@amazon.com\nLi Erran Li\nAWS AI\nlilimam@amazon.com\nAbstract\nSelf-supervised pretraining on large unlabeled datasets\nhas shown tremendous success in improving the task per-\nformance of many 2D and small scale 3D computer vi-\nsion tasks. However, the popular pretraining approaches\nhave not been impactfully applied to outdoor LiDAR point\ncloud perception due to the latter’s scene complexity and\nwide range. We propose a new self-supervised pretrain-\ning method ISCC with two novel pretext tasks for LiDAR\npoint clouds. The first task uncovers semantic information\nby sorting local groups of points in the scene into a glob-\nally consistent set of semantically meaningful clusters using\ncontrastive learning, complemented by a second task which\nreasons about precise surfaces of various parts of the scene\nthrough implicit surface reconstruction to learn geomet-\nric structures. We demonstrate their effectiveness through\ntransfer learning on 3D object detection and semantic seg-\nmentation in real world LiDAR scenes. We further design\nan unsupervised semantic grouping task to show that our\napproach learns highly semantically meaningful features.\n",
        "question": {
            "statement": "What is the main goal of the proposed self-supervised pretraining method ISCC for LiDAR point clouds?",
            "options": [
                "To increase the accuracy of 2D computer vision tasks",
                "To develop a new type of LiDAR sensor",
                "To improve the performance of 3D computer vision tasks such as object detection and semantic segmentation",
                "To reduce the size of LiDAR point cloud datasets"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "9",
                "0"
            ]
        },
        "difference": 9,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Enhancing the Self-Universality for Transferable Targeted Attacks\nZhipeng Wei1,2, Jingjing Chen1,2†\n, Zuxuan Wu1,2, Yu-Gang Jiang1,2\n1Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University\n2Shanghai Collaborative Innovation Center of Intelligent Visual Computing\nzpwei21@m.fudan.edu.cn, {chenjingjing, zxwu, ygj}@fudan.edu.cn\nAbstract\nIn this paper, we propose a novel transfer-based tar-\ngeted attack method that optimizes the adversarial pertur-\nbations without any extra training efforts for auxiliary net-\nworks on training data.\nOur new attack method is pro-\nposed based on the observation that highly universal ad-\nversarial perturbations tend to be more transferable for\ntargeted attacks. Therefore, we propose to make the per-\nturbation to be agnostic to different local regions within\none image, which we called as self-universality. Instead\nof optimizing the perturbations on different images, opti-\nmizing on different regions to achieve self-universality can\nget rid of using extra data. Specifically, we introduce a\nfeature similarity loss that encourages the learned pertur-\nbations to be universal by maximizing the feature similar-\nity between adversarial perturbed global images and ran-\ndomly cropped local regions. With the feature similarity\nloss, our method makes the features from adversarial per-\nturbations to be more dominant than that of benign im-\nages, hence improving targeted transferability. We name the\nproposed attack method as Self-Universality (SU) attack.\nExtensive experiments demonstrate that SU can achieve\nhigh success rates for transfer-based targeted attacks. On\nImageNet-compatible dataset, SU yields an improvement of\n12% compared with existing state-of-the-art methods. Code\nis available at https://github.com/zhipeng-\nwei/Self-Universality.\n",
        "question": {
            "statement": "What is the primary goal of the Self-Universality (SU) attack method?",
            "options": [
                "To improve the transferability of targeted attacks by making adversarial perturbations more universal",
                "To optimize the performance of neural networks on clean images",
                "To reduce the computational resources required for generating adversarial examples",
                "To detect and defend against targeted attacks"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "On Data Scaling in Masked Image Modeling\nZhenda Xie13, Zheng Zhang3†, Yue Cao3†, Yutong Lin23, Yixuan Wei13, Qi Dai3, Han Hu3\n1Tsinghua University\n2Xi’an Jiaotong University\n3Microsoft Research Asia\n{t-zhxie, zhez, yuecao, t-yutonglin, t-yixuanwei, qi.dai, hanhu}@microsoft.com\nAbstract\nScaling properties have been one of the central issues\nin self-supervised pre-training, especially the data scalabil-\nity, which has successfully motivated the large-scale self-\nsupervised pre-trained language models and endowed them\nwith significant modeling capabilities. However, scaling\nproperties seem to be unintentionally neglected in the re-\ncent trending studies on masked image modeling (MIM),\nand some arguments even suggest that MIM cannot ben-\nefit from large-scale data. In this work, we try to break\ndown these preconceptions and systematically study the scal-\ning behaviors of MIM through extensive experiments, with\ndata ranging from 10% of ImageNet-1K to full ImageNet-\n22K, model parameters ranging from 49-million to one-\nbillion, and training length ranging from 125K to 500K\niterations. And our main findings can be summarized in\ntwo folds: 1) masked image modeling remains demand-\ning large-scale data in order to scale up computes and\nmodel parameters; 2) masked image modeling cannot bene-\nfit from more data under a non-overfitting scenario, which\ndiverges from the previous observations in self-supervised\npre-trained language models or supervised pre-trained vi-\nsion models. In addition, we reveal several intriguing prop-\nerties in MIM, such as high sample efficiency in large MIM\nmodels and strong correlation between pre-training vali-\ndation loss and transfer performance. We hope that our\nfindings could deepen the understanding of masked im-\nage modeling and facilitate future developments on large-\nscale vision models. Code and models will be available at\nhttps://github.com/microsoft/SimMIM.\n",
        "question": {
            "statement": "What is a key difference in terms of data scalability between masked image modeling and self-supervised pre-trained language models?",
            "options": [
                "Masked image modeling requires less computational resources than language models",
                "They have similar data scalability properties",
                "Masked image modeling does not benefit from more data under a non-overfitting scenario",
                "Both require small amounts of data to achieve good results"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "On Distillation of Guided Diffusion Models\nChenlin Meng1\nchenlin@cs.stanford.edu\nRobin Rombach2\nrobin@stability.ai\nRuiqi Gao3\nruiqig@google.com\nDiederik Kingma3\ndurk@google.com\nStefano Ermon1\nermon@cs.stanford.edu\nJonathan Ho3\njonathanho@google.com\nTim Salimans3\nsalimans@google.com\n1Stanford University\n2Stability AI & LMU Munich\n3 Google Research, Brain Team\nText-guided generation (2 steps)\nText-guided generation (4 steps)\nImage to image translation (3 steps)\nClass-conditional generation (1 step)\nImage inpainting (2 steps)\nInput\nInput\nMask\nResult 1\nResult 2\nOutput (different styles)\nText-guided generation (1 step)\nFigure 1. Distilled Stable Diffusion samples generated by our method. Our two-stage distillation approach is able to generate realistic\nimages using only 1 to 4 denoising steps on various tasks. Compared to standard classiﬁer-free guided diffusion models, we reduce the total\nnumber of sampling steps by at least 20⇥.\nAbstract\nClassiﬁer-free guided diffusion models have recently been\nshown to be highly effective at high-resolution image genera-\ntion, and they have been widely used in large-scale diffusion\n*Work partially done during an internship at Google\nframeworks including DALL·E 2, Stable Diffusion and Ima-\ngen. However, a downside of classiﬁer-free guided diffusion\nmodels is that they are computationally expensive at infer-\nence time since they require evaluating two diffusion models,\na class-conditional model and an unconditional model, tens\nto hundreds of times. To deal with this limitation, we pro-\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14297\npose an approach to distilling classiﬁer-free guided diffusion\nmodels into models that are fast to sample from: Given a\npre-trained classiﬁer-free guided model, we ﬁrst learn a sin-\ngle model to match the output of the combined conditional\nand unconditional models, and then we progressively distill\nthat model to a diffusion model that requires much fewer\nsampling steps. For standard diffusion models trained on the\npixel-space, our approach is able to generate images visually\ncomparable to that of the original model using as few as 4\nsampling steps on ImageNet 64x64 and CIFAR-10, achieving\nFID/IS scores comparable to that of the original model while\nbeing up to 256 times faster to sample from. For diffusion\nmodels trained on the latent-space (e.g., Stable Diffusion),\nour approach is able to generate high-ﬁdelity images using\nas few as 1 to 4 denoising steps, accelerating inference by\nat least 10-fold compared to existing methods on ImageNet\n256x256 and LAION datasets. We further demonstrate the\neffectiveness of our approach on text-guided image editing\nand inpainting, where our distilled model is able to generate\nhigh-quality results using as few as 2-4 denoising steps.\n",
        "question": {
            "statement": "What is a major drawback of classifier-free guided diffusion models?",
            "options": [
                "They are computationally expensive at inference time",
                "They produce low-quality images",
                "They are difficult to implement",
                "They require a lot of training data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Collaborative Static and Dynamic Vision-Language Streams\nfor Spatio-Temporal Video Grounding\nZihang Lin1, Chaolei Tan1, Jian-Fang Hu1,3,4*\n, Zhi Jin1, Tiancai Ye2, Wei-Shi Zheng1,3,4\n1Sun Yat-sen University, China\n2Tencent, China\n3Guangdong Province Key Laboratory of Information Security Technology, China\n4 Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\n{linzh59, tanchlei}@mail2.sysu.edu.cn, {hujf5, jinzh26}@mail.sysu.edu.cn,\ntiancaiye@tencent.com, wszheng@ieee.org\nAbstract\nSpatio-Temporal Video Grounding (STVG) aims to lo-\ncalize the target object spatially and temporally accord-\ning to the given language query. It is a challenging task\nin which the model should well understand dynamic visual\ncues (e.g., motions) and static visual cues (e.g., object ap-\npearances) in the language description, which requires ef-\nfective joint modeling of spatio-temporal visual-linguistic\ndependencies.\nIn this work, we propose a novel frame-\nwork in which a static vision-language stream and a dy-\nnamic vision-language stream are developed to collabora-\ntively reason the target tube. The static stream performs\ncross-modal understanding in a single frame and learns\nto attend to the target object spatially according to intra-\nframe visual cues like object appearances. The dynamic\nstream models visual-linguistic dependencies across mul-\ntiple consecutive frames to capture dynamic cues like mo-\ntions. We further design a novel cross-stream collabora-\ntive block between the two streams, which enables the static\nand dynamic streams to transfer useful and complementary\ninformation from each other to achieve collaborative rea-\nsoning. Experimental results show the effectiveness of the\ncollaboration of the two streams and our overall frame-\nwork achieves new state-of-the-art performance on both\nHCSTVG and VidSTG datasets.\n",
        "question": {
            "statement": "What is the primary goal of Spatio-Temporal Video Grounding (STVG)?",
            "options": [
                "To generate a language query based on the video content",
                "To classify videos into different categories",
                "To localize the target object spatially and temporally according to the given language query.",
                "To extract audio features from videos"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Weakly Supervised Class-agnostic Motion Prediction for Autonomous Driving\nRuibo Li1,2,\nHanyu Shi2,\nZiang Fu3,\nZhe Wang3,\nGuosheng Lin1,2∗\n1S-Lab, Nanyang Technological University\n2School of Computer Science and Engineering, Nanyang Technological University\n3SenseTime Research\nE-mail: ruibo001@e.ntu.edu.sg , gslin@ntu.edu.sg\nAbstract\nUnderstanding the motion behavior of dynamic environ-\nments is vital for autonomous driving, leading to increas-\ning attention in class-agnostic motion prediction in LiDAR\npoint clouds. Outdoor scenes can often be decomposed into\nmobile foregrounds and static backgrounds, which enables\nus to associate motion understanding with scene parsing.\nBased on this observation, we study a novel weakly su-\npervised motion prediction paradigm, where fully or par-\ntially (1%, 0.1%) annotated foreground/background binary\nmasks are used for supervision, rather than using expen-\nsive motion annotations. To this end, we propose a two-\nstage weakly supervised approach, where the segmentation\nmodel trained with the incomplete binary masks in Stage1\nwill facilitate the self-supervised learning of the motion\nprediction network in Stage2 by estimating possible mov-\ning foregrounds in advance. Furthermore, for robust self-\nsupervised motion learning, we design a Consistency-aware\nChamfer Distance loss by exploiting multi-frame informa-\ntion and explicitly suppressing potential outliers. Compre-\nhensive experiments show that, with fully or partially bi-\nnary masks as supervision, our weakly supervised models\nsurpass the self-supervised models by a large margin and\nperform on par with some supervised ones. This further\ndemonstrates that our approach achieves a good compro-\nmise between annotation effort and performance.\n",
        "question": {
            "statement": "What type of information is used to supervise the training of a motion prediction model in a weakly supervised approach?",
            "options": [
                "Foreground/background binary masks",
                "GPS and accelerometer readings",
                "Expensive motion annotations",
                "Fully labeled point cloud data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Learning A Sparse Transformer Network for Effective Image Deraining\nXiang Chen1\nHao Li1\nMingqiang Li2\nJinshan Pan1*\n1School of Computer Science and Engineering, Nanjing University of Science and Technology\n2Information Science Academy, China Electronics Technology Group Corporation\nAbstract\nTransformers-based methods have achieved significant\nperformance in image deraining as they can model the\nnon-local information which is vital for high-quality im-\nage reconstruction.\nIn this paper, we find that most ex-\nisting Transformers usually use all similarities of the to-\nkens from the query-key pairs for the feature aggrega-\ntion.\nHowever, if the tokens from the query are differ-\nent from those of the key, the self-attention values esti-\nmated from these tokens also involve in feature aggregation,\nwhich accordingly interferes with the clear image restora-\ntion. To overcome this problem, we propose an effective\nDeRaining network, Sparse Transformer (DRSformer) that\ncan adaptively keep the most useful self-attention values\nfor feature aggregation so that the aggregated features bet-\nter facilitate high-quality image reconstruction.\nSpecifi-\ncally, we develop a learnable top-k selection operator to\nadaptively retain the most crucial attention scores from the\nkeys for each query for better feature aggregation.\nSi-\nmultaneously, as the naive feed-forward network in Trans-\nformers does not model the multi-scale information that is\nimportant for latent clear image restoration, we develop\nan effective mixed-scale feed-forward network to gener-\nate better features for image deraining. To learn an en-\nriched set of hybrid features, which combines local con-\ntext from CNN operators, we equip our model with mix-\nture of experts feature compensator to present a coop-\neration refinement deraining scheme.\nExtensive experi-\nmental results on the commonly used benchmarks demon-\nstrate that the proposed method achieves favorable perfor-\nmance against state-of-the-art approaches. The source code\nand trained models are available at https://github.\ncom/cschenxiang/DRSformer.\n",
        "question": {
            "statement": "What is a limitation of traditional transformer-based methods in image deraining?",
            "options": [
                "They rely solely on convolutional neural networks",
                "They are unable to model non-local information",
                "They use all similarity values from query-key pairs for feature aggregation",
                "They are ineffective for low-quality image reconstruction"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "MD-VQA: Multi-Dimensional Quality Assessment for UGC Live Videos\nZicheng Zhang1,\n* Wei Wu2,\n* Wei Sun1,\n* Danyang Tu1, Wei Lu1,\nXiongkuo Min1, Ying Chen2, Guangtao Zhai1,3†\n1Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University \n2Alibaba Group\n3MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\n1{zzc1998,sunguwei,danyangtu,SJTU-Luwei,minxiongkuo,zhaiguangtao}@sjtu.edu.cn,\n2{guokui.ww,yingchen}@alibaba-inc.com\nAbstract\nUser-generated content (UGC) live videos are often\nbothered by various distortions during capture procedures\nand thus exhibit diverse visual qualities. Such source videos\nare further compressed and transcoded by media server\nproviders before being distributed to end-users. Because\nof the flourishing of UGC live videos, effective video qual-\nity assessment (VQA) tools are needed to monitor and per-\nceptually optimize live streaming videos in the distributing\nprocess. In this paper, we address UGC Live VQA prob-\nlems by constructing a first-of-a-kind subjective UGC Live\nVQA database and developing an effective evaluation tool.\nConcretely, 418 source UGC videos are collected in real\nlive streaming scenarios and 3,762 compressed ones at dif-\nferent bit rates are generated for the subsequent subjective\nVQA experiments.\nBased on the built database, we de-\nvelop a Multi-Dimensional VQA (MD-VQA) evaluator to\nmeasure the visual quality of UGC live videos from seman-\ntic, distortion, and motion aspects respectively. Extensive\nexperimental results show that MD-VQA achieves state-of-\nthe-art performance on both our UGC Live VQA database\nand existing compressed UGC VQA databases.\n",
        "question": {
            "statement": "What is the primary goal of developing effective Video Quality Assessment (VQA) tools for User-Generated Content (UGC) live videos?",
            "options": [
                "To improve the capturing procedure of UGC live videos",
                "To increase the bit rate of compressed UGC live videos",
                "To perceptually optimize live streaming videos in the distributing process",
                "To reduce the number of distortions in UGC live videos"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Few-shot Geometry-Aware Keypoint Localization\nXingzhe He1*\nGaurav Bharaj2\nDavid Ferman2\nHelge Rhodin1\nPablo Garrido2\n1 University of British Columbia\n2 Flawless AI\nAbstract\nSupervised keypoint localization methods rely on large\nmanually labeled image datasets, where objects can deform,\narticulate, or occlude. However, creating such large keypoint\nlabels is time-consuming and costly, and is often error-prone\ndue to inconsistent labeling. Thus, we desire an approach\nthat can learn keypoint localization with fewer yet consis-\ntently annotated images. To this end, we present a novel\nformulation that learns to localize semantically consistent\nkeypoint deﬁnitions, even for occluded regions, for varying\nobject categories. We use a few user-labeled 2D images as\ninput examples, which are extended via self-supervision us-\ning a larger unlabeled dataset. Unlike unsupervised methods,\nthe few-shot images act as semantic shape constraints for\nobject localization. Furthermore, we introduce 3D geometry-\naware constraints to uplift keypoints, achieving more ac-\ncurate 2D localization. Our general-purpose formulation\npaves the way for semantically conditioned generative mod-\neling and attains competitive or state-of-the-art accuracy\non several datasets, including human faces, eyes, animals,\ncars, and never-before-seen mouth interior (teeth) localiza-\ntion tasks, not attempted by the previous few-shot methods.\nProject page: https://xingzhehe.github.io/FewShot3DKP/\n",
        "question": {
            "statement": "What is the main limitation of traditional supervised keypoint localization methods?",
            "options": [
                "They are limited to specific object categories",
                "They are not suitable for real-time applications",
                "They are unable to handle object deformation",
                "They require large manually labeled image datasets"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "3",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "TensoIR: Tensorial Inverse Rendering\nHaian Jin∗1\nIsabella Liu∗2\nPeijia Xu3\nXiaoshuai Zhang2\nSongfang Han2\nSai Bi4\nXiaowei Zhou1\nZexiang Xu\n†4\nHao Su\n†2\n1 Zhejiang University\n2 UC San Diego\n3 Kingstar Technology Inc.\n4 Adobe Research\nAbstract\nWe propose TensoIR, a novel inverse rendering approach\nbased on tensor factorization and neural fields.\nUnlike\nprevious works that use purely MLP-based neural fields,\nthus suffering from low capacity and high computation\ncosts, we extend TensoRF, a state-of-the-art approach for\nradiance field modeling, to estimate scene geometry, sur-\nface reflectance, and environment illumination from multi-\nview images captured under unknown lighting conditions.\nOur approach jointly achieves radiance field reconstruction\nand physically-based model estimation, leading to photo-\nrealistic novel view synthesis and relighting results. Bene-\nfiting from the efficiency and extensibility of the TensoRF-\nbased representation, our method can accurately model\nsecondary shading effects (like shadows and indirect light-\ning) and generally support input images captured under sin-\ngle or multiple unknown lighting conditions. The low-rank\ntensor representation allows us to not only achieve fast and\ncompact reconstruction but also better exploit shared in-\nformation under an arbitrary number of capturing lighting\nconditions. We demonstrate the superiority of our method\nto baseline methods qualitatively and quantitatively on var-\nious challenging synthetic and real-world scenes.\n",
        "question": {
            "statement": "What is a key advantage of using a low-rank tensor representation in inverse rendering?",
            "options": [
                "It requires less computational power for radiance field reconstruction",
                "It allows for faster and more compact reconstruction",
                "It provides a more accurate estimation of surface reflectance",
                "It enables the modeling of complex shapes and geometries"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "5",
                "10",
                "3",
                "7"
            ]
        },
        "difference": 3,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Deep Depth Estimation from Thermal Image\nUkcheol Shin\nKAIST\nshinwc159@gmail.com\nJinsun Park\nPusan National University\njspark@pusan.ac.kr\nIn So Kweon\nKAIST\niskweon77@kaist.ac.kr\nAbstract\nRobust and accurate geometric understanding against\nadverse weather conditions is one top prioritized condi-\ntions to achieve a high-level autonomy of self-driving cars.\nHowever, autonomous driving algorithms relying on the vis-\nible spectrum band are easily impacted by weather and\nlighting conditions.\nA long-wave infrared camera, also\nknown as a thermal imaging camera, is a potential res-\ncue to achieve high-level robustness. However, the miss-\ning necessities are the well-established large-scale dataset\nand public benchmark results.\nTo this end, in this pa-\nper, we first built a large-scale Multi-Spectral Stereo (MS2)\ndataset, including stereo RGB, stereo NIR, stereo thermal,\nand stereo LiDAR data along with GNSS/IMU informa-\ntion. The collected dataset provides about 195K synchro-\nnized data pairs taken from city, residential, road, campus,\nand suburban areas in the morning, daytime, and nighttime\nunder clear-sky, cloudy, and rainy conditions. Secondly,\nwe conduct an exhaustive validation process of monocu-\nlar and stereo depth estimation algorithms designed on\nvisible spectrum bands to benchmark their performance\nin the thermal image domain. Lastly, we propose a uni-\nfied depth network that effectively bridges monocular depth\nand stereo depth tasks from a conditional random field\napproach perspective.\nOur dataset and source code are\navailable at https://github.com/UkcheolShin/\nMS2-MultiSpectralStereoDataset.\n",
        "question": {
            "statement": "What is a limitation of current autonomous driving algorithms that rely on visible spectrum bands?",
            "options": [
                "They are only effective in urban areas",
                "They are not compatible with thermal imaging cameras",
                "They require a lot of computational power",
                "They are easily impacted by weather and lighting conditions."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "High Fidelity 3D Hand Shape Reconstruction\nvia Scalable Graph Frequency Decomposition\nTianyu Luan1\nYuanhao Zhai1\nJingjing Meng1\nZhong Li2\nZhang Chen2\nYi Xu2\nJunsong Yuan1\n1State University of New York at Buffalo\n2OPPO US Research Center, InnoPeak Technology, Inc.\n{tianyulu,yzhai6,jmeng2,jsyuan}@buffalo.edu\n{zhong.li,zhang.chen,yi.xu}@oppo.com\nAbstract\nDespite the impressive performance obtained by recent\nsingle-image hand modeling techniques, they lack the capa-\nbility to capture sufficient details of the 3D hand mesh. This\ndeficiency greatly limits their applications when high-fidelity\nhand modeling is required, e.g., personalized hand model-\ning. To address this problem, we design a frequency split\nnetwork to generate 3D hand mesh using different frequency\nbands in a coarse-to-fine manner. To capture high-frequency\npersonalized details, we transform the 3D mesh into the\nfrequency domain, and propose a novel frequency decom-\nposition loss to supervise each frequency component. By\nleveraging such a coarse-to-fine scheme, hand details that\ncorrespond to the higher frequency domain can be preserved.\nIn addition, the proposed network is scalable, and can stop\nthe inference at any resolution level to accommodate dif-\nferent hardware with varying computational powers. To\nquantitatively evaluate the performance of our method in\nterms of recovering personalized shape details, we intro-\nduce a new evaluation metric named Mean Signal-to-Noise\nRatio (MSNR) to measure the signal-to-noise ratio of each\nmesh frequency component. Extensive experiments demon-\nstrate that our approach generates fine-grained details for\nhigh-fidelity 3D hand reconstruction, and our evaluation\nmetric is more effective for measuring mesh details com-\npared with traditional metrics. The code is available at\nhttps://github.com/tyluann/FreqHand.\n",
        "question": {
            "statement": "What is the main limitation of recent single-image hand modeling techniques?",
            "options": [
                "They are limited to specific hand poses",
                "They require multiple images as input",
                "They are unable to handle varying lighting conditions",
                "They cannot capture sufficient details of the 3D hand mesh"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Rethinking Image Super Resolution from\nLong-Tailed Distribution Learning Perspective\nYuanbiao Gou1, Peng Hu1, Jiancheng Lv1, Hongyuan Zhu2, Xi Peng1∗\n1 College of Computer Science, Sichuan University, China\n2 Institute for Infocomm Research (I2R), A*STAR, Singapore\n{gouyuanbiao, penghu.ml, hongyuanzhu.cn, pengx.gm}@gmail.com; lvjiancheng@scu.edu.cn\nAbstract\nExisting studies have empirically observed that the reso-\nlution of the low-frequency region is easier to enhance than\nthat of the high-frequency one. Although plentiful works\nhave been devoted to alleviating this problem, little under-\nstanding is given to explain it. In this paper, we try to give\na feasible answer from a machine learning perspective, i.e.,\nthe twin ﬁtting problem caused by the long-tailed pixel dis-\ntribution in natural images. With this explanation, we refor-\nmulate image super resolution (SR) as a long-tailed distri-\nbution learning problem and solve it by bridging the gaps\nof the problem between in low- and high-level vision tasks.\nAs a result, we design a long-tailed distribution learning so-\nlution, that rebalances the gradients from the pixels in the\nlow- and high-frequency region, by introducing a static and\na learnable structure prior. The learned SR model achieves\nbetter balance on the ﬁtting of the low- and high-frequency\nregion so that the overall performance is improved. In the\nexperiments, we evaluate the solution on four CNN- and one\nTransformer-based SR models w.r.t. six datasets and three\ntasks, and experimental results demonstrate its superiority.\n",
        "question": {
            "statement": "What is a common challenge in image super-resolution that existing studies have empirically observed?",
            "options": [
                "Image super-resolution is equally effective across all frequency regions",
                "There is no correlation between frequency regions and enhancement difficulty",
                "The resolution of the high-frequency region is easier to enhance than that of the low-frequency one",
                "The resolution of the low-frequency region is easier to enhance than that of the high-frequency one"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Tangentially Elongated Gaussian Belief Propagation\nfor Event-based Incremental Optical Flow Estimation\nJun Nagata† and Yusuke Sekikawa† DENSO IT LAB., INC., Japan.\nAbstract\nOptical ﬂow estimation is a fundamental functionality in\ncomputer vision.\nAn event-based camera, which asyn-\nchronously detects sparse intensity changes, is an ideal\ndevice for realizing low-latency estimation of the optical\nﬂow owing to its low-latency sensing mechanism.\nAn\nexisting method using local plane ﬁtting of events could\nutilize the sparsity to realize incremental updates for\nlow-latency estimation; however, its output is merely a\nnormal component of the full optical ﬂow.\nAn alterna-\ntive approach using a frame-based deep neural network\ncould estimate the full ﬂow; however, its intensive non-\nincremental dense operation prohibits the low-latency\nestimation. We propose tangentially elongated Gaussian\n(TEG) belief propagation (BP) that realizes incremental\nfull-ﬂow estimation. We model the probability of full ﬂow\nas the joint distribution of TEGs from the normal ﬂow\nmeasurements, such that the marginal of this distribution\nwith correct prior equals the full ﬂow. We formulate the\nmarginalization using a message-passing based on the\nBP to realize efﬁcient incremental updates using sparse\nmeasurements. In addition to the theoretical justiﬁcation,\nwe evaluate the effectiveness of the TEGBP in real-world\ndatasets; it outperforms SOTA incremental quasi-full ﬂow\nmethod by a large margin.\n(The code is available at\nhttps://github.com/DensoITLab/tegbp/).\n",
        "question": {
            "statement": "What type of camera is particularly well-suited for low-latency optical flow estimation due to its sensing mechanism?",
            "options": [
                "Event-based camera",
                "Laser scanner",
                "Stereo camera",
                "Frame-based camera"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "BEV-Guided Multi-Modality Fusion for Driving Perception\nYunze Man\nUIUC\nyunzem2@illinois.edu\nLiang-Yan Gui\nUIUC\nlgui@illinois.edu\nYu-Xiong Wang\nUIUC\nyxw@illinois.edu\nAbstract\nIntegrating multiple sensors and addressing diverse\ntasks in an end-to-end algorithm are challenging yet crit-\nical topics for autonomous driving.\nTo this end, we in-\ntroduce BEVGuide, a novel Bird’s Eye-View (BEV) repre-\nsentation learning framework, representing the first attempt\nto unify a wide range of sensors under direct BEV guid-\nance in an end-to-end fashion. Our architecture accepts in-\nput from a diverse sensor pool, including but not limited\nto Camera, Lidar and Radar sensors, and extracts BEV\nfeature embeddings using a versatile and general trans-\nformer backbone. We design a BEV-guided multi-sensor\nattention block to take queries from BEV embeddings and\nlearn the BEV representation from sensor-specific features.\nBEVGuide is efficient due to its lightweight backbone de-\nsign and highly flexible as it supports almost any input sen-\nsor configurations. Extensive experiments demonstrate that\nour framework achieves exceptional performance in BEV\nperception tasks with a diverse sensor set. Project page is\nat https://yunzeman.github.io/BEVGuide.\n",
        "question": {
            "statement": "What is the primary advantage of the BEVGuide framework in terms of sensor configuration?",
            "options": [
                "It only works with radar sensors",
                "It requires a specific combination of camera and lidar sensors",
                "It needs a minimum of five different types of sensors",
                "It supports almost any input sensor configuration"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "On the Importance of Accurate Geometry Data for Dense 3D Vision Tasks\nHyunJun Jung∗1, Patrick Ruhkamp∗1,2, Guangyao Zhai1, Nikolas Brasch1, Yitong Li1,\nYannick Verdie1,3, Jifei Song3, Yiren Zhou3, Anil Armagan3, Slobodan Ilic1,4,\nAles Leonardis3, Nassir Navab1, Benjamin Busam1,2\n1 Technical University of Munich, 2 3Dwe.ai, 3 Huawei Noah’s Ark Lab, 4 Siemens AG, ∗Equal Contribution\nhyunjun.jung@tum.de, p.ruhkamp@tum.de, guangyao.zhai@tum.de, b.busam@tum.de\nAbstract\nLearning-based methods to solve dense 3D vision prob-\nlems typically train on 3D sensor data. The respectively\nused principle of measuring distances provides advantages\nand drawbacks.\nThese are typically not compared nor\ndiscussed in the literature due to a lack of multi-modal\ndatasets. Texture-less regions are problematic for structure\nfrom motion and stereo, reflective material poses issues for\nactive sensing, and distances for translucent objects are in-\ntricate to measure with existing hardware. Training on in-\naccurate or corrupt data induces model bias and hampers\ngeneralisation capabilities. These effects remain unnoticed\nif the sensor measurement is considered as ground truth\nduring the evaluation. This paper investigates the effect of\nsensor errors for the dense 3D vision tasks of depth estima-\ntion and reconstruction. We rigorously show the significant\nimpact of sensor characteristics on the learned predictions\nand notice generalisation issues arising from various tech-\nnologies in everyday household environments. For evalu-\nation, we introduce a carefully designed dataset1 compris-\ning measurements from commodity sensors, namely D-ToF,\nI-ToF, passive/active stereo, and monocular RGB+P. Our\nstudy quantifies the considerable sensor noise impact and\npaves the way to improved dense vision estimates and tar-\ngeted data fusion.\n",
        "question": {
            "statement": "What is a common issue when training machine learning models for dense 3D vision tasks using real-world sensor data?",
            "options": [
                "Increased computational efficiency",
                "Improved model accuracy and robustness",
                "Model bias and hampered generalization capabilities",
                "Enhanced sensor durability"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Self-supervised AutoFlow\nHsin-Ping Huang1,2, Charles Herrmann1, Junhwa Hur1, Erika Lu1,\nKyle Sargent1, Austin Stone1, Ming-Hsuan Yang1,2, Deqing Sun1\n1Google Research\n2University of California, Merced\nModel (𝜑𝜃)\nSintel Final ↓\n2.41\n2.59\nKITTI ↓\n3.86\n4.22\nAutoFlow (𝜆)\nTarget data w/ GT\nSupervised AutoFlow\nModel (𝜑𝜃)\nSelf-AutoFlow (𝜆)\nProposed: Self-supervised AutoFlow\nDAVIS ↑\n50.4\n51.9\nTarget data w/o GT\n?\nFigure 1. Self-supervised AutoFlow learns to generate an optical ﬂow training set through self-supervision on the target domain. It\nperforms comparable to supervised AutoFlow on Sintel and KITTI without requiring ground truth (GT) and learns a better dataset for\nreal-world DAVIS, where GT is not available. We report optical ﬂow accuracy on Sintel and KITTI, and keypoint propagation accuracy on\nDAVIS.\nAbstract\nRecently, AutoFlow has shown promising results on\nlearning a training set for optical ﬂow, but requires ground\ntruth labels in the target domain to compute its search met-\nric.\nObserving a strong correlation between the ground\ntruth search metric and self-supervised losses, we introduce\nself-supervised AutoFlow to handle real-world videos with-\nout ground truth labels. Using self-supervised loss as the\nsearch metric, our self-supervised AutoFlow performs on\npar with AutoFlow on Sintel and KITTI where ground truth\nis available, and performs better on the real-world DAVIS\ndataset. We further explore using self-supervised AutoFlow\nin the (semi-)supervised setting and obtain competitive re-\nsults against the state of the art.\n",
        "question": {
            "statement": "What is the main advantage of self-supervised AutoFlow over traditional AutoFlow?",
            "options": [
                "It is more accurate than traditional AutoFlow on all datasets",
                "It does not require ground truth labels in the target domain",
                "It is faster than traditional AutoFlow",
                "It can only be used on synthetic datasets"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Towards Better Gradient Consistency for Neural Signed Distance Functions via\nLevel Set Alignment\nBaorui Ma1∗\n, Junsheng Zhou1∗, Yu-Shen Liu1†\n, Zhizhong Han2\n1School of Software, BNRist, Tsinghua University, Beijing, China\n2Department of Computer Science, Wayne State University, Detroit, USA\n{mbr18,zhoujs21}@mails.tsinghua.edu.cn, liuyushen@tsinghua.edu.cn, h312h@wayne.edu\nAbstract\nNeural signed distance functions (SDFs) have shown re-\nmarkable capability in representing geometry with detail-\ns. However, without signed distance supervision, it is still\na challenge to infer SDFs from point clouds or multi-view\nimages using neural networks. In this paper, we claim that\ngradient consistency in the ﬁeld, indicated by the parallelis-\nm of level sets, is the key factor affecting the inference ac-\ncuracy. Hence, we propose a level set alignment loss to\nevaluate the parallelism of level sets, which can be mini-\nmized to achieve better gradient consistency. Our novel-\nty lies in that we can align all level sets to the zero lev-\nel set by constraining gradients at queries and their pro-\njections on the zero level set in an adaptive way. Our in-\nsight is to propagate the zero level set to everywhere in the\nﬁeld through consistent gradients to eliminate uncertainty\nin the ﬁeld that is caused by the discreteness of 3D point\nclouds or the lack of observations from multi-view images.\nOur proposed loss is a general term which can be used up-\non different methods to infer SDFs from 3D point clouds\nand multi-view images. Our numerical and visual compar-\nisons demonstrate that our loss can signiﬁcantly improve\nthe accuracy of SDFs inferred from point clouds or multi-\nview images under various benchmarks.\nCode and data\nare available at https://github.com/mabaorui/\nTowardsBetterGradient.\n",
        "question": {
            "statement": "What is the main challenge in inferring Neural Signed Distance Functions (SDFs) from point clouds or multi-view images?",
            "options": [
                "Insufficient training data",
                "Limited computational resources",
                "Inability to represent complex geometries",
                "Lack of signed distance supervision"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "2",
                "8"
            ]
        },
        "difference": 6,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Human Pose as Compositional Tokens\nZigang Geng1,3 , Chunyu Wang3*, Yixuan Wei2,3, Ze Liu1,3, Houqiang Li1, Han Hu3*\n1University of Science and Technology of China\n2Tsinghua University\n3Microsoft Research Asia\nhttps://sites.google.com/view/pctpose\nAbstract\nHuman pose is typically represented by a coordinate vec-\ntor of body joints or their heatmap embeddings. While easy\nfor data processing, unrealistic pose estimates are admit-\nted due to the lack of dependency modeling between the\nbody joints. In this paper, we present a structured repre-\nsentation, named Pose as Compositional Tokens (PCT), to\nexplore the joint dependency. It represents a pose by M dis-\ncrete tokens with each characterizing a sub-structure with\nseveral interdependent joints (see Figure 1). The composi-\ntional design enables it to achieve a small reconstruction\nerror at a low cost. Then we cast pose estimation as a clas-\nsification task. In particular, we learn a classifier to pre-\ndict the categories of the M tokens from an image. A pre-\nlearned decoder network is used to recover the pose from\nthe tokens without further post-processing. We show that\nit achieves better or comparable pose estimation results as\nthe existing methods in general scenarios, yet continues to\nwork well when occlusion occurs, which is ubiquitous in\npractice. The code and models are publicly available at\nhttps://github.com/Gengzigang/PCT.\n",
        "question": {
            "statement": "What is the main advantage of representing human pose using compositional tokens?",
            "options": [
                "It requires less training data than other approaches",
                "It is more computationally efficient than other methods",
                "It can capture joint dependencies and reduce reconstruction error",
                "It can only be applied to simple poses"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "3",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Next3D: Generative Neural Texture Rasterization for 3D-Aware Head Avatars\nJingxiang Sun1\nXuan Wang2\nLizhen Wang14\nXiaoyu Li3\nYong Zhang3\nHongwen Zhang1\nYebin Liu1\n1Tsinghua University\n2Ant Group\n3Tencent AI Lab\n4NNKosmos\nFigure 1. Our 3D GAN synthesizes generative, high-quality, and 3D-consistent facial avatars from unstructured 2D images. Unlike current\nanimatable 3D GANs that only modify yaw-pitch head poses and facial expressions, our approach enables fine-grained control over full-\nhead rotations, facial expressions, eye blinks, and gaze directions with strict 3D consistency and a high level of photorealism. Our approach\nalso provides strong 3D priors for downstream tasks such as 3D-aware stylization.\nAbstract\n3D-aware generative adversarial networks (GANs) syn-\nthesize high-fidelity and multi-view-consistent facial images\nusing only collections of single-view 2D imagery. Towards\nfine-grained control over facial attributes, recent efforts\nincorporate 3D Morphable Face Model (3DMM) to de-\nscribe deformation in generative radiance fields either ex-\nplicitly or implicitly. Explicit methods provide fine-grained\nexpression control but cannot handle topological changes\ncaused by hair and accessories, while implicit ones can\nmodel varied topologies but have limited generalization\ncaused by the unconstrained deformation fields. We pro-\npose a novel 3D GAN framework for unsupervised learn-\ning of generative, high-quality and 3D-consistent facial\navatars from unstructured 2D images. To achieve both de-\nformation accuracy and topological flexibility, we propose\na 3D representation called Generative Texture-Rasterized\nTri-planes.\nThe proposed representation learns Genera-\ntive Neural Textures on top of parametric mesh templates\nand then projects them into three orthogonal-viewed feature\nplanes through rasterization, forming a tri-plane feature\nrepresentation for volume rendering. In this way, we com-\nbine both fine-grained expression control of mesh-guided\nexplicit deformation and the flexibility of implicit volumet-\nric representation. We further propose specific modules for\nmodeling mouth interior which is not taken into account\nby 3DMM. Our method demonstrates state-of-the-art 3D-\naware synthesis quality and animation ability through ex-\ntensive experiments. Furthermore, serving as 3D prior, our\nanimatable 3D representation boosts multiple applications\nincluding one-shot facial avatars and 3D-aware styliza-\ntion. Project page: https://mrtornado24.github.io/Next3D/.\nCode: https://github.com/MrTornado24/Next3D.\n",
        "question": {
            "statement": "What is the main limitation of explicit methods that incorporate 3D Morphable Face Model (3DMM) in generative radiance fields?",
            "options": [
                "They require large amounts of labeled data",
                "They have limited generalization due to the unconstrained deformation fields",
                "They cannot handle topological changes caused by hair and accessories",
                "They are unable to model varied facial expressions"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Bringing Inputs to Shared Domains for\n3D Interacting Hands Recovery in the Wild\nGyeongsik Moon\nMeta Reality Labs\nmks0601@gmail.com\nAbstract\nDespite recent achievements, existing 3D interacting\nhands recovery methods have shown results mainly on mo-\ntion capture (MoCap) environments, not on in-the-wild\n(ITW) ones. This is because collecting 3D interacting hands\ndata in the wild is extremely challenging, even for the 2D\ndata. We present InterWild, which brings MoCap and ITW\nsamples to shared domains for robust 3D interacting hands\nrecovery in the wild with a limited amount of ITW 2D/3D\ninteracting hands data. 3D interacting hands recovery con-\nsists of two sub-problems: 1) 3D recovery of each hand and\n2) 3D relative translation recovery between two hands. For\nthe first sub-problem, we bring MoCap and ITW samples to\na shared 2D scale space. Although ITW datasets provide\na limited amount of 2D/3D interacting hands, they contain\nlarge-scale 2D single hand data. Motivated by this, we use\na single hand image as an input for the first sub-problem\nregardless of whether two hands are interacting. Hence, in-\nteracting hands of MoCap datasets are brought to the 2D\nscale space of single hands of ITW datasets. For the sec-\nond sub-problem, we bring MoCap and ITW samples to a\nshared appearance-invariant space. Unlike the first sub-\nproblem, 2D labels of ITW datasets are not helpful for the\nsecond sub-problem due to the 3D translation’s ambiguity.\nHence, instead of relying on ITW samples, we amplify the\ngeneralizability of MoCap samples by taking only a geo-\nmetric feature without an image as an input for the second\nsub-problem. As the geometric feature is invariant to ap-\npearances, MoCap and ITW samples do not suffer from a\nhuge appearance gap between the two datasets. The code\nis publicly available1.\n",
        "question": {
            "statement": "What is a major challenge in collecting data for 3D interacting hands recovery in real-world scenarios?",
            "options": [
                "The task requires specialized software expertise",
                "The cost of motion capture equipment is prohibitively high",
                "There is a lack of interest in 3D interacting hands recovery research",
                "Collecting 3D data is difficult due to the complexity of real-world environments"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "WINNER: Weakly-supervised hIerarchical decompositioN and aligNment for\nspatio-tEmporal video gRounding\nMengze Li 1 *\nHan Wang 1*\nWenqiao Zhang 2\nJiaxu Miao 1\nZhou Zhao 1,3,4†\nShengyu Zhang 1†\nWei Ji 2†\nFei Wu 3,4,1\n1 Zhejiang University, 2 National university of Singapore,\n3 Shanghai Institute for Advanced Study of Zhejiang University, 4 Shanghai AI Laboratory\n{mengzeli, zhouzhao, sy zhang}@zju.edu.cn\nweiji0523@gmail.com\nAbstract\nSpatio-temporal video grounding aims to localize the\naligned visual tube corresponding to a language query.\nExisting techniques achieve such alignment by exploiting\ndense boundary and bounding box annotations, which can\nbe prohibitively expensive. To bridge the gap, we inves-\ntigate the weakly-supervised setting, where models learn\nfrom easily accessible video-language data without anno-\ntations. We identify that intra-sample spurious correlations\namong video-language components can be alleviated if the\nmodel captures the decomposed structures of video and lan-\nguage data. In this light, we propose a novel framework,\nnamely WINNER, for hierarchical video-text understand-\ning. WINNER first builds the language decomposition tree\nin a bottom-up manner, upon which the structural attention\nmechanism and top-down feature backtracking jointly build\na multi-modal decomposition tree, permitting a hierarchi-\ncal understanding of unstructured videos. The multi-modal\ndecomposition tree serves as the basis for multi-hierarchy\nlanguage-tube matching. A hierarchical contrastive learn-\ning objective is proposed to learn the multi-hierarchy cor-\nrespondence and distinguishment with intra-sample and\ninter-sample video-text decomposition structures, achieving\nvideo-language decomposition structure alignment. Exten-\nsive experiments demonstrate the rationality of our design\nand its effectiveness beyond state-of-the-art weakly super-\nvised methods, even some supervised methods.\n",
        "question": {
            "statement": "What is the main goal of Spatio-temporal video grounding?",
            "options": [
                "Classify videos into predefined categories",
                "Generate captions for videos based on their content",
                "Track objects across frames in a video",
                "Localize the aligned visual tube corresponding to a language query."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "2",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "All are Worth Words: A ViT Backbone for Diffusion Models\nFan Bao1, Shen Nie2, Kaiwen Xue2, Yue Cao3, Chongxuan Li2*, Hang Su1, Jun Zhu1∗\n1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center\n1Tsinghua-Bosch Joint ML Center, THBI Lab,Tsinghua University, Beijing, 100084 China\n2Gaoling School of Artificial Intelligence, Renmin University of China,\n2Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China\n3Beijing Academy of Artificial Intelligence\nbf19@mails.tsinghua.edu.cn; nieshen@ruc.edu.cn; {kevin.kaiwenxue, caoyue10}@gmail.com\nchongxuanli@ruc.edu.cn; {suhangss, dcszj}@tsinghua.edu.cn\nAbstract\nVision transformers (ViT) have shown promise in vari-\nous vision tasks while the U-Net based on a convolutional\nneural network (CNN) remains dominant in diffusion mod-\nels. We design a simple and general ViT-based architecture\n(named U-ViT) for image generation with diffusion mod-\nels. U-ViT is characterized by treating all inputs includ-\ning the time, condition and noisy image patches as tokens\nand employing long skip connections between shallow and\ndeep layers. We evaluate U-ViT in unconditional and class-\nconditional image generation, as well as text-to-image gen-\neration tasks, where U-ViT is comparable if not superior to\na CNN-based U-Net of a similar size. In particular, latent\ndiffusion models with U-ViT achieve record-breaking FID\nscores of 2.29 in class-conditional image generation on Im-\nageNet 256×256, and 5.48 in text-to-image generation on\nMS-COCO, among methods without accessing large exter-\nnal datasets during the training of generative models.\nOur results suggest that, for diffusion-based image mod-\neling, the long skip connection is crucial while the down-\nsampling and up-sampling operators in CNN-based U-Net\nare not always necessary. We believe that U-ViT can pro-\nvide insights for future research on backbones in diffu-\nsion models and benefit generative modeling on large scale\ncross-modality datasets.\n",
        "question": {
            "statement": "What is a key characteristic of the U-ViT architecture designed for image generation with diffusion models?",
            "options": [
                "Using downsampling and upsampling operators",
                "Employing short skip connections between layers",
                "Utilizing a CNN-based backbone",
                "Treating all inputs including time, condition, and noisy image patches as tokens"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Open-Vocabulary Point-Cloud Object Detection without 3D Annotation\nYuheng Lu1∗, Chenfeng Xu2∗, Xiaobao Wei1, Xiaodong Xie1,\nMasayoshi Tomizuka2, Kurt Keutzer2, Shanghang Zhang1†\n1National Key Laboratory for Multimedia Information Processing,\nSchool of Computer Science, Peking University\n2University of California Berkeley\n{yuhenglu, xiaobaowei, donxie, shanghang}@pku.edu.cn\n{xuchenfeng, tomizuka, keutzer}@berkeley.edu,\nAbstract\nThe goal of open-vocabulary detection is to identify\nnovel objects based on arbitrary textual descriptions. In this\npaper, we address open-vocabulary 3D point-cloud detec-\ntion by a dividing-and-conquering strategy, which involves:\n1) developing a point-cloud detector that can learn a gen-\neral representation for localizing various objects, and 2)\nconnecting textual and point-cloud representations to en-\nable the detector to classify novel object categories based\non text prompting.\nSpecifically, we resort to rich im-\nage pre-trained models, by which the point-cloud detec-\ntor learns localizing objects under the supervision of pre-\ndicted 2D bounding boxes from 2D pre-trained detectors.\nMoreover, we propose a novel de-biased triplet cross-modal\ncontrastive learning to connect the modalities of image,\npoint-cloud and text, thereby enabling the point-cloud de-\ntector to benefit from vision-language pre-trained models,\ni.e., CLIP. The novel use of image and vision-language pre-\ntrained models for point-cloud detectors allows for open-\nvocabulary 3D object detection without the need for 3D\nannotations. Experiments demonstrate that the proposed\nmethod improves at least 3.03 points and 7.47 points over\na wide range of baselines on the ScanNet and SUN RGB-D\ndatasets, respectively. Furthermore, we provide a compre-\nhensive analysis to explain why our approach works. Code\nis available at https://github.com/lyhdet/OV-3DET\n",
        "question": {
            "statement": "What is the main advantage of using image and vision-language pre-trained models for point-cloud detectors?",
            "options": [
                "They enable open-vocabulary 3D object detection without the need for 3D annotations",
                "They require less computational resources than traditional methods",
                "They improve the accuracy of 2D object detection",
                "They can only be applied to specific object categories"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Starting from Non-Parametric Networks for 3D Point Cloud Analysis\nRenrui Zhang1,5, Liuhui Wang2,6, Yali Wang4,5, Peng Gao5, Hongsheng Li1, Jianbo Shi†3\n1CUHK MMLab\n2Peking University\n3University of Pennsylvania\n4Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\n5Shanghai Artificial Intelligence Laboratory\n6Heisenberg Robotics\n{zhangrenrui, gaopeng}@pjlab.org.cn,\njshi@seas.upenn.edu\nwangliuhui0401@pku.edu.cn,\nhsli@ee.cuhk.edu.hk\nAbstract\nWe present a Non-parametric Network for 3D point\ncloud analysis, Point-NN, which consists of purely non-\nlearnable components: farthest point sampling (FPS), k-\nnearest neighbors (k-NN), and pooling operations, with\ntrigonometric functions. Surprisingly, it performs well on\nvarious 3D tasks, requiring no parameters or training,\nand even surpasses existing fully trained models.\nStart-\ning from this basic non-parametric model, we propose two\nextensions.\nFirst, Point-NN can serve as a base archi-\ntectural framework to construct Parametric Networks by\nsimply inserting linear layers on top.\nGiven the supe-\nrior non-parametric foundation, the derived Point-PN ex-\nhibits a high performance-efficiency trade-off with only a\nfew learnable parameters. Second, Point-NN can be re-\ngarded as a plug-and-play module for the already trained\n3D models during inference. Point-NN captures the comple-\nmentary geometric knowledge and enhances existing meth-\nods for different 3D benchmarks without re-training. We\nhope our work may cast a light on the community for un-\nderstanding 3D point clouds with non-parametric meth-\nods.\nCode is available at https://github.com/\nZrrSkywalker/Point-NN.\n",
        "question": {
            "statement": "What is the main characteristic of the Point-NN network for 3D point cloud analysis?",
            "options": [
                "It relies heavily on pre-trained models for initialization.",
                "It is specifically designed for 2D image analysis.",
                "It does not require any training or parameters.",
                "It uses a combination of convolutional and recurrent neural networks."
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "9",
                "0"
            ]
        },
        "difference": 9,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "CVT-SLR: Contrastive Visual-Textual Transformation for Sign Language\nRecognition with Variational Alignment\nJiangbin Zheng1, Yile Wang1,2, Cheng Tan1, Siyuan Li1,\nGe Wang1, Jun Xia1, Yidong Chen3, Stan Z. Li1*\n1AI Lab, Research Center for Industries of the Future, Westlake University\n2Institute for AI Industry Research (AIR), Tsinghua University\n3School of Informatics, Xiamen University\n{zhengjiangbin,wangyile,tancheng,lisiyuan,wangge,xiajun,Stan.ZQ.Li}@westlake.edu.cn\nydchen@xmu.edu.cn\nAbstract\nSign language recognition (SLR) is a weakly supervised\ntask that annotates sign videos as textual glosses. Recent\nstudies show that insufficient training caused by the lack\nof large-scale available sign datasets becomes the main\nbottleneck for SLR. Most SLR works thereby adopt pre-\ntrained visual modules and develop two mainstream solu-\ntions. The multi-stream architectures extend multi-cue vi-\nsual features, yielding the current SOTA performances but\nrequiring complex designs and might introduce potential\nnoise. Alternatively, the advanced single-cue SLR frame-\nworks using explicit cross-modal alignment between vi-\nsual and textual modalities are simple and effective, po-\ntentially competitive with the multi-cue framework.\nIn\nthis work, we propose a novel contrastive visual-textual\ntransformation for SLR, CVT-SLR, to fully explore the pre-\ntrained knowledge of both the visual and language modali-\nties. Based on the single-cue cross-modal alignment frame-\nwork, we propose a variational autoencoder (VAE) for pre-\ntrained contextual knowledge while introducing the com-\nplete pretrained language module.\nThe VAE implicitly\naligns visual and textual modalities while benefiting from\npretrained contextual knowledge as the traditional contex-\ntual module. Meanwhile, a contrastive cross-modal align-\nment algorithm is designed to explicitly enhance the consis-\ntency constraints. Extensive experiments on public datasets\n(PHOENIX-2014 and PHOENIX-2014T) demonstrate that\nour proposed CVT-SLR consistently outperforms existing\nsingle-cue methods and even outperforms SOTA multi-cue\nmethods. The source codes and models are available at\nhttps://github.com/binbinjiang/CVT-SLR.\n*Corresponding author.\n",
        "question": {
            "statement": "What is a major challenge faced by sign language recognition tasks?",
            "options": [
                "Complexity of sign languages",
                "Lack of pre-trained visual modules",
                "Insufficient training data",
                "Noise in multi-cue frameworks"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "HGFormer: Hierarchical Grouping Transformer for Domain Generalized\nSemantic Segmentation\nJian Ding1,2,3, Nan Xue1, Gui-Song Xia1,2*\n, Bernt Schiele3, Dengxin Dai3\n1NERCMS, School of Computer Science, Wuhan University, China\n2State Key Lab. LIESMARS, Wuhan University, China\n3Max Planck Institute for Informatics, Saarland Informatics Campus, Germany\n{jian.ding, xuenan, guisong.xia}@whu.edu.cn, {schiele, ddai}@mpi-inf.mpg.de\nAbstract\nCurrent semantic segmentation models have achieved\ngreat success under the independent and identically dis-\ntributed (i.i.d.) condition. However, in real-world appli-\ncations, test data might come from a different domain than\ntraining data. Therefore, it is important to improve model\nrobustness against domain differences.\nThis work stud-\nies semantic segmentation under the domain generalization\nsetting, where a model is trained only on the source domain\nand tested on the unseen target domain. Existing works\nshow that Vision Transformers are more robust than CNNs\nand show that this is related to the visual grouping property\nof self-attention. In this work, we propose a novel hierarchi-\ncal grouping transformer (HGFormer) to explicitly group\npixels to form part-level masks and then whole-level masks.\nThe masks at different scales aim to segment out both parts\nand a whole of classes. HGFormer combines mask clas-\nsiﬁcation results at both scales for class label prediction.\nWe assemble multiple interesting cross-domain settings by\nusing seven public semantic segmentation datasets. Exper-\niments show that HGFormer yields more robust semantic\nsegmentation results than per-pixel classiﬁcation methods\nand ﬂat-grouping transformers, and outperforms previous\nmethods signiﬁcantly. Code will be available at https:\n//github.com/dingjiansw101/HGFormer.\n",
        "question": {
            "statement": "What is the main goal of training a semantic segmentation model under the domain generalization setting?",
            "options": [
                "To achieve high accuracy on the source domain",
                "To enable the model to perform well on unseen target domains",
                "To reduce the number of parameters in the model",
                "To increase the speed of inference"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "NeRDi: Single-View NeRF Synthesis\nwith Language-Guided Diffusion as General Image Priors\nCongyue Deng2∗Chiyu “Max” Jiang1\nCharles R. Qi1\nXinchen Yan1\nYin Zhou1\nLeonidas Guibas2,3\nDragomir Anguelov1\n1Waymo\n2Stanford University\n3Google Research\nFigure 1. From left to right: We present a single-image NeRF synthesis framework for in-the-wild images without 3D supervision by\nleveraging general priors from large-scale image diffusion models. Given an input image, we optimize for a NeRF by minimizing an image\ndistribution loss for arbitrary-view renderings with the diffusion model conditioned on the input image. We design a two-section semantic\nfeature as the conditioning input to the diffusion model. The ﬁrst section is the image caption s0 which carries the overall semantics; the\nsecond section is a text embedding s∗extracted from the input image with textual inversion, which captures additional visual cues. Our\ntwo-section semantic feature provides an appropriate image prior, allowing the synthesis of a realistic NeRF coherent to the input image.\nAbstract\n2D-to-3D reconstruction is an ill-posed problem, yet hu-\nmans are good at solving this problem due to their prior\nknowledge of the 3D world developed over years. Driven by\nthis observation, we propose NeRDi, a single-view NeRF\nsynthesis framework with general image priors from 2D\ndiffusion models. Formulating single-view reconstruction\nas an image-conditioned 3D generation problem, we op-\ntimize the NeRF representations by minimizing a diffusion\nloss on its arbitrary view renderings with a pretrained im-\nage diffusion model under the input-view constraint. We\nleverage off-the-shelf vision-language models and introduce\na two-section language guidance as conditioning inputs to\nthe diffusion model. This is essentially helpful for improving\nmultiview content coherence as it narrows down the general\nimage prior conditioned on the semantic and visual features\nof the single-view input image. Additionally, we introduce\na geometric loss based on estimated depth maps to regular-\nize the underlying 3D geometry of the NeRF. Experimental\nresults on the DTU MVS dataset show that our method can\nsynthesize novel views with higher quality even compared\nto existing methods trained on this dataset. We also demon-\nstrate our generalizability in zero-shot NeRF synthesis for\nin-the-wild images.\n",
        "question": {
            "statement": "What is the primary advantage of using a two-section semantic feature as a conditioning input to the diffusion model in NeRDi?",
            "options": [
                "It helps to improve multiview content coherence",
                "It enables the use of unpretrained image diffusion models",
                "It allows for faster optimization of the NeRF representations",
                "It eliminates the need for geometric loss regularization"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Rethinking Out-of-distribution (OOD) Detection:\nMasked Image Modeling is All You Need\nJingyao Li1\nPengguang Chen2\nZexin He1\nShaozuo Yu1\nShu Liu2\nJiaya Jia1,2\nThe Chinese University of Hong Kong1\nSmartMore2\njingyao.li@link.cuhk.edu.hk\nleojia@cse.cuhk.edu.hk\nAbstract\nThe core of out-of-distribution (OOD) detection is to\nlearn the in-distribution (ID) representation, which is dis-\ntinguishable from OOD samples. Previous work applied\nrecognition-based methods to learn the ID features, which\ntend to learn shortcuts instead of comprehensive repre-\nsentations. In this work, we find surprisingly that simply\nusing reconstruction-based methods could boost the per-\nformance of OOD detection significantly. We deeply ex-\nplore the main contributors of OOD detection and find that\nreconstruction-based pretext tasks have the potential to pro-\nvide a generally applicable and efficacious prior, which\nbenefits the model in learning intrinsic data distributions\nof the ID dataset. Specifically, we take Masked Image Mod-\neling as a pretext task for our OOD detection framework\n(MOOD). Without bells and whistles, MOOD outperforms\nprevious SOTA of one-class OOD detection by 5.7%, multi-\nclass OOD detection by 3.0%, and near-distribution OOD\ndetection by 2.1%. It even defeats the 10-shot-per-class out-\nlier exposure OOD detection, although we do not include\nany OOD samples for our detection. Codes are available at\nhttps://github.com/lijingyao20010602/MOOD.\n",
        "question": {
            "statement": "What type of method has been found to improve out-of-distribution detection performance significantly?",
            "options": [
                "recognition-based methods",
                "reconstruction-based methods",
                "unsupervised learning methods",
                "supervised learning methods"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Look Before You Match: Instance Understanding Matters\nin Video Object Segmentation\nJunke Wang1,2, Dongdong Chen3, Zuxuan Wu1,2†, Chong Luo4, Chuanxin Tang4,\nXiyang Dai3, Yucheng Zhao4, Yujia Xie3, Lu Yuan3, Yu-Gang Jiang1,2†\n1Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University\n2Shanghai Collaborative Innovation Center of Intelligent Visual Computing\n3Microsoft Cloud + AI, 4Microsoft Research Asia\nAbstract\nExploring dense matching between the current frame\nand past frames for long-range context modeling, memory-\nbased methods have demonstrated impressive results in\nvideo object segmentation (VOS) recently.\nNevertheless,\ndue to the lack of instance understanding ability, the above\napproaches are oftentimes brittle to large appearance vari-\nations or viewpoint changes resulted from the movement of\nobjects and cameras. In this paper, we argue that instance\nunderstanding matters in VOS, and integrating it with\nmemory-based matching can enjoy the synergy, which is in-\ntuitively sensible from the definition of VOS task, i.e., identi-\nfying and segmenting object instances within the video. To-\nwards this goal, we present a two-branch network for VOS,\nwhere the query-based instance segmentation (IS) branch\ndelves into the instance details of the current frame and the\nVOS branch performs spatial-temporal matching with the\nmemory bank. We employ the well-learned object queries\nfrom IS branch to inject instance-specific information into\nthe query key, with which the instance-augmented match-\ning is further performed. In addition, we introduce a multi-\npath fusion block to effectively combine the memory readout\nwith multi-scale features from the instance segmentation de-\ncoder, which incorporates high-resolution instance-aware\nfeatures to produce final segmentation results. Our method\nachieves state-of-the-art performance on DAVIS 2016/2017\nval (92.6% and 87.1%), DAVIS 2017 test-dev (82.8%), and\nYouTube-VOS 2018/2019 val (86.3% and 86.3%), outper-\nforming alternative methods by clear margins.\n",
        "question": {
            "statement": "What is a limitation of memory-based methods in video object segmentation?",
            "options": [
                "They are brittle to large appearance variations or viewpoint changes",
                "They are unable to model long-range context",
                "They require manual annotation of objects",
                "They are limited to segmenting static objects"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "RIAV-MVS: Recurrent-Indexing an Asymmetric Volume for Multi-View Stereo\nChangjiang Cai, Pan Ji, Qingan Yan, Yi Xu\nOPPO US Research Center, InnoPeak Technology, Inc.\nAbstract\nThis paper presents a learning-based method for multi-\nview depth estimation from posed images. Our core idea is\na “learning-to-optimize” paradigm that iteratively indexes\na plane-sweeping cost volume and regresses the depth map\nvia a convolutional Gated Recurrent Unit (GRU). Since the\ncost volume plays a paramount role in encoding the multi-\nview geometry, we aim to improve its construction both at\npixel- and frame- levels. At the pixel level, we propose to\nbreak the symmetry of the Siamese network (which is typi-\ncally used in MVS to extract image features) by introducing\na transformer block to the reference image (but not to the\nsource images). Such an asymmetric volume allows the net-\nwork to extract global features from the reference image to\npredict its depth map. Given potential inaccuracies in the\nposes between reference and source images, we propose to\nincorporate a residual pose network to correct the relative\nposes. This essentially rectifies the cost volume at the frame\nlevel. We conduct extensive experiments on real-world MVS\ndatasets and show that our method achieves state-of-the-art\nperformance in terms of both within-dataset evaluation and\ncross-dataset generalization.\n",
        "question": {
            "statement": "What approach is used in RIAV-MVS to construct the cost volume at the pixel level?",
            "options": [
                "Asymmetric volume using a transformer block",
                "Recurrent Neural Network (RNN)",
                "Symmetric volume using a Siamese network",
                "Convolutional Neural Network (CNN)"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Object Detection with Self-Supervised Scene Adaptation\nZekun Zhang1\nMinh Hoai1,2\n1Stony Brook University, Stony Brook, NY 11794, USA\n2VinAI Research, Hanoi, Vietnam\n{zekzhang,minhhoai}@cs.stonyrbook.edu\nAbstract\nThis paper proposes a novel method to improve the per-\nformance of a trained object detector on scenes with fixed\ncamera perspectives based on self-supervised adaptation.\nGiven a specific scene, the trained detector is adapted using\npseudo-ground truth labels generated by the detector itself\nand an object tracker in a cross-teaching manner. When\nthe camera perspective is fixed, our method can utilize the\nbackground equivariance by proposing artifact-free object\nmixup as a means of data augmentation, and utilize accu-\nrate background extraction as an additional input modal-\nity. We also introduce a large-scale and diverse dataset\nfor the development and evaluation of scene-adaptive ob-\nject detection. Experiments on this dataset show that our\nmethod can improve the average precision of the original\ndetector, outperforming the previous state-of-the-art self-\nsupervised domain adaptive object detection methods by\na large margin.\nOur dataset and code are published at\nhttps://github.com/cvlab-stonybrook/scenes100.\n",
        "question": {
            "statement": "What technique is proposed in the paper to adapt a trained object detector to a specific scene with a fixed camera perspective?",
            "options": [
                "Generating pseudo-ground truth labels through a cross-teaching approach with an object tracker",
                "Fine-tuning the detector on a small set of annotated images",
                "Using a pre-trained model on a large dataset",
                "Applying transfer learning from a similar scene"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "9",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 9,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face\nRecognition\nZexin Li1* Bangjie Yin3* Taiping Yao3\nJunfeng Guo2\nShouhong Ding3† Simin Chen2\nCong Liu1†\n1University of California, Riverside\n2The University of Texas at Dallas\n3Tencent\n{zli536, congl}@ucr.edu, {junfeng.guo, simin.chen}@utdallas.edu,\n{bangjieyin, taipingyao, ericshding}@tencent.com\nAbstract\nA hard challenge in developing practical face recogni-\ntion (FR) attacks is due to the black-box nature of the target\nFR model, i.e., inaccessible gradient and parameter infor-\nmation to attackers. While recent research took an impor-\ntant step towards attacking black-box FR models through\nleveraging transferability, their performance is still limited,\nespecially against online commercial FR systems that can\nbe pessimistic (e.g., a less than 50% ASR–attack success\nrate on average). Motivated by this, we present Sibling-\nAttack, a new FR attack technique for the first time explores\na novel multi-task perspective (i.e., leveraging extra infor-\nmation from multi-correlated tasks to boost attacking trans-\nferability). Intuitively, Sibling-Attack selects a set of tasks\ncorrelated with FR and picks the Attribute Recognition (AR)\ntask as the task used in Sibling-Attack based on theoret-\nical and quantitative analysis. Sibling-Attack then devel-\nops an optimization framework that fuses adversarial gra-\ndient information through (1) constraining the cross-task\nfeatures to be under the same space, (2) a joint-task meta\noptimization framework that enhances the gradient com-\npatibility among tasks, and (3) a cross-task gradient stabi-\nlization method which mitigates the oscillation effect during\nattacking. Extensive experiments demonstrate that Sibling-\nAttack outperforms state-of-the-art FR attack techniques by\na non-trivial margin, boosting ASR by 12.61% and 55.77%\non average on state-of-the-art pre-trained FR models and\ntwo well-known, widely used commercial FR systems.\n",
        "question": {
            "statement": "What is a major challenge in developing effective attacks against face recognition systems?",
            "options": [
                "Limited computing resources",
                "Inadequate understanding of human facial features",
                "Insufficient training data",
                "The lack of access to the target model's parameters and gradients"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Extracting Class Activation Maps from Non-Discriminative Features as well\nZhaozheng Chen\nSingapore Management University\nzzchen.2019@phdcs.smu.edu.sg\nQianru Sun\nSingapore Management University\nqianrusun@smu.edu.sg\nAbstract\nExtracting class activation maps (CAM) from a classifi-\ncation model often results in poor coverage on foreground\nobjects, i.e., only the discriminative region (e.g., the “head”\nof “sheep”) is recognized and the rest (e.g., the “leg” of\n“sheep”) mistakenly as background. The crux behind is\nthat the weight of the classifier (used to compute CAM) cap-\ntures only the discriminative features of objects. We tackle\nthis by introducing a new computation method for CAM\nthat explicitly captures non-discriminative features as well,\nthereby expanding CAM to cover whole objects. Specifi-\ncally, we omit the last pooling layer of the classification\nmodel, and perform clustering on all local features of an\nobject class, where “local” means “at a spatial pixel posi-\ntion”. We call the resultant K cluster centers local proto-\ntypes — represent local semantics like the “head”, “leg”,\nand “body” of “sheep”. Given a new image of the class, we\ncompare its unpooled features to every prototype, derive K\nsimilarity matrices, and then aggregate them into a heatmap\n(i.e., our CAM). Our CAM thus captures all local features of\nthe class without discrimination. We evaluate it in the chal-\nlenging tasks of weakly-supervised semantic segmentation\n(WSSS), and plug it in multiple state-of-the-art WSSS meth-\nods, such as MCTformer [45] and AMN [26], by simply re-\nplacing their original CAM with ours. Our extensive exper-\niments on standard WSSS benchmarks (PASCAL VOC and\nMS COCO) show the superiority of our method: consistent\nimprovements with little computational overhead. Our code\nis provided at https://github.com/zhaozhengChen/LPCAM.\n",
        "question": {
            "statement": "What is the main limitation of traditional class activation maps (CAMs) when applied to object recognition tasks?",
            "options": [
                "They only capture the most discriminative regions of an object",
                "They require a large amount of labeled training data",
                "They are computationally expensive to generate",
                "They are limited to recognizing objects in a specific pose"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Multiplicative Fourier Level of Detail\nYishun Dou2\nZhong Zheng2\nQiaoqiao Jin1\nBingbing Ni1,2*\n1Shanghai Jiao Tong University, Shanghai 200240, China\n2 Huawei\nyishun.dou@gmail.com\nnibingbing@sjtu.edu.cn\nAbstract\nWe develop a simple yet surprisingly effective implicit\nrepresenting scheme called Multiplicative Fourier Level of\nDetail (MFLOD) motivated by the recent success of mul-\ntiplicative filter network. Built on multi-resolution feature\ngrid/volume (e.g., the sparse voxel octree), each level’s fea-\nture is first modulated by a sinusoidal function and then\nelement-wisely multiplied by a linear transformation of pre-\nvious layer’s representation in a layer-to-layer recursive\nmanner, yielding the scale-aggregated encodings for a sub-\nsequent simple linear forward to get final output. In con-\ntrast to previous hybrid representations relying on inter-\nleaved multilevel fusion and nonlinear activation-based de-\ncoding, MFLOD could be elegantly characterized as a lin-\near combination of sine basis functions with varying am-\nplitude, frequency, and phase upon the learned multilevel\nfeatures, thus offering great feasibility in Fourier analysis.\nComprehensive experimental results on implicit neural rep-\nresentation learning tasks including image fitting, 3D shape\nrepresentation, and neural radiance fields well demonstrate\nthe superior quality and generalizability achieved by the\nproposed MFLOD scheme.\n",
        "question": {
            "statement": "What is a key characteristic of the Multiplicative Fourier Level of Detail (MFLOD) representation scheme?",
            "options": [
                "It relies on interleaved multilevel fusion and nonlinear activation-based decoding.",
                "It can be elegantly characterized as a linear combination of sine basis functions with varying amplitude, frequency, and phase.",
                "It does not allow for subsequent simple linear forwarding to get final output.",
                "It uses a single resolution feature grid for all levels."
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Bootstrap Your Own Prior:\nTowards Distribution-Agnostic Novel Class Discovery\nMuli Yang1, Liancheng Wang1, Cheng Deng1*\n, and Hanwang Zhang2\n1School of Electronic Engineering, Xidian University, Xi’an, China\n2School of Computer Science and Engineering, Nanyang Technological University, Singapore\n{mlyang, lcwang9}@stu.xidian.edu.cn, chdeng@mail.xidian.edu.cn, hanwangzhang@ntu.edu.sg\nAbstract\nNovel Class Discovery (NCD) aims to discover unknown\nclasses without any annotation, by exploiting the transfer-\nable knowledge already learned from a base set of known\nclasses.\nExisting works hold an impractical assumption\nthat the novel class distribution prior is uniform, yet neglect\nthe imbalanced nature of real-world data. In this paper,\nwe relax this assumption by proposing a new challenging\ntask: distribution-agnostic NCD, which allows data drawn\nfrom arbitrary unknown class distributions and thus ren-\nders existing methods useless or even harmful. We tackle\nthis challenge by proposing a new method, dubbed “Boot-\nstrapping Your Own Prior (BYOP)”, which iteratively es-\ntimates the class prior based on the model prediction it-\nself. At each iteration, we devise a dynamic temperature\ntechnique that better estimates the class prior by encour-\naging sharper predictions for less-confident samples. Thus,\nBYOP obtains more accurate pseudo-labels for the novel\nsamples, which are beneficial for the next training itera-\ntion. Extensive experiments show that existing methods suf-\nfer from imbalanced class distributions, while BYOP1 out-\nperforms them by clear margins, demonstrating its effec-\ntiveness across various distribution scenarios.\n",
        "question": {
            "statement": "What is a key limitation of existing Novel Class Discovery methods?",
            "options": [
                "They require annotated data",
                "They are limited to specific domains",
                "They assume a uniform novel class distribution prior",
                "They are computationally expensive"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "HexPlane: A Fast Representation for Dynamic Scenes\nAng Cao\nJustin Johnson\nUniversity of Michigan, Ann Arbor\n{ancao, justincj}@umich.edu\nAbstract\nModeling and re-rendering dynamic 3D scenes is a chal-\nlenging task in 3D vision. Prior approaches build on NeRF\nand rely on implicit representations. This is slow since it re-\nquires many MLP evaluations, constraining real-world ap-\nplications. We show that dynamic 3D scenes can be ex-\nplicitly represented by six planes of learned features, lead-\ning to an elegant solution we call HexPlane. A HexPlane\ncomputes features for points in spacetime by fusing vec-\ntors extracted from each plane, which is highly efficient.\nPairing a HexPlane with a tiny MLP to regress output col-\nors and training via volume rendering gives impressive re-\nsults for novel view synthesis on dynamic scenes, match-\ning the image quality of prior work but reducing training\ntime by more than 100×. Extensive ablations confirm our\nHexPlane design and show that it is robust to different fea-\nture fusion mechanisms, coordinate systems, and decoding\nmechanisms. HexPlane is a simple and effective solution\nfor representing 4D volumes, and we hope they can broadly\ncontribute to modeling spacetime for dynamic 3D scenes.1\n",
        "question": {
            "statement": "What is the main advantage of using HexPlane representation for dynamic 3D scenes?",
            "options": [
                "Highly efficient computation",
                "Robustness to different lighting conditions",
                "Flexibility in choosing feature fusion mechanisms",
                "Ability to handle high-resolution images"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Accidental Light Probes\nHong-Xing Yu1\nSamir Agarwala1\nCharles Herrmann2\nRichard Szeliski2\nNoah Snavely2\nJiajun Wu1\nDeqing Sun2\n1Stanford University\n2Google Research\nAbstract\nRecovering lighting in a scene from a single image is\na fundamental problem in computer vision. While a mir-\nror ball light probe can capture omnidirectional lighting,\nlight probes are generally unavailable in everyday images.\nIn this work, we study recovering lighting from accidental\nlight probes (ALPs)—common, shiny objects like Coke cans,\nwhich often accidentally appear in daily scenes. We propose\na physically-based approach to model ALPs and estimate\nlighting from their appearances in single images. The main\nidea is to model the appearance of ALPs by photogram-\nmetrically principled shading and to invert this process via\ndifferentiable rendering to recover incidental illumination.\nWe demonstrate that we can put an ALP into a scene to allow\nhigh-fidelity lighting estimation. Our model can also recover\nlighting for existing images that happen to contain an ALP*.\nI’d rather be Shiny. — Tamatoa from Moana, 2016\n",
        "question": {
            "statement": "What type of objects can be used to recover lighting information in a scene?",
            "options": [
                "Transparent objects",
                "Opaque objects",
                "Mirrors",
                "Shiny objects"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "DiffusioNeRF: Regularizing Neural Radiance Fields\nwith Denoising Diffusion Models\nJamie Wynn\nDaniyar Turmukhambetov\nNiantic\nwww.github.com/nianticlabs/diffusionerf\nAbstract\nUnder good conditions, Neural Radiance Fields (NeRFs)\nhave shown impressive results on novel view synthesis tasks.\nNeRFs learn a scene’s color and density fields by minimiz-\ning the photometric discrepancy between training views and\ndifferentiable renderings of the scene. Once trained from\na sufficient set of views, NeRFs can generate novel views\nfrom arbitrary camera positions. However, the scene geom-\netry and color fields are severely under-constrained, which\ncan lead to artifacts, especially when trained with few input\nviews.\nTo alleviate this problem we learn a prior over scene\ngeometry and color, using a denoising diffusion model\n(DDM). Our DDM is trained on RGBD patches of the syn-\nthetic Hypersim dataset and can be used to predict the gra-\ndient of the logarithm of a joint probability distribution of\ncolor and depth patches. We show that, these gradients of\nlogarithms of RGBD patch priors serve to regularize geom-\netry and color of a scene. During NeRF training, random\nRGBD patches are rendered and the estimated gradient of\nthe log-likelihood is backpropagated to the color and den-\nsity fields. Evaluations on LLFF, the most relevant dataset,\nshow that our learned prior achieves improved quality in\nthe reconstructed geometry and improved generalization to\nnovel views. Evaluations on DTU show improved recon-\nstruction quality among NeRF methods.\n",
        "question": {
            "statement": "What technique is used to improve the quality of Neural Radiance Fields (NeRFs) in scenes with limited input views?",
            "options": [
                "Applying data augmentation techniques to the input views",
                "Increasing the number of neural network layers",
                "Using a different optimization algorithm for training",
                "Using a denoising diffusion model to learn a prior over scene geometry and color"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Superclass Learning with Representation Enhancement\nZeyu Gan1,2\nSuyun Zhao1,2, *\nJinlong Kang2\nLiyuan Shang2\nHong Chen1,2\nCuiping Li1,2\n1Key Lab of Data Engineering and Knowledge Engineering of MOE Renmin University of China\n2Renmin University of China, Beijing, China\n{zygan, zhaosuyun, kangjinlong, shangliyuan4032, chong, licuiping}@ruc.edu.cn\nAbstract\nIn many real scenarios, data are often divided into\na handful of artificial super categories in terms of ex-\npert knowledge rather than the representations of images.\nConcretely, a superclass may contain massive and vari-\nous raw categories, such as refuse sorting.\nDue to the\nlack of common semantic features, the existing classifica-\ntion techniques are intractable to recognize superclass with-\nout raw class labels, thus they suffer severe performance\ndamage or require huge annotation costs. To narrow this\ngap, this paper proposes a superclass learning framework,\ncalled SuperClass Learning with Representation Enhance-\nment(SCLRE), to recognize super categories by leverag-\ning enhanced representation.\nSpecifically, by exploiting\nthe self-attention technique across the batch, SCLRE col-\nlapses the boundaries of those raw categories and enhances\nthe representation of each superclass.\nOn the enhanced\nrepresentation space, a superclass-aware decision bound-\nary is then reconstructed. Theoretically, we prove that by\nleveraging attention techniques the generalization error of\nSCLRE can be bounded under superclass scenarios. Exper-\nimentally, extensive results demonstrate that SCLRE outper-\nforms the baseline and other contrastive-based methods on\nCIFAR-100 datasets and four high-resolution datasets.\n",
        "question": {
            "statement": "What is a major challenge in recognizing supercategories in classification tasks?",
            "options": [
                "The lack of common semantic features among raw categories",
                "The need for huge annotation costs",
                "The requirement for expert knowledge in all domains",
                "The presence of too many raw categories"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "5",
                "3",
                "2"
            ]
        },
        "difference": 5,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "WIRE: Wavelet Implicit Neural Representations\nVishwanath Saragadam, Daniel LeJeune, Jasper Tan, Guha Balakrishnan,\nAshok Veeraraghavan, Richard G. Baraniuk\nRice University\nhttps://vishwa91.github.io/wire\nAbstract\nImplicit neural representations (INRs) have recently ad-\nvanced numerous vision-related areas. INR performance\ndepends strongly on the choice of activation function em-\nployed in its MLP network. A wide range of nonlinearities\nhave been explored, but, unfortunately, current INRs de-\nsigned to have high accuracy also suffer from poor robust-\nness (to signal noise, parameter variation, etc.). Inspired by\nharmonic analysis, we develop a new, highly accurate and\nrobust INR that does not exhibit this tradeoff. Our Wavelet\nImplicit neural REpresentation (WIRE) uses as its ac-\ntivation function the complex Gabor wavelet that is well-\nknown to be optimally concentrated in space–frequency and\nto have excellent biases for representing images. A wide\nrange of experiments (image denoising, image inpainting,\nsuper-resolution, computed tomography reconstruction, im-\nage overfitting, and novel view synthesis with neural radi-\nance fields) demonstrate that WIRE defines the new state of\nthe art in INR accuracy, training time, and robustness.\n",
        "question": {
            "statement": "What property of the complex Gabor wavelet makes it suitable as an activation function in implicit neural representations?",
            "options": [
                "ability to learn from large datasets",
                "high computational efficiency",
                "simple implementation",
                "optimal concentration in space-frequency"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation\nLingting Zhu1* Xian Liu2* Xuanyu Liu1\nRui Qian2\nZiwei Liu3\nLequan Yu1†\n1The University of Hong Kong\n2The Chinese University of Hong Kong\n3S-Lab, Nanyang Technological University\n{ltzhu99, u3008631}@connect.hku.hk, lqyu@hku.hk,\n{alvinliu, qr021}@ie.cuhk.edu.hk,\nziwei.liu@ntu.edu.sg\nAbstract\nAnimating virtual avatars to make co-speech gestures\nfacilitates various applications in human-machine interac-\ntion. The existing methods mainly rely on generative adver-\nsarial networks (GANs), which typically suffer from noto-\nrious mode collapse and unstable training, thus making it\ndifficult to learn accurate audio-gesture joint distributions.\nIn this work, we propose a novel diffusion-based framework,\nnamed Diffusion Co-Speech Gesture (DiffGesture), to\neffectively capture the cross-modal audio-to-gesture asso-\nciations and preserve temporal coherence for high-fidelity\naudio-driven co-speech gesture generation. Specifically, we\nfirst establish the diffusion-conditional generation process\non clips of skeleton sequences and audio to enable the\nwhole framework. Then, a novel Diffusion Audio-Gesture\nTransformer is devised to better attend to the information\nfrom multiple modalities and model the long-term temporal\ndependency. Moreover, to eliminate temporal inconsistency,\nwe propose an effective Diffusion Gesture Stabilizer with\nan annealed noise sampling strategy. Benefiting from the\narchitectural advantages of diffusion models, we further\nincorporate implicit classifier-free guidance to trade off\nbetween diversity and gesture quality.\nExtensive experi-\nments demonstrate that DiffGesture achieves state-of-the-\nart performance, which renders coherent gestures with bet-\nter mode coverage and stronger audio correlations. Code is\navailable at https://github.com/Advocate99/DiffGesture.\n",
        "question": {
            "statement": "What is a common problem faced by generative adversarial networks (GANs) when learning audio-gesture joint distributions?",
            "options": [
                "high computational cost",
                "requirement for large amounts of labeled data",
                "mode collapse and unstable training",
                "difficulty in handling multimodal data"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Data-efficient Large Scale Place Recognition with Graded Similarity Supervision\nMaría Leyva-Vallina\nUniversity of Groningen\nm.leyva.vallina@rug.nl\nNicola Strisciuglio\nUniversity of Twente\nn.strisciuglio@utwente.nl\nNicolai Petkov\nUniversity of Groningen\nn.petkov@rug.nl\nAbstract\nVisual place recognition (VPR) is a fundamental task of\ncomputer vision for visual localization. Existing methods are\ntrained using image pairs that either depict the same place\nor not. Such a binary indication does not consider continu-\nous relations of similarity between images of the same place\ntaken from different positions, determined by the continuous\nnature of camera pose. The binary similarity induces a noisy\nsupervision signal into the training of VPR methods, which\nstall in local minima and require expensive hard mining al-\ngorithms to guarantee convergence. Motivated by the fact\nthat two images of the same place only partially share visual\ncues due to camera pose differences, we deploy an automatic\nre-annotation strategy to re-label VPR datasets. We compute\ngraded similarity labels for image pairs based on available\nlocalization metadata. Furthermore, we propose a new Gen-\neralized Contrastive Loss (GCL) that uses graded similarity\nlabels for training contrastive networks. We demonstrate\nthat the use of the new labels and GCL allow to dispense\nfrom hard-pair mining, and to train image descriptors that\nperform better in VPR by nearest neighbor search, obtaining\nsuperior or comparable results than methods that require\nexpensive hard-pair mining and re-ranking techniques.\n",
        "question": {
            "statement": "What is a limitation of traditional visual place recognition methods?",
            "options": [
                "They are limited to recognizing places indoors",
                "They are unable to recognize places at night",
                "They rely heavily on GPS data",
                "They do not account for continuous relations of similarity between images"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "MAGVLT: Masked Generative Vision-and-Language Transformer\nSungwoong Kim1,∗,†,\nDaejin Jo2,∗,\nDonghoon Lee2,∗,\nJongmin Kim2,∗\n1Department of Artificial Intelligence, Korea University, Seoul, South Korea\n2Kakao Brain, Seongnam, South Korea\nswkim01@korea.ac.kr, {daejin.jo, dhlee, jmkim}@kakaobrain.com\nAbstract\nWhile generative modeling on multimodal image-text\ndata has been actively developed with large-scale paired\ndatasets, there have been limited attempts to generate both\nimage and text data by a single model rather than a gener-\nation of one fixed modality conditioned on the other modal-\nity. In this paper, we explore a unified generative vision-\nand-language (VL) model that can produce both images\nand text sequences. Especially, we propose a generative VL\ntransformer based on the non-autoregressive mask predic-\ntion, named MAGVLT, and compare it with an autoregres-\nsive generative VL transformer (ARGVLT). In comparison\nto ARGVLT, the proposed MAGVLT enables bidirectional\ncontext encoding, fast decoding by parallel token predic-\ntions in an iterative refinement, and extended editing capa-\nbilities such as image and text infilling. For rigorous train-\ning of our MAGVLT with image-text pairs from scratch, we\ncombine the image-to-text, text-to-image, and joint image-\nand-text mask prediction tasks. Moreover, we devise two\nadditional tasks based on the step-unrolled mask prediction\nand the selective prediction on the mixture of two image-text\npairs. Experimental results on various downstream gener-\nation tasks of VL benchmarks show that our MAGVLT out-\nperforms ARGVLT by a large margin even with significant\ninference speedup. Particularly, MAGVLT achieves com-\npetitive results on both zero-shot image-to-text and text-to-\nimage generation tasks from MS-COCO by one moderate-\nsized model (fewer than 500M parameters) even without the\nuse of monomodal data and networks.\n",
        "question": {
            "statement": "What is the main advantage of the MAGVLT model compared to the ARGVLT model?",
            "options": [
                "larger number of parameters",
                "ability to generate only text sequences",
                "use of monomodal data and networks",
                "bidirectional context encoding and fast decoding"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Improving Zero-shot Generalization and Robustness of Multi-modal Models\nYunhao Ge1,2∗, Jie Ren1∗, Andrew Gallagher1, Yuxiao Wang1, Ming-Hsuan Yang1,\nHartwig Adam1, Laurent Itti2, Balaji Lakshminarayanan1†, Jiaping Zhao1†\n1Google Research\n2University of Southern California\n∗co-first author,\n†correspondence to {balajiln, jiapingz}@google.com\nAbstract\nMulti-modal image-text models such as CLIP and LiT\nhave demonstrated impressive performance on image clas-\nsification benchmarks and their zero-shot generalization\nability is particularly exciting. While the top-5 zero-shot\naccuracies of these models are very high, the top-1 accu-\nracies are much lower (over 25% gap in some cases). We\ninvestigate the reasons for this performance gap and find\nthat many of the failure cases are caused by ambiguity in\nthe text prompts. First, we develop a simple and efficient\nzero-shot post-hoc method to identify images whose top-1\nprediction is likely to be incorrect, by measuring consis-\ntency of the predictions w.r.t. multiple prompts and image\ntransformations. We show that our procedure better pre-\ndicts mistakes, outperforming the popular max logit base-\nline on selective prediction tasks. Next, we propose a simple\nand efficient way to improve accuracy on such uncertain im-\nages by making use of the WordNet hierarchy; specifically\nwe augment the original class by incorporating its parent\nand children from the semantic label hierarchy, and plug the\naugmentation into text prompts. We conduct experiments\non both CLIP and LiT models with five different ImageNet-\nbased datasets. For CLIP, our method improves the top-\n1 accuracy by 17.13% on the uncertain subset and 3.6%\non the entire ImageNet validation set. We also show that\nour method improves across ImageNet shifted datasets, four\nother datasets, and other model architectures such as LiT.\nThe proposed method1 is hyperparameter-free, requires\nno additional model training and can be easily scaled to\nother large multi-modal architectures. Code is available\nat https://github.com/gyhandy/Hierarchy-CLIP.\n",
        "question": {
            "statement": "What is a common reason why multi-modal image-text models such as CLIP and LiT perform poorly on certain image classification tasks?",
            "options": [
                "Inadequate image features extraction",
                "Insufficient training data",
                "Ambiguity in the text prompts",
                "Lack of robustness to image transformations"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Adaptive Graph Convolutional Subspace Clustering\nLai Wei, Zhengwei Chen, Jun Yin, Changming Zhu, Rigui Zhou, Jin Liu\nShanghai Maritime University\nHaigang Avenue 1550, Shanghai, China\nweilai@shmtu.edu.cn, 965976272@qq.com, junyin@shmtu.edu.cn, cmzhu@shmtu.edu.cn\nrgzhou@shmtu.edu.cn, jinliu@shmtu.edu.cn\nAbstract\nSpectral-type\nsubspace\nclustering\nalgorithms\nhave\nshown excellent performance in many subspace clustering\napplications. The existing spectral-type subspace cluster-\ning algorithms either focus on designing constraints for the\nreconstruction coefficient matrix or feature extraction meth-\nods for finding latent features of original data samples. In\nthis paper, inspired by graph convolutional networks, we\nuse the graph convolution technique to develop a feature\nextraction method and a coefficient matrix constraint si-\nmultaneously. And the graph-convolutional operator is up-\ndated iteratively and adaptively in our proposed algorithm.\nHence, we call the proposed method adaptive graph con-\nvolutional subspace clustering (AGCSC). We claim that,\nby using AGCSC, the aggregated feature representation of\noriginal data samples is suitable for subspace clustering,\nand the coefficient matrix could reveal the subspace struc-\nture of the original data set more faithfully. Finally, plenty\nof subspace clustering experiments prove our conclusions\nand show that AGCSC 1 outperforms some related methods\nas well as some deep models.\n",
        "question": {
            "statement": "What is the main advantage of the Adaptive Graph Convolutional Subspace Clustering (AGCSC) method?",
            "options": [
                "It is a type of k-means clustering algorithm",
                "It uses traditional machine learning techniques to improve clustering results",
                "It is a deep learning model that can handle high-dimensional data",
                "It can extract suitable features for subspace clustering and reveal the subspace structure of the data more faithfully"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "3",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "DIP: Dual Incongruity Perceiving Network for Sarcasm Detection\nChangsong Wen*\nGuoli Jia*\nJufeng Yang†\nTMCC, College of Computer Science, Nankai University, China\ndowndric@163.com, exped1230@gmail.com, yangjufeng@nankai.edu.cn\nAbstract\nSarcasm indicates the literal meaning is contrary to the\nreal attitude. Considering the popularity and complemen-\ntarity of image-text data, we investigate the task of multi-\nmodal sarcasm detection. Diﬀerent from other multi-modal\ntasks, for the sarcastic data, there exists intrinsic incon-\ngruity between a pair of image and text as demonstrated\nin psychological theories.\nTo tackle this issue, we pro-\npose a Dual Incongruity Perceiving (DIP) network con-\nsisting of two branches to mine the sarcastic information\nfrom factual and aﬀective levels. For the factual aspect,\nwe introduce a channel-wise reweighting strategy to ob-\ntain semantically discriminative embeddings, and leverage\ngaussian distribution to model the uncertain correlation\ncaused by the incongruity. The distribution is generated\nfrom the latest data stored in the memory bank, which can\nadaptively model the diﬀerence of semantic similarity be-\ntween sarcastic and non-sarcastic data. For the aﬀective\naspect, we utilize siamese layers with shared parameters\nto learn cross-modal sentiment information. Furthermore,\nwe use the polarity value to construct a relation graph\nfor the mini-batch, which forms the continuous contrastive\nloss to acquire aﬀective embeddings. Extensive experiments\ndemonstrate that our proposed method performs favorably\nagainst state-of-the-art approaches. Our code is released\non https://github.com/downdric/MSD.\n",
        "question": {
            "statement": "What is a key characteristic of sarcastic data that distinguishes it from other types of multimodal data?",
            "options": [
                "Ambiguous language usage",
                "Intrinsic incongruity between image and text",
                "Highly emotional tone",
                "Strong visual cues"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Compositor: Bottom-up Clustering and Compositing for Robust Part and Object\nSegmentation\nJu He1* Jieneng Chen1* Ming-Xian Lin2\nQihang Yu1\nAlan Yuille1\n1Johns Hopkins University\n2Chinese Academy of Sciences\nAbstract\nIn this work, we present a robust approach for joint part\nand object segmentation. Specifically, we reformulate ob-\nject and part segmentation as an optimization problem and\nbuild a hierarchical feature representation including pixel,\npart, and object-level embeddings to solve it in a bottom-up\nclustering manner. Pixels are grouped into several clus-\nters where the part-level embeddings serve as cluster cen-\nters. Afterwards, object masks are obtained by compositing\nthe part proposals. This bottom-up interaction is shown to\nbe effective in integrating information from lower seman-\ntic levels to higher semantic levels.\nBased on that, our\nnovel approach Compositor produces part and object seg-\nmentation masks simultaneously while improving the mask\nquality. Compositor achieves state-of-the-art performance\non PartImageNet and Pascal-Part by outperforming previ-\nous methods by around 0.9% and 1.3% on PartImageNet,\n0.4% and 1.7% on Pascal-Part in terms of part and object\nmIoU and demonstrates better robustness against occlusion\nby around 4.4% and 7.1% on part and object respectively.\n",
        "question": {
            "statement": "What is the primary advantage of using a bottom-up clustering approach in object and part segmentation?",
            "options": [
                "Integrating information from lower semantic levels to higher semantic levels",
                "Reducing computational complexity",
                "Improving feature extraction at the object level",
                "Increasing the number of object proposals"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Instance-Aware Domain Generalization for Face Anti-Spoofing\nQianyu Zhou1,3*\n, Ke-Yue Zhang2*\n, Taiping Yao2, Xuequan Lu4, Ran Yi1,\nShouhong Ding2†\n, Lizhuang Ma1†\n1Shanghai Jiao Tong University; 2Youtu Lab, Tencent;\n3 Shanghai Key Laboratory of Computer Software Evaluating and Testing; 4 Deakin University.\n1{zhouqianyu,ranyi}@sjtu.edu.cn, 1ma-lz@cs.sjtu.edu.cn,\n2{zkyezhang,taipingyao,ericshding}@tencent.com, 4xuequan.lu@deakin.edu.au\nAbstract\nFace anti-spoofing (FAS) based on domain generaliza-\ntion (DG) has been recently studied to improve the gener-\nalization on unseen scenarios. Previous methods typically\nrely on domain labels to align the distribution of each do-\nmain for learning domain-invariant representations. How-\never, artificial domain labels are coarse-grained and sub-\njective, which cannot reflect real domain distributions ac-\ncurately. Besides, such domain-aware methods focus on\ndomain-level alignment, which is not fine-grained enough\nto ensure that learned representations are insensitive to do-\nmain styles. To address these issues, we propose a novel\nperspective for DG FAS that aligns features on the instance\nlevel without the need for domain labels.\nSpecifically,\nInstance-Aware Domain Generalization framework is pro-\nposed to learn the generalizable feature by weakening the\nfeatures’ sensitivity to instance-specific styles. Concretely,\nwe propose Asymmetric Instance Adaptive Whitening to\nadaptively eliminate the style-sensitive feature correlation,\nboosting the generalization.\nMoreover, Dynamic Kernel\nGenerator and Categorical Style Assembly are proposed to\nfirst extract the instance-specific features and then generate\nthe style-diversified features with large style shifts, respec-\ntively, further facilitating the learning of style-insensitive\nfeatures. Extensive experiments and analysis demonstrate\nthe superiority of our method over state-of-the-art competi-\ntors. Code will be publicly available at this link.\n",
        "question": {
            "statement": "What is a limitation of traditional domain-aware face anti-spoofing methods?",
            "options": [
                "They rely on coarse-grained and subjective domain labels",
                "They are sensitive to lighting conditions",
                "They are not effective in real-world scenarios",
                "They require a large amount of training data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "3",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "OSRT: Omnidirectional Image Super-Resolution with\nDistortion-aware Transformer\nFanghua Yu1∗\nXintao Wang2*\nMingdeng Cao2,3\nGen Li4\nYing Shan2\nChao Dong1,5†\n1ShenZhen Key Lab of Computer Vision and Pattern Recognition\nShenzhen Institute of Advanced Technology, Chinese Academy of Sciences\n2ARC, Tencent PCG\n3The University of Tokyo\n4Platform Technologies, Tencent Online Video\n5Shanghai Artiﬁcial Intelligence Laboratory\nfanghuayu96@gmail.com, xintaowang@tencent.com, cmd@g.ecc.u-tokyo.ac.jp\n{genli, yingsshan}@tencent.com, chao.dong@siat.ac.cn\nAbstract\nOmnidirectional images (ODIs) have obtained lots of re-\nsearch interest for immersive experiences. Although ODIs\nrequire extremely high resolution to capture details of the\nentire scene, the resolutions of most ODIs are insufﬁcient.\nPrevious methods attempt to solve this issue by image\nsuper-resolution (SR) on equirectangular projection (ERP)\nimages. However, they omit geometric properties of ERP in\nthe degradation process, and their models can hardly gener-\nalize to real ERP images. In this paper, we propose Fisheye\ndownsampling, which mimics the real-world imaging pro-\ncess and synthesizes more realistic low-resolution samples.\nThen we design a distortion-aware Transformer (OSRT) to\nmodulate ERP distortions continuously and self-adaptively.\nWithout a cumbersome process, OSRT outperforms previ-\nous methods by about 0.2dB on PSNR. Moreover, we pro-\npose a convenient data augmentation strategy, which syn-\nthesizes pseudo ERP images from plain images. This simple\nstrategy can alleviate the over-ﬁtting problem of large net-\nworks and signiﬁcantly boost the performance of ODISR.\nExtensive experiments have demonstrated the state-of-the-\nart performance of our OSRT.\n",
        "question": {
            "statement": "What is a major limitation of previous omnidirectional image super-resolution methods?",
            "options": [
                "They require very high computational resources.",
                "They cannot handle color images.",
                "They are only applicable to fisheye lenses.",
                "They do not consider the geometric properties of equirectangular projections."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Rethinking the Approximation Error in 3D Surface Fitting for Point Cloud\nNormal Estimation\nHang Du*, Xuejun Yan*, Jingjing Wang, Di Xie†, and Shiliang Pu\nHikvision Research Institute, Hangzhou, China\n{duhang, yanxuejun, wangjingjing9, xiedi, pushiliang.hri}@hikvision.com\nAbstract\nMost existing approaches for point cloud normal esti-\nmation aim to locally fit a geometric surface and calcu-\nlate the normal from the fitted surface. Recently, learning-\nbased methods have adopted a routine of predicting point-\nwise weights to solve the weighted least-squares surface\nfitting problem.\nDespite achieving remarkable progress,\nthese methods overlook the approximation error of the fit-\nting problem, resulting in a less accurate fitted surface. In\nthis paper, we first carry out in-depth analysis of the ap-\nproximation error in the surface fitting problem. Then, in\norder to bridge the gap between estimated and precise sur-\nface normals, we present two basic design principles: 1) ap-\nplies the Z-direction Transform to rotate local patches for\na better surface fitting with a lower approximation error;\n2) models the error of the normal estimation as a learn-\nable term. We implement these two principles using deep\nneural networks, and integrate them with the state-of-the-\nart (SOTA) normal estimation methods in a plug-and-play\nmanner. Extensive experiments verify our approaches bring\nbenefits to point cloud normal estimation and push the fron-\ntier of state-of-the-art performance on both synthetic and\nreal-world datasets. The code is available at https://\ngithub.com/hikvision-research/3DVision.\n",
        "question": {
            "statement": "What is a common limitation of existing approaches for point cloud normal estimation?",
            "options": [
                "They overlook the approximation error of the fitting problem",
                "They are too computationally expensive",
                "They are limited to small-scale datasets",
                "They require manual tuning of hyperparameters"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "CCuantuMM: Cycle-Consistent Quantum-Hybrid Matching of Multiple Shapes\nHarshil Bhatia1,2 Edith Tretschk2 Zorah L¨\nahner3 Marcel Seelbach Benkner3\nMichael Moeller3 Christian Theobalt2 Vladislav Golyanik2\n1Indian Institute of Technology, Jodhpur\n2MPI for Informatics, SIC\n3Universit¨\nat Siegen\nAbstract\nJointly matching multiple, non-rigidly deformed 3D\nshapes is a challenging, NP-hard problem.\nA perfect\nmatching is necessarily cycle-consistent:\nFollowing the\npairwise point correspondences along several shapes must\nend up at the starting vertex of the original shape. Unfor-\ntunately, existing quantum shape-matching methods do not\nsupport multiple shapes and even less cycle consistency.\nThis paper addresses the open challenges and introduces\nthe first quantum-hybrid approach for 3D shape multi-\nmatching; in addition, it is also cycle-consistent. Its itera-\ntive formulation is admissible to modern adiabatic quantum\nhardware and scales linearly with the total number of input\nshapes. Both these characteristics are achieved by reduc-\ning the N-shape case to a sequence of three-shape match-\nings, the derivation of which is our main technical contribu-\ntion. Thanks to quantum annealing, high-quality solutions\nwith low energy are retrieved for the intermediate NP-\nhard objectives. On benchmark datasets, the proposed ap-\nproach significantly outperforms extensions to multi-shape\nmatching of a previous quantum-hybrid two-shape match-\ning method and is on-par with classical multi-matching\nmethods. Our source code is available at 4dqv.mpi-\ninf.mpg.de/CCuantuMM/.\n",
        "question": {
            "statement": "What is a key property of a perfect matching in jointly matching multiple, non-rigidly deformed 3D shapes?",
            "options": [
                "The shapes are matched based on their size and orientation",
                "The matching is done using a machine learning algorithm",
                "The shapes are matched based on their color and texture",
                "The pairwise point correspondences along several shapes end up at the starting vertex of the original shape"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Multi-Realism Image Compression with a Conditional Generator\nEirikur Agustsson\nGoogle Research\nReykjav´\nık, Iceland\neirikur@google.com\nDavid Minnen\nGoogle Research\nMountain View, USA\ndminnen@google.com\nGeorge Toderici\nGoogle Research\nMountain View, USA\ngtoderici@google.com\nFabian Mentzer\nGoogle Research\nZ¨\nurich, Switzerland\nmentzer@google.com\nAbstract\nBy optimizing the rate-distortion-realism trade-off, gen-\nerative compression approaches produce detailed, realis-\ntic images, even at low bit rates, instead of the blurry re-\nconstructions produced by rate-distortion optimized mod-\nels. However, previous methods do not explicitly control\nhow much detail is synthesized, which results in a common\ncriticism of these methods: users might be worried that a\nmisleading reconstruction far from the input image is gen-\nerated. In this work, we alleviate these concerns by train-\ning a decoder that can bridge the two regimes and navigate\nthe distortion-realism trade-off. From a single compressed\nrepresentation, the receiver can decide to either reconstruct\na low mean squared error reconstruction that is close to\nthe input, a realistic reconstruction with high perceptual\nquality, or anything in between. With our method, we set a\nnew state-of-the-art in distortion-realism, pushing the fron-\ntier of achievable distortion-realism pairs, i.e., our method\nachieves better distortions at high realism and better real-\nism at low distortion than ever before.\n",
        "question": {
            "statement": "What is a major concern about generative compression approaches that optimize the rate-distortion-realism trade-off?",
            "options": [
                "The reconstructed image is always of high quality",
                "The generated image may be misleadingly different from the original",
                "The approach requires a large amount of training data",
                "The compressed representation is always lossless"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "SparsePose: Sparse-View Camera Pose Regression and Refinement\nSamarth Sinha1\nJason Y. Zhang2\nAndrea Tagliasacchi1,3,4 Igor Gilitschenski1\nDavid B. Lindell1,5\n1University of Toronto\n2Carnegie Mellon University\n3Simon Fraser University\n4Google\n5Vector Institute\nAbstract\nCamera pose estimation is a key step in standard 3D re-\nconstruction pipelines that operate on a dense set of im-\nages of a single object or scene.\nHowever, methods for\npose estimation often fail when only a few images are avail-\nable because they rely on the ability to robustly identify and\nmatch visual features between image pairs.\nWhile these\nmethods can work robustly with dense camera views, cap-\nturing a large set of images can be time-consuming or im-\npractical.\nWe propose SparsePose for recovering accu-\nrate camera poses given a sparse set of wide-baseline im-\nages (fewer than 10). The method learns to regress initial\ncamera poses and then iteratively refine them after train-\ning on a large-scale dataset of objects (Co3D: Common\nObjects in 3D). SparsePose significantly outperforms con-\nventional and learning-based baselines in recovering ac-\ncurate camera rotations and translations. We also demon-\nstrate our pipeline for high-fidelity 3D reconstruction using\nonly 5-9 images of an object. Project webpage: https:\n//sparsepose.github.io/\n",
        "question": {
            "statement": "What is a common limitation of traditional camera pose estimation methods?",
            "options": [
                "They can only be applied to dynamic scenes",
                "They are restricted to indoor environments",
                "They require a large number of images",
                "They are limited to narrow-baseline images"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "UniSim: A Neural Closed-Loop Sensor Simulator\nZe Yang1,2*\nYun Chen1,2∗\nJingkang Wang1,2∗\nSivabalan Manivasagam1,2∗\nWei-Chiu Ma1,3\nAnqi Joyce Yang1,2\nRaquel Urtasun1,2\n1Waabi\n2University of Toronto\n3Massachusetts Institute of Technology\n{zyang, ychen, jwang, siva, weichiu, jyang, urtasun}@waabi.ai\nClosed-loop simulation for safety-critical scenarios\nActor Removal\nActor Modification\nSDV Sensor Lift\nSDV Lane Change\nScene manipulation with actor removal, modification, sensor configuration changes, and modified ego-trajectories\nOriginal Scenario\nModify Sedan Route\nInsert New Truck\nOriginal Scenario\nClosed-loop simulation for vehicle cut-in\nOriginal\nOriginal\nOriginal\nOriginal\nFigure 1. Top: UniSim takes recorded sensor data from a data collection platform and creates manipulable digital twins. Bottom: UniSim\ngenerates realistic, temporally consistent sensor simulations for new scenarios, enabling closed-loop autonomy evaluation. The autonomy\nsystem reactively interacts with the scenario, receives new sensor data, and changes lanes (see planned trajectory inset). All images and\nLiDAR in figure simulated by UniSim. Please refer to our project page https://waabi.ai/research/unisim/ for more results.\nAbstract\nRigorously testing autonomy systems is essential for\nmaking safe self-driving vehicles (SDV) a reality. It requires\none to generate safety critical scenarios beyond what can\nbe collected safely in the world, as many scenarios happen\nrarely on our roads. To accurately evaluate performance,\nwe need to test the SDV on these scenarios in closed-loop,\nwhere the SDV and other actors interact with each other\nat each timestep. Previously recorded driving logs provide\n*Indicates equal contribution.\na rich resource to build these new scenarios from, but for\nclosed loop evaluation, we need to modify the sensor data\nbased on the new scene configuration and the SDV’s deci-\nsions, as actors might be added or removed and the tra-\njectories of existing actors and the SDV will differ from the\noriginal log. In this paper, we present UniSim, a neural\nsensor simulator that takes a single recorded log captured\nby a sensor-equipped vehicle and converts it into a realistic\nclosed-loop multi-sensor simulation. UniSim builds neural\nfeature grids to reconstruct both the static background and\ndynamic actors in the scene, and composites them together\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n1389\nto simulate LiDAR and camera data at new viewpoints, with\nactors added or removed and at new placements. To better\nhandle extrapolated views, we incorporate learnable priors\nfor dynamic objects, and leverage a convolutional network\nto complete unseen regions. Our experiments show UniSim\ncan simulate realistic sensor data with small domain gap\non downstream tasks. With UniSim, we demonstrate, for the\nfirst time, closed-loop evaluation of an autonomy system on\nsafety-critical scenarios as if it were in the real world.\n",
        "question": {
            "statement": "What is the main goal of UniSim in the context of autonomous vehicles?",
            "options": [
                "To collect and store sensor data from real-world driving logs",
                "To simulate realistic sensor data for safety-critical scenarios",
                "To develop a new type of sensor for autonomous vehicles",
                "To improve the accuracy of GPS navigation in autonomous vehicles"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Open-set Fine-grained Retrieval via Prompting Vision-Language Evaluator\nShijie Wang1, Jianlong Chang2, Haojie Li1,3*, Zhihui Wang1, Wanli Ouyang4, Qi Tian2\n1International School of Information Science & Engineering, Dalian University of Technology, China\n2 Huawei Cloud & AI, China\n3College of Computer and Engineering, Shandong University of Science and Technology, China\n4 Sense Time Computer Vision Research Group, The University of Sydney, Australia\nAbstract\nOpen-set fine-grained retrieval is an emerging challenge\nthat requires an extra capability to retrieve unknown sub-\ncategories during evaluation. However, current works focus\non close-set visual concepts, where all the subcategories\nare pre-defined, and make it hard to capture discrimina-\ntive knowledge from unknown subcategories, consequently\nfailing to handle unknown subcategories in open-world sce-\nnarios. In this work, we propose a novel Prompting vision-\nLanguage Evaluator (PLEor) framework based on the re-\ncently introduced contrastive language-image pretraining\n(CLIP) model, for open-set fine-grained retrieval. PLEor\ncould leverage pre-trained CLIP model to infer the discrep-\nancies encompassing both pre-defined and unknown subcat-\negories, called category-specific discrepancies, and trans-\nfer them to the backbone network trained in the close-set\nscenarios. To make pre-trained CLIP model sensitive to\ncategory-specific discrepancies, we design a dual prompt\nscheme to learn a vision prompt specifying the category-\nspecific discrepancies, and turn random vectors with cate-\ngory names in a text prompt into category-specific discrep-\nancy descriptions. Moreover, a vision-language evaluator\nis proposed to semantically align the vision and text prompts\nbased on CLIP model, and reinforce each other. In addi-\ntion, we propose an open-set knowledge transfer to transfer\nthe category-specific discrepancies into the backbone net-\nwork using knowledge distillation mechanism. Quantitative\nand qualitative experiments show that our PLEor achieves\npromising performance on open-set fine-grained datasets.\n",
        "question": {
            "statement": "What is the main limitation of current approaches to fine-grained retrieval?",
            "options": [
                "They rely heavily on hand-crafted features",
                "They require large amounts of labeled data",
                "They are only applicable to specific domains",
                "They are limited to pre-defined subcategories"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "2",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting\nTarasha Khurana∗\nPeiyun Hu∗\nDavid Held\nDeva Ramanan\nCarnegie Mellon University\nFuture 4D Occupancy\nFuture Point Clouds\nHistorical LiDAR Sweeps\nt = {-T ... 0}\nt = 1\nt = {1 ... T}\nt = T\n. . .\nFigure 1. We focus on the problem of scene perception and forecasting for autonomous systems. As traditional methods rely on costly\nhuman annotations, we look towards emerging self-supervisable and scalable tasks such as point cloud forecasting [11,18,19]. However,\nwe argue that the formulation of point cloud forecasting unnecessarily focuses on learning the sensor extrinsics and intrinsics as part of\npredicting future point clouds, whereas the only physical quantity of central importance to autonomous perception is future spacetime 4D\noccupancy. We recast the task as that of 4D occupancy forecasting and show how using the same data as point cloud forecasting, one can\nlearn a meaningful and generic intermediate quantity – future spacetime 4D occupancy.\nAbstract\nPredicting how the world can evolve in the future is cru-\ncial for motion planning in autonomous systems. Classi-\ncal methods are limited because they rely on costly human\nannotations in the form of semantic class labels, bounding\nboxes, and tracks or HD maps of cities to plan their mo-\ntion — and thus are difﬁcult to scale to large unlabeled\ndatasets. One promising self-supervised task is 3D point\ncloud forecasting [11,18–20] from unannotated LiDAR se-\nquences. We show that this task requires algorithms to im-\nplicitly capture (1) sensor extrinsics (i.e., the egomotion\nof the autonomous vehicle), (2) sensor intrinsics (i.e., the\nsampling pattern speciﬁc to the particular LiDAR sensor),\nand (3) the shape and motion of other objects in the scene.\nBut autonomous systems should make predictions about the\nworld and not their sensors! To this end, we factor out (1)\nand (2) by recasting the task as one of spacetime (4D) oc-\n∗Equal contribution\ncupancy forecasting. But because it is expensive to obtain\nground-truth 4D occupancy, we “render” point cloud data\nfrom 4D occupancy predictions given sensor extrinsics and\nintrinsics, allowing one to train and test occupancy algo-\nrithms with unannotated LiDAR sequences. This also al-\nlows one to evaluate and compare point cloud forecasting\nalgorithms across diverse datasets, sensors, and vehicles.\n",
        "question": {
            "statement": "What is the main limitation of traditional methods for scene perception and forecasting in autonomous systems?",
            "options": [
                "They rely on costly human annotations",
                "They are unable to handle complex scenes",
                "They are limited to indoor environments",
                "They require high-quality cameras"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Learning to Predict Scene-Level Implicit 3D from Posed RGBD Data\nNilesh Kulkarni, Linyi Jin, Justin Johnson, David F. Fouhey\nUniversity of Michigan\nAbstract\nWe introduce a method that can learn to predict scene-\nlevel implicit functions for 3D reconstruction from posed\nRGBD data. At test time, our system maps a previously\nunseen RGB image to a 3D reconstruction of a scene via\nimplicit functions. While implicit functions for 3D recon-\nstruction have often been tied to meshes, we show that we\ncan train one using only a set of posed RGBD images. This\nsetting may help 3D reconstruction unlock the sea of ac-\ncelerometer+RGBD data that is coming with new phones.\nOur system, D2-DRDF, can match and sometimes outper-\nform current methods that use mesh supervision and shows\nbetter robustness to sparse data.\n",
        "question": {
            "statement": "What type of data does the proposed method D2-DRDF utilize for training to achieve 3D reconstruction?",
            "options": [
                "posed RGBD images",
                "unposed RGB images",
                "meshes",
                "accelerometer data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Global-to-Local Modeling for Video-based 3D Human Pose and Shape\nEstimation\nXiaolong Shen1,2∗, Zongxin Yang1, Xiaohan Wang1, Jianxin Ma2, Chang Zhou2, Yi Yang1\n1 ReLER, CCAI, Zhejiang University\n2 DAMO Academy, Alibaba Group\nAbstract\nVideo-based 3D human pose and shape estimations are\nevaluated by intra-frame accuracy and inter-frame smooth-\nness. Although these two metrics are responsible for dif-\nferent ranges of temporal consistency, existing state-of-\nthe-art methods treat them as a unified problem and use\nmonotonous modeling structures (e.g., RNN or attention-\nbased block) to design their networks.\nHowever, using\na single kind of modeling structure is difficult to balance\nthe learning of short-term and long-term temporal corre-\nlations, and may bias the network to one of them, lead-\ning to undesirable predictions like global location shift,\ntemporal inconsistency, and insufficient local details. To\nsolve these problems, we propose to structurally decou-\nple the modeling of long-term and short-term correlations\nin an end-to-end framework, Global-to-Local Transformer\n(GLoT). First, a global transformer is introduced with a\nMasked Pose and Shape Estimation strategy for long-term\nmodeling. The strategy stimulates the global transformer\nto learn more inter-frame correlations by randomly mask-\ning the features of several frames. Second, a local trans-\nformer is responsible for exploiting local details on the hu-\nman mesh and interacting with the global transformer by\nleveraging cross-attention. Moreover, a Hierarchical Spa-\ntial Correlation Regressor is further introduced to refine\nintra-frame estimations by decoupled global-local repre-\nsentation and implicit kinematic constraints.\nOur GLoT\nsurpasses previous state-of-the-art methods with the low-\nest model parameters on popular benchmarks, i.e., 3DPW,\nMPI-INF-3DHP, and Human3.6M. Codes are available at\nhttps://github.com/sxl142/GLoT.\n",
        "question": {
            "statement": "What is a limitation of using a single type of modeling structure for video-based 3D human pose and shape estimation?",
            "options": [
                "It increases the computational cost of the model",
                "It eliminates the need for hierarchical spatial correlation regressors",
                "It reduces the accuracy of intra-frame estimations",
                "It biases the network towards either short-term or long-term temporal correlations"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "9",
            "options": [
                "2",
                "0",
                "3",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Shifted Diffusion for Text-to-image Generation\nYufan Zhou 1*\n, Bingchen Liu 2, Yizhe Zhu 2, Xiao Yang 2, Changyou Chen 1, Jinhui Xu 1\n1 State University of New York at Buffalo\n2 ByteDance\n{yufanzho, changyou, jinhui}@buffalo.edu, {bingchenliu, yizhe.zhu, yangxiao.0}@bytedance.com\nAbstract\nWe present Corgi, a novel method for text-to-image gen-\neration. Corgi is based on our proposed shifted diffusion\nmodel, which achieves better image embedding generation\nfrom input text. Unlike the baseline diffusion model used in\nDALL-E 2, our method seamlessly encodes prior knowledge\nof the pre-trained CLIP model in its diffusion process by\ndesigning a new initialization distribution and a new tran-\nsition step of the diffusion. Compared to the strong DALL-E\n2 baseline, our method performs better in generating im-\nage embedding from the text in terms of both efficiency\nand effectiveness, resulting in better text-to-image genera-\ntion. Extensive large-scale experiments are conducted and\nevaluated in terms of both quantitative measures and hu-\nman evaluation, indicating a stronger generation ability of\nour method compared to existing ones. Furthermore, our\nmodel enables semi-supervised and language-free training\nfor text-to-image generation, where only part or none of the\nimages in the training dataset have an associated caption.\nTrained with only 1.7% of the images being captioned, our\nsemi-supervised model obtains FID results comparable to\nDALL-E 2 on zero-shot text-to-image generation evaluated\non MS-COCO. Corgi also achieves new state-of-the-art re-\nsults across different datasets on downstream language-free\ntext-to-image generation tasks, outperforming the previous\nmethod, Lafite, by a large margin.\n",
        "question": {
            "statement": "What is the main advantage of the shifted diffusion model over the baseline diffusion model in text-to-image generation?",
            "options": [
                "It only works with specific types of captions",
                "It can incorporate prior knowledge from pre-trained models",
                "It generates higher resolution images",
                "It requires less computational resources"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "8",
                "0",
                "2"
            ]
        },
        "difference": 6,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "TriVol: Point Cloud Rendering via Triple Volumes\nTao Hu 1∗\nXiaogang Xu1∗\nRuihang Chu 1\nJiaya Jia1,2\n1 The Chinese University of Hong Kong\n2 SmartMore\n{taohu,xgxu,rhchu,leojia}@cse.cuhk.edu.hk\nGraphics Rendering\nGraphics Rendering\nGround Truth\nGround Truth\nTriVol (Ours)\nTriVol (Ours)\nPoint-NeRF\nNPBG++\nPoint Cloud & Camera\nFigure 1. Given the colored point cloud of a category-specific scene or object, our TriVol can render photo-realistic images.\nAbstract\nExisting learning-based methods for point cloud render-\ning adopt various 3D representations and feature query-\ning mechanisms to alleviate the sparsity problem of point\nclouds. However, artifacts still appear in rendered images,\ndue to the challenges in extracting continuous and discrim-\ninative 3D features from point clouds. In this paper, we\npresent a dense while lightweight 3D representation, named\nTriVol, that can be combined with NeRF to render photo-\nrealistic images from point clouds. Our TriVol consists of\ntriple slim volumes, each of which is encoded from the point\ncloud. TriVol has two advantages. First, it fuses respective\nfields at different scales and thus extracts local and non-\nlocal features for discriminative representation.\nSecond,\nsince the volume size is greatly reduced, our 3D decoder\ncan be efficiently inferred, allowing us to increase the res-\nolution of the 3D space to render more point details. Ex-\ntensive experiments on different benchmarks with varying\nkinds of scenes/objects demonstrate our framework’s effec-\ntiveness compared with current approaches. Moreover, our\nframework has excellent generalization ability to render a\ncategory of scenes/objects without fine-tuning. The source\ncode is available at https://github.com/dvlab-\nresearch/TriVol.git.\n*Equal Contribution.\n",
        "question": {
            "statement": "What is a key advantage of using triple slim volumes in point cloud rendering?",
            "options": [
                "It eliminates the need for feature querying mechanisms",
                "It reduces the memory required for storing point cloud data",
                "It allows for efficient inference of the 3D decoder and increased resolution of the 3D space",
                "It enables real-time rendering of high-poly models"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "3",
                "8",
                "3"
            ]
        },
        "difference": 5,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "PointListNet: Deep Learning on 3D Point Lists\nHehe Fan1,2\nLinchao Zhu1\nYi Yang1\nMohan Kankanhalli2\n1Zhejiang University\n2National University of Singapore\nA\nsentence\nis\na\nlist\nof\nwords.\n(a) text: 1D word list\n(b) image: 2D pixel grid\n(c) 3D point cloud/set\n(d) protein: 3D point list\nFigure 1. Data structure comparison of text, image, point cloud and protein. (a) Texts are regular 1D lists of words. The position is the\nword’s sequential order in the text and the feature is the word itself. (b) Images are regular 2D grids of pixels. The position is the row and\ncolumn where the pixel is located and the feature is the color. (c) Point clouds are irregular 3D point sets. The position is the 3D coordinate\nand the feature is the point attributes. (d) Proteins can be seen as 3D point lists. The position of an amino acid involves a regular 1D\nsequential order and an irregular 3D coordinate. The feature is the amino acid (residue) type.\nAbstract\nDeep neural networks on regular 1D lists (e.g., natural\nlanguages) and irregular 3D sets (e.g., point clouds) have\nmade tremendous achievements. The key to natural lan-\nguage processing is to model words and their regular or-\nder dependency in texts. For point cloud understanding, the\nchallenge is to understand the geometry via irregular point\ncoordinates, in which point-feeding orders do not matter.\nHowever, there are a few kinds of data that exhibit both reg-\nular 1D list and irregular 3D set structures, such as proteins\nand non-coding RNAs. In this paper, we refer to them as\n3D point lists and propose a Transformer-style PointListNet\nto model them. First, PointListNet employs non-parametric\ndistance-based attention because we find sometimes it is the\ndistance, instead of the feature or type, that mainly deter-\nmines how much two points, e.g., amino acids, are corre-\nlated in the micro world. Second, different from the vanilla\nTransformer that directly performs a simple linear transfor-\nmation on inputs to generate values and does not explicitly\nmodel relative relations, our PointListNet integrates the 1D\norder and 3D Euclidean displacements into values. We con-\nduct experiments on protein fold classification and enzyme\nreaction classification. Experimental results show the effec-\ntiveness of the proposed PointListNet.\n",
        "question": {
            "statement": "What characteristic do proteins and non-coding RNAs exhibit that distinguishes them from other types of data?",
            "options": [
                "Only irregular 3D set structure",
                "Only regular 1D list structure",
                "Neither regular 1D list nor irregular 3D set structure",
                "Both regular 1D list and irregular 3D set structures"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "3DAvatarGAN: Bridging Domains for Personalized Editable Avatars\nRameen Abdal²1\nHsin-Ying Lee2\nPeihao Zhu²1\nMenglei Chai2\nAliaksandr Siarohin2\nPeter Wonka1\nSergey Tulyakov2\n1KAUST\n2Snap Inc.\nFigure 1. Editable 3D avatars.\nWe present 3DAvatarGAN, a 3D GAN able to produce and edit personalized 3D avatars from a single\nphotograph (real or generated). Our method distills information from a 2D-GAN trained on 2D artistic datasets like Caricatures, Pixar\ntoons, Cartoons, Comics etc. and requires no camera annotations.\nAbstract\nModern 3D-GANs synthesize geometry and texture by\ntraining on large-scale datasets with a consistent structure.\nTraining such models on stylized, artistic data, with often\nunknown, highly variable geometry, and camera informa-\ntion has not yet been shown possible. Can we train a 3D\nGAN on such artistic data, while maintaining multi-view\nconsistency and texture quality? To this end, we propose\nan adaptation framework, where the source domain is a\npre-trained 3D-GAN, while the target domain is a 2D-GAN\ntrained on artistic datasets.\nWe, then, distill the knowl-\nedge from a 2D generator to the source 3D generator. To\ndo that, we first propose an optimization-based method to\nalign the distributions of camera parameters across do-\nmains. Second, we propose regularizations necessary to\nlearn high-quality texture, while avoiding degenerate ge-\nometric solutions, such as flat shapes.\nThird, we show\na deformation-based technique for modeling exaggerated\ngeometry of artistic domains, enablingÐas a byproductÐ\npersonalized geometric editing. Finally, we propose a novel\ninversion method for 3D-GANs linking the latent spaces of\nthe source and the target domains. Our contributionsÐfor\nthe first timeÐallow for the generation, editing, and anima-\ntion of personalized artistic 3D avatars on artistic datasets.\nProject Page: https:/rameenabdal.github.io/3DAvatarGAN\n",
        "question": {
            "statement": "What is a key benefit of the 3DAvatarGAN approach?",
            "options": [
                "Personalized geometric editing",
                "Only generating realistic 3D avatars",
                "Texture quality improvement alone",
                "Multi-view consistency alone"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "2",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Target-referenced Reactive Grasping for Dynamic Objects\nJirong Liu1,2, Ruo Zhang2, Hao-Shu Fang2, Minghao Gou2, Hongjie Fang2,\nChenxi Wang2,3, Sheng Xu2, Hengxu Yan2, Cewu Lu1,2†\n1Shanghai Qi Zhi institute, 2Shanghai Jiao Tong University, 3Flexiv Robotics, LTD\njirong@sjtu.edu.cn, {ruozhang0608, fhaoshu}@gmail.com,\n{gmh2015, galaxies, wcx1997, xs1020, hengxuyan, lucewu}@sjtu.edu.cn\nAbstract\nReactive grasping, which enables the robot to success-\nfully grasp dynamic moving objects, is of great interest in\nrobotics. Current methods mainly focus on the temporal\nsmoothness of the predicted grasp poses but few consider\ntheir semantic consistency.\nConsequently, the predicted\ngrasps are not guaranteed to fall on the same part of the\nsame object, especially in cluttered scenes. In this paper,\nwe propose to solve reactive grasping in a target-referenced\nsetting by tracking through generated grasp spaces. Given\na targeted grasp pose on an object and detected grasp\nposes in a new observation, our method is composed of two\nstages: 1) discovering grasp pose correspondences through\nan attentional graph neural network and selecting the one\nwith the highest similarity with respect to the target pose; 2)\nrefining the selected grasp poses based on target and histor-\nical information. We evaluate our method on a large-scale\nbenchmark GraspNet-1Billion. We also collect 30 scenes\nof dynamic objects for testing. The results suggest that our\nmethod outperforms other representative methods. Further-\nmore, our real robot experiments achieve an average suc-\ncess rate of over 80 percent. Code and demos are available\nat: https://graspnet.net/reactive.\n",
        "question": {
            "statement": "What is a key limitation of current methods for reactive grasping of dynamic objects?",
            "options": [
                "They are limited to grasping static objects.",
                "They require prior knowledge of the object's shape and size.",
                "They do not ensure grasp poses fall on the same part of the same object.",
                "They are too slow to react to moving objects."
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "High-Fidelity Guided Image Synthesis with Latent Diffusion Models\nJaskirat Singh1\nStephen Gould1,2\nLiang Zheng1,2\n1The Australian National University\n2Australian Centre for Robotic Vision\n{jaskirat.singh, stephen.gould, liang.zheng}@anu.edu.au\n(b) SDEdit\n(c) Loopback\n(d) Ours\n(e) Target Domain\n(a) Reference\nText Prompt:\n“a photo of a beautiful landscape”\nSemantic Guide\nOutputs\ncastle\nstream\nflowers\ncastle\nlake\npavement\nmountains\nforest\nlake\nrocks\nforest\npyramid\nFigure 1. Overview. We propose a novel stroke based guided image synthesis framework which (Left) resolves the intrinsic domain shift\nproblem in prior works (b), wherein the final images lack details and often resemble simplistic representations of the target domain (e)\n(generated using only text-conditioning). Iteratively reperforming the guided synthesis with the generated outputs (c) seems to improve\nrealism but it is expensive and the generated outputs might lose faithfulness with the reference (a) with each iteration. (Right) Additionally,\nthe user is also able to specify the semantics of different painted regions without requiring any additional training or finetuning.\nAbstract\nControllable image synthesis with user scribbles has\ngained huge public interest with the recent advent of text-\nconditioned latent diffusion models. The user scribbles con-\ntrol the color composition while the text prompt provides\ncontrol over the overall image semantics. However, we note\nthat prior works in this direction suffer from an intrinsic\ndomain shift problem wherein the generated outputs often\nlack details and resemble simplistic representations of the\ntarget domain. In this paper, we propose a novel guided im-\nage synthesis framework, which addresses this problem by\nmodelling the output image as the solution of a constrained\noptimization problem. We show that while computing an\nexact solution to the optimization is infeasible, an approx-\nimation of the same can be achieved while just requiring a\nsingle pass of the reverse diffusion process. Additionally,\nwe show that by simply defining a cross-attention based\ncorrespondence between the input text tokens and the user\nstroke-painting, the user is also able to control the seman-\ntics of different painted regions without requiring any con-\nditional training or finetuning. Human user study results\nshow that the proposed approach outperforms the previous\nstate-of-the-art by over 85.32% on the overall user satis-\nfaction scores. Project page for our paper is available at\nhttps://1jsingh.github.io/gradop.\n",
        "question": {
            "statement": "What is a major limitation of prior work in controllable image synthesis with user scribbles?",
            "options": [
                "Text prompts are not effective",
                "Training datasets are limited",
                "Generated outputs often lack details and resemble simplistic representations",
                "User scribbles are difficult to interpret"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "1"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models\nJiarui Xu1*\nSifei Liu2†\nArash Vahdat2†\nWonmin Byeon2\nXiaolong Wang1\nShalini De Mello2\n1UC San Diego\n2NVIDIA\nInput Image\nK-Means Clustering of \nInternal Diﬀusion Features\nOpen-Vocabulary Panoptic Segmentation\nPrediction from ODISE\nFigure 1. We learn open-vocabulary panoptic segmentation with the internal representation of text-to-image diffusion models. K-Means\nclustering of the diffusion model’s internal representation shows semantically differentiated and localized information wherein objects\nare well grouped together (middle figure). We leverage these dense and rich diffusion features to perform open-vocabulary panoptic\nsegmentation (right figure).\nAbstract\nWe present ODISE: Open-vocabulary DIffusion-based\npanoptic SEgmentation, which unifies pre-trained text-\nimage diffusion and discriminative models to perform open-\nvocabulary panoptic segmentation.\nText-to-image diffu-\nsion models have the remarkable ability to generate high-\nquality images with diverse open-vocabulary language de-\nscriptions. This demonstrates that their internal represen-\ntation space is highly correlated with open concepts in the\nreal world. Text-image discriminative models like CLIP, on\nthe other hand, are good at classifying images into open-\nvocabulary labels. We leverage the frozen internal repre-\nsentations of both these models to perform panoptic seg-\nmentation of any category in the wild. Our approach out-\nperforms the previous state of the art by significant margins\non both open-vocabulary panoptic and semantic segmen-\ntation tasks. In particular, with COCO training only, our\nmethod achieves 23.4 PQ and 30.0 mIoU on the ADE20K\ndataset, with 8.3 PQ and 7.9 mIoU absolute improvement\nover the previous state of the art.\nWe open-source our\ncode and models at https://github.com/NVlabs/\nODISE.\n*Jiarui Xu was an intern at NVIDIA during the project. † equal contri-\nbution.\n",
        "question": {
            "statement": "What type of models are leveraged to perform open-vocabulary panoptic segmentation?",
            "options": [
                "semantic segmentation models and instance segmentation models",
                "computer vision models and natural language processing models",
                "object detection models and image classification models",
                "text-to-image diffusion and discriminative models"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Contrastive Semi-supervised Learning for Underwater Image Restoration via\nReliable Bank\nShirui Huang1*, Keyan Wang1*†, Huan Liu2, Jun Chen2, Yunsong Li1\n1 Xidian University, 2 McMaster University, * Equal Contribution, † Corresponding Author\nsrhuang@stu.xidian.edu.cn,{kywang, ysli}@mail.xidian.edu.cn,{liuh127, chenjun}@mcmaster.ca\nAbstract\nDespite the remarkable achievement of recent underwa-\nter image restoration techniques, the lack of labeled data\nhas become a major hurdle for further progress. In this\nwork, we propose a mean-teacher based Semi-supervised\nUnderwater Image Restoration (Semi-UIR) framework to\nincorporate the unlabeled data into network training. How-\never, the naive mean-teacher method suffers from two main\nproblems: (1) The consistency loss used in training might\nbecome ineffective when the teacher’s prediction is wrong.\n(2) Using L1 distance may cause the network to over-\nfit wrong labels, resulting in confirmation bias.\nTo ad-\ndress the above problems, we first introduce a reliable\nbank to store the “best-ever” outputs as pseudo ground\ntruth.\nTo assess the quality of outputs, we conduct an\nempirical analysis based on the monotonicity property to\nselect the most trustworthy NR-IQA method. Besides, in\nview of the confirmation bias problem, we incorporate con-\ntrastive regularization to prevent the overfitting on wrong\nlabels. Experimental results on both full-reference and non-\nreference underwater benchmarks demonstrate that our\nalgorithm has obvious improvement over SOTA methods\nquantitatively and qualitatively. Code has been released at\nhttps://github.com/Huang-ShiRui/Semi-UIR.\n",
        "question": {
            "statement": "What is a common issue with the naive mean-teacher method in semi-supervised learning?",
            "options": [
                "The student model learns faster than the teacher model",
                "The mean-teacher method always outperforms fully supervised learning",
                "The teacher's incorrect predictions can lead to ineffective consistency loss",
                "The teacher model requires more computational resources"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Improving Robustness of Semantic Segmentation to Motion-Blur using\nClass-Centric Augmentation\nAakanksha\nIndian Institute of Technology Madras\naakankshajha30@gmail.com\nA. N. Rajagopalan\nIndian Institute of Technology Madras\nraju@ee.iitm.ac.in\nAbstract\nSemantic segmentation involves classifying each pixel\ninto one of a pre-defined set of object/stuff classes. Such\na fine-grained detection and localization of objects in the\nscene is challenging by itself.\nThe complexity increases\nmanifold in the presence of blur. With cameras becoming\nincreasingly light-weight and compact, blur caused by mo-\ntion during capture time has become unavoidable. Most\nresearch has focused on improving segmentation perfor-\nmance for sharp clean images and the few works that deal\nwith degradations, consider motion-blur as one of many\ngeneric degradations. In this work, we focus exclusively\non motion-blur and attempt to achieve robustness for se-\nmantic segmentation in its presence. Based on the observa-\ntion that segmentation annotations can be used to generate\nsynthetic space-variant blur, we propose a Class-Centric\nMotion-Blur Augmentation (CCMBA) strategy.\nOur ap-\nproach involves randomly selecting a subset of semantic\nclasses present in the image and using the segmentation\nmap annotations to blur only the corresponding regions.\nThis enables the network to simultaneously learn seman-\ntic segmentation for clean images, images with egomotion\nblur, as well as images with dynamic scene blur. We demon-\nstrate the effectiveness of our approach for both CNN and\nVision Transformer-based semantic segmentation networks\non PASCAL VOC and Cityscapes datasets. We also illus-\ntrate the improved generalizability of our method to com-\nplex real-world blur by evaluating on the commonly used\ndeblurring datasets GoPro and REDS .\n",
        "question": {
            "statement": "What is a common challenge in semantic segmentation tasks?",
            "options": [
                "Object detection and localization in high-resolution images",
                "Object detection and localization in grayscale images",
                "Object detection and localization in blurred scenes",
                "Object detection and localization in low-light conditions"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "8",
                "0"
            ]
        },
        "difference": 6,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "DC2: Dual-Camera Defocus Control by Learning to Refocus\nHadi Alzayer1,2\nAbdullah Abuolaim1\nLeung Chun Chan1\nYang Yang1\nYing Chen Lou1\nJia-Bin Huang2\nAbhishek Kar1\n1Google\n2University of Maryland, College Park\nUltra-wide\nWide\nRefocused\nDeblurred\nShallow DoF\nInput\nOur results\nFigure 1. Post-capture depth-of-field (DoF) control from dual camera. (Left) Using photos captured from dual cameras with a wide\nfield of view (FoV) and ultra-wide FoV, our method enables various DoF manipulations. (Right) We showcase our results of refocusing\n(changing the focal plane), deblurring (creating all-in-focus imagery), and synthesizing a shallower DoF (producing bokeh effects).\nAbstract\nSmartphone cameras today are increasingly approach-\ning the versatility and quality of professional cameras\nthrough a combination of hardware and software advance-\nments. However, fixed aperture remains a key limitation,\npreventing users from controlling the depth of field (DoF)\nof captured images. At the same time, many smartphones\nnow have multiple cameras with different fixed apertures -\nspecifically, an ultra-wide camera with wider field of view\nand deeper DoF and a higher resolution primary camera\nwith shallower DoF. In this work, we propose DC2, a system\nfor defocus control for synthetically varying camera aper-\nture, focus distance and arbitrary defocus effects by fusing\ninformation from such a dual-camera system. Our key in-\nsight is to leverage real-world smartphone camera dataset\nby using image refocus as a proxy task for learning to con-\ntrol defocus. Quantitative and qualitative evaluations on\nreal-world data demonstrate our system’s efficacy where we\noutperform state-of-the-art on defocus deblurring, bokeh\nrendering, and image refocus. Finally, we demonstrate cre-\native post-capture defocus control enabled by our method,\nincluding tilt-shift and content-based defocus effects.\n",
        "question": {
            "statement": "What is a limitation of current smartphone cameras that prevents users from controlling the depth of field of captured images?",
            "options": [
                "low resolution",
                "limited storage",
                "poor lighting",
                "fixed aperture"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Task-specific Fine-tuning via Variational Information Bottleneck for\nWeakly-supervised Pathology Whole Slide Image Classification\nHonglin Li1,3, Chenglu Zhu2,3, Yunlong Zhang1,3, Yuxuan Sun1,3,\nZhongyi Shui1,3, Wenwei Kuang3,4, Sunyi Zheng2,3, Lin Yang2,3*\n1College of Computer Science and Technology, Zhejiang University\n2Research Center for Industries of the Future and 3School of Engineering, Westlake University\n4The University of Hong Kong\n{lihonglin,yanglin}@westlake.edu.cn\nAbstract\nWhile Multiple Instance Learning (MIL) has shown\npromising results in digital Pathology Whole Slide Image\n(WSI) analysis, such a paradigm still faces performance\nand generalization problems due to high computational\ncosts and limited supervision of Gigapixel WSIs. To deal\nwith the computation problem, previous methods utilize a\nfrozen model pretrained from ImageNet to obtain repre-\nsentations, however, it may lose key information owing to\nthe large domain gap and hinder the generalization ability\nwithout image-level training-time augmentation. Though\nSelf-supervised Learning (SSL) proposes viable represen-\ntation learning schemes, the downstream task-specific fea-\ntures via partial label tuning are not explored. To alleviate\nthis problem, we propose an efficient WSI fine-tuning frame-\nwork motivated by the Information Bottleneck theory. The\ntheory enables the framework to find the minimal sufficient\nstatistics of WSI, thus supporting us to fine-tune the back-\nbone into a task-specific representation only depending on\nWSI-level weak labels. The WSI-MIL problem is further\nanalyzed to theoretically deduce our fine-tuning method.\nWe evaluate the method on five pathological WSI datasets\non various WSI heads. The experimental results show sig-\nnificant improvements in both accuracy and generalization\ncompared with previous works. Source code will be avail-\nable at https://github.com/invoker-LL/WSI-finetuning.\n",
        "question": {
            "statement": "What is a major limitation of using pre-trained models from ImageNet for digital pathology whole slide image analysis?",
            "options": [
                "High computational costs",
                "Limited availability of training data",
                "Losing key information due to large domain gap",
                "Insufficient number of parameters in the model"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "High-Res Facial Appearance Capture from Polarized Smartphone Images\nDejan Azinovi´\nc1\nOlivier Maury2 Christophe Hery2 Matthias Nießner1\nJustus Thies3\n1Technical University of Munich\n2Meta Reality Labs\n3Max Planck Institute for Intelligent Systems\nFigure 1. Our method obtains high-resolution skin textures from two RGB input sequences captured with polarization foils attached to\na smartphone. The core idea is to separate the skin’s diffuse and specular response by capturing one cross-polarized and one parallel-\npolarized sequence. We recover an accurate geometry with multi-view stereo, fit a parametric head model, and employ a differentiable\nrendering strategy to recover 4K diffuse albedo, specular gain and normal maps. These can be used with off-the-shelf rendering software,\nsuch as Blender, to produce photo-realistic images from novel views, under novel illumination and with subsurface scattering (SSS).\nAbstract\nWe propose a novel method for high-quality facial tex-\nture reconstruction from RGB images using a novel captur-\ning routine based on a single smartphone which we equip\nwith an inexpensive polarization foil. Specifically, we turn\nthe flashlight into a polarized light source and add a polar-\nization filter on top of the camera. Leveraging this setup,\nwe capture the face of a subject with cross-polarized and\nparallel-polarized light. For each subject, we record two\nshort sequences in a dark environment under flash illu-\nmination with different light polarization using the modi-\nfied smartphone. Based on these observations, we recon-\nstruct an explicit surface mesh of the face using structure\nfrom motion.\nWe then exploit the camera and light co-\nlocation within a differentiable renderer to optimize the fa-\ncial textures using an analysis-by-synthesis approach. Our\nAll data has been captured at the Technical University of Munich.\nmethod optimizes for high-resolution normal textures, dif-\nfuse albedo, and specular albedo using a coarse-to-fine op-\ntimization scheme. We show that the optimized textures can\nbe used in a standard rendering pipeline to synthesize high-\nquality photo-realistic 3D digital humans in novel environ-\nments.\n",
        "question": {
            "statement": "What is the purpose of using cross-polarized and parallel-polarized light in facial texture reconstruction?",
            "options": [
                "To simulate different environmental lighting conditions",
                "To separate the skin's diffuse and specular response",
                "To reduce the amount of data required for processing",
                "To increase the resolution of the captured images"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Unifying Vision, Text, and Layout for Universal Document Processing\nZineng Tang1,2\nZiyi Yang2*\nGuoxin Wang3\nYuwei Fang2\nYang Liu2\nChenguang Zhu2\nMichael Zeng2\nCha Zhang3\nMohit Bansal1⇤\n1University of North Carolina at Chapel Hill\n2Microsoft Azure Cognitive Services Research\n3Microsoft Azure Visual Document Intelligence\nAbstract\nWe propose Universal Document Processing (UDOP),\na foundation Document AI model which uniﬁes text, im-\nage, and layout modalities together with varied task for-\nmats, including document understanding and generation.\nUDOP leverages the spatial correlation between textual con-\ntent and document image to model image, text, and layout\nmodalities with one uniform representation. With a novel\nVision-Text-Layout Transformer, UDOP uniﬁes pretraining\nand multi-domain downstream tasks into a prompt-based\nsequence generation scheme. UDOP is pretrained on both\nlarge-scale unlabeled document corpora using innovative\nself-supervised objectives and diverse labeled data. UDOP\nalso learns to generate document images from text and lay-\nout modalities via masked image reconstruction. To the\nbest of our knowledge, this is the ﬁrst time in the ﬁeld of\ndocument AI that one model simultaneously achieves high-\nquality neural document editing and content customization.\nOur method sets the state-of-the-art on 8 Document AI tasks,\ne.g., document understanding and QA, across diverse data\ndomains like ﬁnance reports, academic papers, and web-\nsites. UDOP ranks ﬁrst on the leaderboard of the Document\nUnderstanding Benchmark.1\n",
        "question": {
            "statement": "What is the primary goal of the proposed Universal Document Processing (UDOP) model?",
            "options": [
                "To unify vision, text, and layout modalities for universal document processing",
                "To prioritize image recognition over text analysis",
                "To focus solely on document understanding and generation",
                "To develop a specialized model for finance report analysis"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "5",
                "0"
            ]
        },
        "difference": 5,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "OPE-SR: Orthogonal Position Encoding for Designing a Parameter-free\nUpsampling Module in Arbitrary-scale Image Super-Resolution\nGaochao Song1\nQian Sun2*\nLuo Zhang3\nRan Su1\nJianfeng Shi2\nYing He3\n1Tianjin University\n2Nanjing University of Information Science and Technology\n3Nanyang Technological University\ngaochaosong 21@tju.edu.cn\nsunqian@nuist.edu.cn\nAbstract\nArbitrary-scale image super-resolution (SR) is often\ntackled using the implicit neural representation (INR) ap-\nproach, which relies on a position encoding scheme to im-\nprove its representation ability. In this paper, we introduce\northogonal position encoding (OPE), an extension of po-\nsition encoding, and an OPE-Upscale module to replace\nthe INR-based upsampling module for arbitrary-scale im-\nage super-resolution. Our OPE-Upscale module takes 2D\ncoordinates and latent code as inputs, just like INR, but\ndoes not require any training parameters. This parameter-\nfree feature allows the OPE-Upscale module to directly\nperform linear combination operations, resulting in con-\ntinuous image reconstruction and achieving arbitrary-scale\nimage reconstruction.\nAs a concise SR framework, our\nmethod is computationally efficient and consumes less mem-\nory than state-of-the-art methods, as confirmed by exten-\nsive experiments and evaluations. In addition, our method\nachieves comparable results with state-of-the-art methods\nin arbitrary-scale image super-resolution. Lastly, we show\nthat OPE corresponds to a set of orthogonal basis, validat-\ning our design principle. 1\n",
        "question": {
            "statement": "What is a key advantage of the OPE-Upscale module in image super-resolution?",
            "options": [
                "It is slower than other methods",
                "It can only be used for fixed-scale image super-resolution",
                "It does not require any training parameters",
                "It requires more memory than other methods"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Robust Test-Time Adaptation in Dynamic Scenarios\nLonghui Yuan\nBinhui Xie\nShuang Li\f\nSchool of Computer Science and Technology, Beijing Institute of Technology\n{longhuiyuan,binhuixie,shuangli}@bit.edu.cn\nAbstract\nTest-time adaptation (TTA) intends to adapt the pre-\ntrained model to test distributions with only unlabeled test\ndata streams.\nMost of the previous TTA methods have\nachieved great success on simple test data streams such as\nindependently sampled data from single or multiple distri-\nbutions. However, these attempts may fail in dynamic sce-\nnarios of real-world applications like autonomous driving,\nwhere the environments gradually change and the test data\nis sampled correlatively over time. In this work, we ex-\nplore such practical test data streams to deploy the model\non the fly, namely practical test-time adaptation (PTTA).\nTo do so, we elaborate a Robust Test-Time Adaptation\n(RoTTA) method against the complex data stream in PTTA.\nMore specifically, we present a robust batch normalization\nscheme to estimate the normalization statistics. Meanwhile,\na memory bank is utilized to sample category-balanced data\nwith consideration of timeliness and uncertainty. Further, to\nstabilize the training procedure, we develop a time-aware\nreweighting strategy with a teacher-student model. Exten-\nsive experiments prove that RoTTA enables continual test-\ntime adaptation on the correlatively sampled data streams.\nOur method is easy to implement, making it a good choice\nfor rapid deployment.\nThe code is publicly available at\nhttps://github.com/BIT-DA/RoTTA\n",
        "question": {
            "statement": "What is the main challenge in traditional test-time adaptation methods when applied to real-world scenarios?",
            "options": [
                "Adapting to new categories of data",
                "Incorporating domain knowledge into the model",
                "Handling large amounts of labeled data",
                "Dealing with correlatively sampled data streams"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Initialization Noise in Image Gradients and Saliency Maps\nAnn-Christin Woerl, Jan Disselhoff, Michael Wand\nInstitute of Computer Science\nJohannes Gutenberg University Mainz, Germany\n{awoerl, jadissel, wandm}@uni-mainz.de\nAbstract\nIn this paper, we examine gradients of logits of image\nclassification CNNs by input pixel values. We observe that\nthese fluctuate considerably with training randomness, such\nas the random initialization of the networks.\nWe extend\nour study to gradients of intermediate layers, obtained via\nGradCAM, as well as popular network saliency estimators\nsuch as DeepLIFT, SHAP, LIME, Integrated Gradients, and\nSmoothGrad. While empirical noise levels vary, qualita-\ntively different attributions to image features are still pos-\nsible with all of these, which comes with implications for\ninterpreting such attributions, in particular when seeking\ndata-driven explanations of the phenomenon generating the\ndata. Finally, we demonstrate that the observed artefacts\ncan be removed by marginalization over the initialization\ndistribution by simple stochastic integration.\n",
        "question": {
            "statement": "What is a potential issue with interpreting attributions from image classification CNNs?",
            "options": [
                "The complexity of the CNN architecture",
                "The type of activation functions used",
                "The quality of the training dataset",
                "Random initialization of the networks affects the results"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "2",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Revisiting Self-Similarity: Structural Embedding for Image Retrieval\nSeongwon Lee\nSuhyeon Lee\nHongje Seong\nEuntai Kim*\nSchool of Electrical and Electronic Engineering, Yonsei University, Seoul, Korea\n{won4113, hyeon93, hjseong, etkim}@yonsei.ac.kr\nAbstract\nDespite advances in global image representation, exist-\ning image retrieval approaches rarely consider geometric\nstructure during the global retrieval stage. In this work,\nwe revisit the conventional self-similarity descriptor from\na convolutional perspective, to encode both the visual and\nstructural cues of the image to global image representation.\nOur proposed network, named Structural Embedding Net-\nwork (SENet), captures the internal structure of the images\nand gradually compresses them into dense self-similarity\ndescriptors while learning diverse structures from various\nimages. These self-similarity descriptors and original im-\nage features are fused and then pooled into global embed-\nding, so that global embedding can represent both geomet-\nric and visual cues of the image.\nAlong with this novel\nstructural embedding, our proposed network sets new state-\nof-the-art performances on several image retrieval bench-\nmarks, convincing its robustness to look-alike distractors.\nThe code and models are available: https://github.\ncom/sungonce/SENet.\n",
        "question": {
            "statement": "What is the main advantage of using structural embedding in image retrieval?",
            "options": [
                "It captures both visual and geometric cues of the image",
                "It uses less computational resources than other methods",
                "It relies solely on global image representation",
                "It is only applicable to specific types of images"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "DetCLIPv2: Scalable Open-Vocabulary Object Detection Pre-training via\nWord-Region Alignment\nLewei Yao1,2, Jianhua Han2, Xiaodan Liang3†, Dan Xu1,\nWei Zhang2, Zhenguo Li2, Hang Xu2†\n1Hong Kong University of Science and Technology, 2Huawei Noah’s Ark Lab\n3Shenzhen Campus of Sun Yat-Sen University\n(a) Hand drawn black and white illus-\ntration, flying eagle with a snake in claws.\n(c) Curator looks on as we consider paintings\nin a shared studio space\n(b) Fried egg on a frying pan with cherry\ntomatoes and parsley\n(e) Little girl witch with black cat, owl, the \nwitch's cauldron, ghost spirits and text on \nviolet background. The concept of Halloween.\n(h) Dark eyed juvenile perched in a conifer\n(g) Playa Esmeralda in Holguin, Cuba. The view\nfrom the top of the beach. Beautiful Caribbean \nsea turquoise.\n(d) A SCUBA diver looks over his shoulder \ntowards a sea lion\n(f) The Vibrant Tattoos \nFigure 1. Visualizations of DetCLIPv2 for open-vocabulary word-region alignment. DetCLIPv2 is able to detect broad concepts.\nAbstract\nThis paper presents DetCLIPv2, an efficient and scalable\ntraining framework that incorporates large-scale image-\ntext pairs to achieve open-vocabulary object detection\n(OVD). Unlike previous OVD frameworks that typically rely\non a pre-trained vision-language model (e.g., CLIP) or ex-\nploit image-text pairs via a pseudo labeling process, Det-\nCLIPv2 directly learns the fine-grained word-region align-\nment from massive image-text pairs in an end-to-end man-\nner. To accomplish this, we employ a maximum word-region\nsimilarity between region proposals and textual words to\nguide the contrastive objective. To enable the model to gain\nlocalization capability while learning broad concepts, Det-\nCLIPv2 is trained with a hybrid supervision from detection,\ngrounding and image-text pair data under a unified data\nformulation. By jointly training with an alternating scheme\nand adopting low-resolution input for image-text pairs, Det-\nCLIPv2 exploits image-text pair data efficiently and ef-\nfectively: DetCLIPv2 utilizes 13× more image-text pairs\nthan DetCLIP with a similar training time and improves\n†Corresponding\nauthor:\nliangxd9@mail.sysu.edu.cn,\nxu.hang@huawei.com\nperformance. With 13M image-text pairs for pre-training,\nDetCLIPv2 demonstrates superior open-vocabulary detec-\ntion performance, e.g., DetCLIPv2 with Swin-T backbone\nachieves 40.4% zero-shot AP on the LVIS benchmark,\nwhich outperforms previous works GLIP/GLIPv2/DetCLIP\nby 14.4/11.4/4.5% AP, respectively, and even beats its fully-\nsupervised counterpart by a large margin.\n",
        "question": {
            "statement": "What is the main advantage of the DetCLIPv2 framework compared to previous open-vocabulary object detection frameworks?",
            "options": [
                "It requires high-resolution input for image-text pairs",
                "It relies on a pre-trained vision-language model",
                "It can utilize a larger amount of image-text pair data for pre-training",
                "It uses a pseudo labeling process to exploit image-text pairs"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Generalizable Local Feature Pre-training for Deformable Shape Analysis\nSouhaib Attaiki\nLei Li\nMaks Ovsjanikov\nLIX, ´\nEcole Polytechnique, IP Paris\nAbstract\nTransfer learning is fundamental for addressing prob-\nlems in settings with little training data.\nWhile several\ntransfer learning approaches have been proposed in 3D,\nunfortunately, these solutions typically operate on an en-\ntire 3D object or even scene-level and thus, as we show,\nfail to generalize to new classes, such as deformable or-\nganic shapes. In addition, there is currently a lack of un-\nderstanding of what makes pre-trained features transfer-\nable across significantly different 3D shape categories. In\nthis paper, we make a step toward addressing these chal-\nlenges. First, we analyze the link between feature local-\nity and transferability in tasks involving deformable 3D ob-\njects, while also comparing different backbones and losses\nfor local feature pre-training. We observe that with proper\ntraining, learned features can be useful in such tasks, but,\ncrucially, only with an appropriate choice of the recep-\ntive field size.\nWe then propose a differentiable method\nfor optimizing the receptive field within 3D transfer learn-\ning. Jointly, this leads to the first learnable features that\ncan successfully generalize to unseen classes of 3D shapes\nsuch as humans and animals. Our extensive experiments\nshow that this approach leads to state-of-the-art results on\nseveral downstream tasks such as segmentation, shape cor-\nrespondence, and classification. Our code is available at\nhttps://github.com/pvnieo/vader.\n",
        "question": {
            "statement": "What is a key factor in determining the effectiveness of pre-trained features in transfer learning for deformable 3D objects?",
            "options": [
                "the number of training samples",
                "the type of loss function used",
                "the type of backbone used",
                "the receptive field size"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "3",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Towards Accurate Image Coding:\nImproved Autoregressive Image Generation with Dynamic Vector Quantization\nMengqi Huang1, Zhendong Mao1, 2*\n, Zhuowei Chen1, Yongdong Zhang1, 2\n1University of Science and Technology of China, Hefei, China;\n2Institute of Artificial intelligence, Hefei Comprehensive National Science Center, Hefei, China\n{huangmq, chenzw01}@mail.ustc.edu.cn, {zdmao, zhyd73}@ustc.edu.cn\nAbstract\nExisting vector quantization (VQ) based autoregressive\nmodels follow a two-stage generation paradigm that first\nlearns a codebook to encode images as discrete codes,\nand then completes generation based on the learned code-\nbook. However, they encode fixed-size image regions into\nfixed-length codes and ignore their naturally different in-\nformation densities, which results in insufficiency in impor-\ntant regions and redundancy in unimportant ones, and fi-\nnally degrades the generation quality and speed.\nMore-\nover, the fixed-length coding leads to an unnatural raster-\nscan autoregressive generation. To address the problem,\nwe propose a novel two-stage framework: (1) Dynamic-\nQuantization VAE (DQ-VAE) which encodes image re-\ngions into variable-length codes based on their informa-\ntion densities for an accurate & compact code represen-\ntation. (2) DQ-Transformer which thereby generates im-\nages autoregressively from coarse-grained (smooth regions\nwith fewer codes) to fine-grained (details regions with\nmore codes) by modeling the position and content of codes\nin each granularity alternately, through a novel stacked-\ntransformer architecture and shared-content, non-shared\nposition input layers designs. Comprehensive experiments\non various generation tasks validate our superiorities in\nboth effectiveness and efficiency.\nCode will be released\nat https://github.com/CrossmodalGroup/\nDynamicVectorQuantization.\n",
        "question": {
            "statement": "What is a limitation of traditional vector quantization-based autoregressive models in image generation?",
            "options": [
                "They are unable to model the position and content of codes",
                "They use a single-stage generation paradigm",
                "They encode fixed-size image regions into fixed-length codes",
                "They generate images from fine-grained to coarse-grained regions"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Generating Human Motion from Textual Descriptions with\nDiscrete Representations\nJianrong Zhang1,3∗, Yangsong Zhang2,3∗, Xiaodong Cun3, Yong Zhang3, Hongwei Zhao1\nHongtao Lu2, Xi Shen3,†, Shan Ying3\n∗Equal contribution\n†Corresponding author\n1Jilin University\n2Shanghai Jiao Tong University\n3Tencent AI Lab\nAbstract\nIn this work, we investigate a simple and must-known con-\nditional generative framework based on Vector Quantised-\nVariational AutoEncoder (VQ-VAE) and Generative Pre-\ntrained Transformer (GPT) for human motion generation\nfrom textural descriptions. We show that a simple CNN-\nbased VQ-VAE with commonly used training recipes (EMA\nand Code Reset) allows us to obtain high-quality discrete rep-\nresentations. For GPT, we incorporate a simple corruption\nstrategy during the training to alleviate training-testing dis-\ncrepancy. Despite its simplicity, our T2M-GPT shows better\nperformance than competitive approaches, including recent\ndiffusion-based approaches. For example, on HumanML3D,\nwhich is currently the largest dataset, we achieve compara-\nble performance on the consistency between text and gener-\nated motion (R-Precision), but with FID 0.116 largely outper-\nforming MotionDiffuse of 0.630. Additionally, we conduct\nanalyses on HumanML3D and observe that the dataset size\nis a limitation of our approach. Our work suggests that VQ-\nVAE still remains a competitive approach for human motion\ngeneration. Our implementation is available on the project\npage: https://mael-zys.github.io/T2M-GPT/.\n",
        "question": {
            "statement": "What type of autoencoder is used in the proposed framework for generating human motion from textual descriptions?",
            "options": [
                "Convolutional Neural Network (CNN)",
                "Recurrent Neural Network (RNN)",
                "Generative Adversarial Network (GAN)",
                "Vector Quantised-Variational AutoEncoder (VQ-VAE)"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Density-Insensitive Unsupervised Domain Adaption on 3D Object Detection\nQianjiang Hu\nDaizong Liu\nWei HuB\nWangxuan Institute of Computer Technology, Peking University\nNo. 128, Zhongguancun North Street, Beijing, China\nhqjpku@pku.edu.cn, dzliu@stu.pku.edu.cn, forhuwei@pku.edu.cn\nAbstract\n3D object detection from point clouds is crucial in safety-\ncritical autonomous driving. Although many works have\nmade great efforts and achieved significant progress on this\ntask, most of them suffer from expensive annotation cost\nand poor transferability to unknown data due to the do-\nmain gap. Recently, few works attempt to tackle the do-\nmain gap in objects, but still fail to adapt to the gap of\nvarying beam-densities between two domains, which is crit-\nical to mitigate the characteristic differences of the LiDAR\ncollectors. To this end, we make the attempt to propose a\ndensity-insensitive domain adaption framework to address\nthe density-induced domain gap. In particular, we first in-\ntroduce Random Beam Re-Sampling (RBRS) to enhance the\nrobustness of 3D detectors trained on the source domain to\nthe varying beam-density. Then, we take this pre-trained de-\ntector as the backbone model, and feed the unlabeled target\ndomain data into our newly designed task-specific teacher-\nstudent framework for predicting its high-quality pseudo la-\nbels. To further adapt the property of density-insensitivity\ninto the target domain, we feed the teacher and student\nbranches with the same sample of different densities, and\npropose an Object Graph Alignment (OGA) module to con-\nstruct two object-graphs between the two branches for en-\nforcing the consistency in both the attribute and relation of\ncross-density objects. Experimental results on three widely\nadopted 3D object detection datasets demonstrate that our\nproposed domain adaption method outperforms the state-\nof-the-art methods, especially over varying-density data.\nCode is available at https://github.com/WoodwindHu/DTS.\n",
        "question": {
            "statement": "What is a major limitation of existing 3D object detection systems that can hinder their performance in real-world applications?",
            "options": [
                "Insufficient training data",
                "Inaccurate object classification models",
                "Domain gap caused by varying beam densities",
                "Limited computational resources"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "2",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Learning from Noisy Labels with Decoupled Meta Label Purifier\nYuanpeng Tu1*\nBoshen Zhang2*\nYuxi Li2*\nLiang Liu2\nJian Li2\nYabiao Wang2\nChengjie Wang2,3†\nCai Rong Zhao1†\n1Dept. of Electronic and Information Engineering, Tongji Univeristy, Shanghai\n2YouTu Lab, Tencent, Shanghai,\n3Shanghai Jiao Tong University\n{2030809, zhaocairong}@tongji.edu.cn\n{boshenzhang, yukiyxli, leoneliu, swordli, caseywang, jasoncjwang}@tencent.com\nAbstract\nTraining deep neural networks (DNN) with noisy labels\nis challenging since DNN can easily memorize inaccurate\nlabels, leading to poor generalization ability.\nRecently,\nthe meta-learning based label correction strategy is widely\nadopted to tackle this problem via identifying and correcting\npotential noisy labels with the help of a small set of clean\nvalidation data. Although training with purified labels can\neffectively improve performance, solving the meta-learning\nproblem inevitably involves a nested loop of bi-level opti-\nmization between model weights and hyper-parameters (i.e.,\nlabel distribution). As compromise, previous methods re-\nsort to a coupled learning process with alternating update.\nIn this paper, we empirically find such simultaneous opti-\nmization over both model weights and label distribution\ncan not achieve an optimal routine, consequently limiting\nthe representation ability of backbone and accuracy of cor-\nrected labels. From this observation, a novel multi-stage\nlabel purifier named DMLP is proposed. DMLP decou-\nples the label correction process into label-free representa-\ntion learning and a simple meta label purifier, In this way,\nDMLP can focus on extracting discriminative feature and\nlabel correction in two distinctive stages. DMLP is a plug-\nand-play label purifier, the purified labels can be directly\nreused in naive end-to-end network retraining or other ro-\nbust learning methods, where state-of-the-art results are\nobtained on several synthetic and real-world noisy datasets,\nespecially under high noise levels. Code is available at\nhttps://github.com/yuanpengtu/DMLP.\n",
        "question": {
            "statement": "What is a common challenge when training deep neural networks with noisy labels?",
            "options": [
                "The model can easily memorize inaccurate labels",
                "The model is unable to learn from noisy labels at all",
                "The model requires a large amount of clean validation data",
                "The model always learns accurate labels"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Language-Guided Audio-Visual Source Separation via Trimodal Consistency\nReuben Tan1\nArijit Ray1\nAndrea Burns1\nBryan A. Plummer1\nJustin Salamon 2\nOriol Nieto2\nBryan Russell2\nKate Saenko1,3\n1Boston University, 2Adobe Research, 3MIT-IBM Watson AI Lab, IBM Research\n{rxtan, aburns4, array, bplum, saenko}@bu.edu, {salamon, onieto, brussell}@adobe.com\nAbstract\nWe propose a self-supervised approach for learning to\nperform audio source separation in videos based on natu-\nral language queries, using only unlabeled video and au-\ndio pairs as training data. A key challenge in this task is\nlearning to associate the linguistic description of a sound-\nemitting object to its visual features and the correspond-\ning components of the audio waveform, all without access\nto annotations during training.\nTo overcome this chal-\nlenge, we adapt off-the-shelf vision-language foundation\nmodels to provide pseudo-target supervision via two novel\nloss functions and encourage a stronger alignment between\nthe audio, visual and natural language modalities. Dur-\ning inference, our approach can separate sounds given text,\nvideo and audio input, or given text and audio input alone.\nWe demonstrate the effectiveness of our self-supervised ap-\nproach on three audio-visual separation datasets, includ-\ning MUSIC, SOLOS and AudioSet, where we outperform\nstate-of-the-art strongly supervised approaches despite not\nusing object detectors or text labels during training. Our\nproject page including publicly available code can be found\nat https://cs-people.bu.edu/rxtan/projects/VAST.\n",
        "question": {
            "statement": "What is a major challenge in learning to perform audio source separation in videos based on natural language queries?",
            "options": [
                "associating linguistic descriptions of sound-emitting objects with their visual features and corresponding audio components",
                "transcribing spoken language into text",
                "recognizing objects in videos",
                "separating sounds from background noise"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Bi-directional Feature Fusion Generative Adversarial Network for Ultra-high\nResolution Pathological Image Virtual Re-staining\nKexin Sun1, Zhineng Chen1, 2*\n, Gongwei Wang3, Jun Liu3, Xiongjun Ye4, Yu-Gang Jiang1\n1School of Computer Science & Shanghai Collaborative Innovation Center of Intelligent Visual Computing,\nFudan University\n2Shanghai Qi Zhi Institute\n3Peking University People’s Hospital\n4Department of Urology, National Cancer Center & National Clinical Research Center for Cancer,\nCancer Hospital, Chinese Academy of Medical Sciences and Peking Union Medical College\nsunkx21@m.fudan.edu.cn, {zhinchen, ygj}@fudan.edu.cn,\nwgw4300@126.com, hmuliujun@163.com, yexiongjun@cicams.ac.cn\nAbstract\nThe cost of pathological examination makes virtual re-\nstaining of pathological images meaningful. However, due\nto the ultra-high resolution of pathological images, tradi-\ntional virtual re-staining methods have to divide a WSI im-\nage into patches for model training and inference. Such a\nlimitation leads to the lack of global information, result-\ning in observable differences in color, brightness and con-\ntrast when the re-stained patches are merged to generate\nan image of larger size. We summarize this issue as the\nsquare effect. Some existing methods try to solve this is-\nsue through overlapping between patches or simple post-\nprocessing. But the former one is not that effective, while\nthe latter one requires carefully tuning. In order to elim-\ninate the square effect, we design a bi-directional feature\nfusion generative adversarial network (BFF-GAN) with a\nglobal branch and a local branch.\nIt learns the inter-\npatch connections through the fusion of global and local\nfeatures plus patch-wise attention. We perform experiments\non both the private dataset RCC and the public dataset AN-\nHIR. The results show that our model achieves competitive\nperformance and is able to generate extremely real images\nthat are deceptive even for experienced pathologists, which\nmeans it is of great clinical significance.\n",
        "question": {
            "statement": "What is a major limitation of traditional virtual re-staining methods for ultra-high resolution pathological images?",
            "options": [
                "They require manual annotation",
                "They cannot capture global information",
                "They are limited to specific types of cancer",
                "They are only applicable to low-resolution images"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Neural Preset for Color Style Transfer\nZhanghan Ke1\nYuhao Liu1\nLei Zhu1\nNanxuan Zhao2\nRynson W.H. Lau1†\n1City University of Hong Kong\n2Adobe Research\nFigure 1. Our Color Style Transfer Results. (a) State-of-the-art methods PhotoNAS [1] and PhotoWCT2 [6] produce distort textures\n(e.g., text in green box) and dissonant colors (e.g., content in orange box). Besides, they have long inference time even on the latest Nvidia\nRTX3090 GPU (red numbers in brackets). In contrast, our method avoids artifacts and is ∼28× faster. (b) Our method can produce faithful\nresults on 8K images, but both PhotoNAS and PhotoWCT2 run into the out-of-memory problem. Zoom in for better visualization.\nAbstract\nIn this paper, we present a Neural Preset technique to\naddress the limitations of existing color style transfer meth-\nods, including visual artifacts, vast memory requirement,\nand slow style switching speed. Our method is based on two\ncore designs. First, we propose Deterministic Neural Color\nMapping (DNCM) to consistently operate on each pixel via\nan image-adaptive color mapping matrix, avoiding artifacts\nand supporting high-resolution inputs with a small memory\nfootprint. Second, we develop a two-stage pipeline by divid-\ning the task into color normalization and stylization, which\nallows efficient style switching by extracting color styles as\npresets and reusing them on normalized input images. Due\nto the unavailability of pairwise datasets, we describe how\nto train Neural Preset via a self-supervised strategy. Vari-\nous advantages of Neural Preset over existing methods are\ndemonstrated through comprehensive evaluations. Besides,\nwe show that our trained model can naturally support mul-\ntiple applications without fine-tuning, including low-light\nimage enhancement, underwater image correction, image\ndehazing, and image harmonization. The project page is:\nhttps://ZHKKKe.github.io/NeuralPreset.\n",
        "question": {
            "statement": "What is the main advantage of using Deterministic Neural Color Mapping (DNCM) in color style transfer?",
            "options": [
                "It can only be used for low-light image enhancement",
                "It allows for faster style switching",
                "It avoids artifacts and supports high-resolution inputs with a small memory footprint",
                "It requires less computational power"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders\nSanghyun Woo1*\nShoubhik Debnath2\nRonghang Hu2\nXinlei Chen2\nZhuang Liu2\nIn So Kweon1\nSaining Xie3†\n1KAIST\n2 Meta AI, FAIR\n3New York University\nAbstract\nDriven by improved architectures and better representa-\ntion learning frameworks, the field of visual recognition has\nenjoyed rapid modernization and performance boost in the\nearly 2020s. For example, modern ConvNets, represented\nby ConvNeXt [33], have demonstrated strong performance\nin various scenarios. While these models were originally\ndesigned for supervised learning with ImageNet labels,\nthey can also potentially benefit from self-supervised learn-\ning techniques such as masked autoencoders (MAE) [14].\nHowever, we found that simply combining these two ap-\nproaches leads to subpar performance. In this paper, we\npropose a fully convolutional masked autoencoder frame-\nwork and a new Global Response Normalization (GRN)\nlayer that can be added to the ConvNeXt architecture to\nenhance inter-channel feature competition. This co-design\nof self-supervised learning techniques and architectural im-\nprovement results in a new model family called ConvNeXt\nV2, which significantly improves the performance of pure\nConvNets on various recognition benchmarks, including\nImageNet classification, COCO detection, and ADE20K\nsegmentation. We also provide pre-trained ConvNeXt V2\nmodels of various sizes, ranging from an efficient 3.7M-\nparameter Atto model with 76.7% top-1 accuracy on Im-\nageNet, to a 650M Huge model that achieves a state-of-the-\nart 88.9% accuracy using only public training data.\nCode: https://github.com/facebookresearch/ConvNeXt-V2\n",
        "question": {
            "statement": "What is the main goal of the proposed approach in ConvNeXt V2?",
            "options": [
                "To replace traditional supervised learning methods with self-supervised ones",
                "To improve the performance of ConvNets by combining self-supervised learning techniques and architectural improvements",
                "To develop a new type of masked autoencoder",
                "To focus solely on improving the efficiency of ConvNets"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "2",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "The Enemy of My Enemy is My Friend:\nExploring Inverse Adversaries for Improving Adversarial Training\nJunhao Dong1, Seyed-Mohsen Moosavi-Dezfooli2, Jianhuang Lai1,3,4 and Xiaohua Xie1,3,4*\n1School of Computer Science and Engineering, Sun Yat-Sen University, China\n2Imperial College London, UK\n3Guangdong Province Key Laboratory of Information Security Technology, China\n4Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, China\ndongjh8@mail2.sysu.edu.cn, seyed.moosavi@imperial.ac.uk, {stsljh, xiexiaoh6}@mail.sysu.edu.cn\nAbstract\nAlthough current deep learning techniques have yielded\nsuperior performance on various computer vision tasks, yet\nthey are still vulnerable to adversarial examples. Adversar-\nial training and its variants have been shown to be the most\neffective approaches to defend against adversarial exam-\nples. A particular class of these methods regularize the dif-\nference between output probabilities for an adversarial and\nits corresponding natural example. However, it may have\na negative impact if a natural example is misclassified. To\ncircumvent this issue, we propose a novel adversarial train-\ning scheme that encourages the model to produce similar\noutput probabilities for an adversarial example and its “in-\nverse adversarial” counterpart. Particularly, the counter-\npart is generated by maximizing the likelihood in the neigh-\nborhood of the natural example. Extensive experiments on\nvarious vision datasets and architectures demonstrate that\nour training method achieves state-of-the-art robustness as\nwell as natural accuracy among robust models. Further-\nmore, using a universal version of inverse adversarial ex-\namples, we improve the performance of single-step adver-\nsarial training techniques at a low computational cost.\n",
        "question": {
            "statement": "What is the main goal of adversarial training in deep learning?",
            "options": [
                "to generate more diverse outputs",
                "to increase the model's robustness against adversarial attacks",
                "to improve the model's accuracy on natural data",
                "to reduce the computational cost of training"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Filtering, Distillation, and Hard Negatives for Vision-Language Pre-Training\nFilip Radenovic1, Abhimanyu Dubey1⇤, Abhishek Kadian1⇤, Todor Mihaylov1⇤, Simon Vandenhende1⇤\nYash Patel2†, Yi Wen1, Vignesh Ramanathan1 and Dhruv Mahajan1‡\n1Meta AI\n2CTU in Prague\nAbstract\nVision-language models trained with contrastive learn-\ning on large-scale noisy data are becoming increasingly\npopular for zero-shot recognition problems. In this paper\nwe improve the following three aspects of the contrastive\npre-training pipeline: dataset noise, model initialization\nand the training objective. First, we propose a straightfor-\nward ﬁltering strategy titled Complexity, Action, and Text-\nspotting (CAT) that signiﬁcantly reduces dataset size, while\nachieving improved performance across zero-shot vision-\nlanguage tasks. Next, we propose an approach titled Con-\ncept Distillation to leverage strong unimodal representa-\ntions for contrastive training that does not increase train-\ning complexity while outperforming prior work. Finally, we\nmodify the traditional contrastive alignment objective, and\npropose an importance-sampling approach to up-sample\nthe importance of hard-negatives without adding additional\ncomplexity.\nOn an extensive zero-shot benchmark of 29\ntasks, our Distilled and Hard-negative Training (DiHT) ap-\nproach improves on 20 tasks compared to the baseline. Fur-\nthermore, for few-shot linear probing, we propose a novel\napproach that bridges the gap between zero-shot and few-\nshot performance, substantially improving over prior work.\nModels are available at github.com/facebookresearch/diht.\n",
        "question": {
            "statement": "What is the main goal of the proposed CAT filtering strategy in vision-language pre-training?",
            "options": [
                "Enhancing the interpretability of the learned representations",
                "Increasing the number of parameters in the model",
                "Improving the quality of the generated text",
                "Reducing dataset size while maintaining or improving performance"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "A Characteristic Function-based Method for Bottom-up Human Pose Estimation\nHaoxuan Qu1, Yujun Cai2, Lin Geng Foo1, Ajay Kumar3, Jun Liu 1,*\n1Singapore University of Technology and Design, Singapore\n2Nanyang Technological University, Singapore\n3The Hong Kong Polytechnic University, Hong Kong\nhaoxuan qu@mymail.sutd.edu.sg, yujun001@e.ntu.edu.sg, lingeng foo@mymail.sutd.edu.sg\najay.kumar@polyu.edu.hk, jun liu@sutd.edu.sg\nAbstract\nMost recent methods formulate the task of human pose\nestimation as a heatmap estimation problem, and use the\noverall L2 loss computed from the entire heatmap to opti-\nmize the heatmap prediction. In this paper, we show that\nin bottom-up human pose estimation where each heatmap\noften contains multiple body joints, using the overall L2\nloss to optimize the heatmap prediction may not be the op-\ntimal choice. This is because, minimizing the overall L2\nloss cannot always lead the model to locate all the body\njoints across different sub-regions of the heatmap more ac-\ncurately. To cope with this problem, from a novel perspec-\ntive, we propose a new bottom-up human pose estimation\nmethod that optimizes the heatmap prediction via minimiz-\ning the distance between two characteristic functions re-\nspectively constructed from the predicted heatmap and the\ngroundtruth heatmap. Our analysis presented in this pa-\nper indicates that the distance between these two charac-\nteristic functions is essentially the upper bound of the L2\nlosses w.r.t. sub-regions of the predicted heatmap. There-\nfore, via minimizing the distance between the two character-\nistic functions, we can optimize the model to provide a more\naccurate localization result for the body joints in different\nsub-regions of the predicted heatmap. We show the effec-\ntiveness of our proposed method through extensive exper-\niments on the COCO dataset and the CrowdPose dataset.\n",
        "question": {
            "statement": "What is a limitation of using the overall L2 loss to optimize heatmap prediction in bottom-up human pose estimation?",
            "options": [
                "It is only applicable to top-down human pose estimation",
                "It is computationally expensive",
                "It requires large amounts of training data",
                "It may not accurately locate all body joints across different sub-regions of the heatmap"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Phase-Shifting Coder: Predicting Accurate Orientation\nin Oriented Object Detection\nYi Yu1,2, Feipeng Da1,2,*\n1School of Automation, Southeast University, Nanjing, China\n2Key Laboratory of Measurement and Control of Complex Systems of Engineering,\nMinistry of Education, Southeast University, Nanjing, China\n{yuyi, dafp}@seu.edu.cn\nAbstract\nWith the vigorous development of computer vision, ori-\nented object detection has gradually been featured. In this\npaper, a novel differentiable angle coder named phase-\nshifting coder (PSC) is proposed to accurately predict the\norientation of objects, along with a dual-frequency version\n(PSCD). By mapping the rotational periodicity of differ-\nent cycles into the phase of different frequencies, we pro-\nvide a unified framework for various periodic fuzzy prob-\nlems caused by rotational symmetry in oriented object de-\ntection.\nUpon such a framework, common problems in\noriented object detection such as boundary discontinuity\nand square-like problems are elegantly solved in a unified\nform. Visual analysis and experiments on three datasets\nprove the effectiveness and the potentiality of our approach.\nWhen facing scenarios requiring high-quality bounding\nboxes, the proposed methods are expected to give a com-\npetitive performance. The codes are publicly available at\nhttps://github.com/open-mmlab/mmrotate.\n",
        "question": {
            "statement": "What is the primary goal of the Phase-Shifting Coder (PSC) in oriented object detection?",
            "options": [
                "To accurately predict the orientation of objects",
                "To detect objects in images",
                "To improve image quality",
                "To classify objects into categories"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Co-SLAM: Joint Coordinate and Sparse Parametric Encodings for\nNeural Real-Time SLAM\nHengyi Wang⋆\nJingwen Wang⋆\nLourdes Agapito\nDepartment of Computer Science, University College London\n{hengyi.wang.21, jingwen.wang.17, l.agapito}@ucl.ac.uk\na) Input RGB-D stream\nb) Our Tracking & Mapping\nOurs\nNICE-SLAM\nScanNet mesh\nc) Zoom in\nTrajectory\nFigure 1. We present Co-SLAM, a neural RGB-D SLAM method that performs online tracking and mapping in real time. We propose a\nnew hybrid representation based on a joint coordinate and sparse-parametric encoding with global bundle adjustment. Our method shows\nfast, high-fidelity scene reconstruction with efficient memory use and plausible hole-filling.\nAbstract\nWe present Co-SLAM, a neural RGB-D SLAM system\nbased on a hybrid representation, that performs robust cam-\nera tracking and high-fidelity surface reconstruction in real\ntime. Co-SLAM represents the scene as a multi-resolution\nhash-grid to exploit its high convergence speed and abil-\nity to represent high-frequency local features.\nIn addi-\ntion, Co-SLAM incorporates one-blob encoding, to encour-\nage surface coherence and completion in unobserved ar-\neas.\nThis joint parametric-coordinate encoding enables\nreal-time and robust performance by bringing the best of\nboth worlds: fast convergence and surface hole filling.\nMoreover, our ray sampling strategy allows Co-SLAM to\nperform global bundle adjustment over all keyframes in-\nstead of requiring keyframe selection to maintain a small\nnumber of active keyframes as competing neural SLAM ap-\nproaches do. Experimental results show that Co-SLAM runs\nat 10−17Hz and achieves state-of-the-art scene reconstruc-\ntion results, and competitive tracking performance in vari-\nous datasets and benchmarks (ScanNet, TUM, Replica, Syn-\nthetic RGBD). Project page: https://hengyiwang.\ngithub.io/projects/CoSLAM\n⋆Indicates equal contribution.\n",
        "question": {
            "statement": "What is a key advantage of using a joint parametric-coordinate encoding in a neural SLAM system?",
            "options": [
                "Enables fast convergence and surface hole filling",
                "Reduces computational resources required",
                "Enhances visualization of reconstructed scenes",
                "Improves camera tracking accuracy"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "3",
                "2",
                "7"
            ]
        },
        "difference": 3,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Improving Visual Representation Learning through Perceptual Understanding\nSamyakh Tukra\nFrederick Hoffman\nKen Chatfield\nTractable AI\n{samyakh.tukra,frederick.hoffman,ken}@tractable.ai\nAbstract\nWe present an extension to masked autoencoders (MAE)\nwhich improves on the representations learnt by the model\nby explicitly encouraging the learning of higher scene-level\nfeatures. We do this by: (i) the introduction of a perceptual\nsimilarity term between generated and real images (ii) in-\ncorporating several techniques from the adversarial train-\ning literature including multi-scale training and adaptive\ndiscriminator augmentation. The combination of these re-\nsults in not only better pixel reconstruction but also repre-\nsentations which appear to capture better higher-level de-\ntails within images. More consequentially, we show how\nour method, Perceptual MAE, leads to better performance\nwhen used for downstream tasks outperforming previous\nmethods. We achieve 78.1% top-1 accuracy linear probing\non ImageNet-1K and up to 88.1% when fine-tuning, with\nsimilar results for other downstream tasks, all without use\nof additional pre-trained models or data.\n",
        "question": {
            "statement": "What is the primary goal of introducing a perceptual similarity term between generated and real images in the extended masked autoencoder model?",
            "options": [
                "To encourage the learning of higher scene-level features",
                "To reduce the need for additional pre-trained models",
                "To improve pixel reconstruction only",
                "To simplify the model architecture"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "3",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Robust Mean Teacher for Continual and Gradual Test-Time Adaptation\nMario D¨\nobler *\nRobert A. Marsden *\nBin Yang\nUniversity of Stuttgart\n{mario.doebler, robert.marsden, bin.yang}@iss.uni-stuttgart.de\nAbstract\nSince experiencing domain shifts during test-time is in-\nevitable in practice, test-time adaption (TTA) continues to\nadapt the model after deployment. Recently, the area of\ncontinual and gradual test-time adaptation (TTA) emerged.\nIn contrast to standard TTA, continual TTA considers not\nonly a single domain shift, but a sequence of shifts. Grad-\nual TTA further exploits the property that some shifts evolve\ngradually over time. Since in both settings long test se-\nquences are present, error accumulation needs to be ad-\ndressed for methods relying on self-training. In this work,\nwe propose and show that in the setting of TTA, the sym-\nmetric cross-entropy is better suited as a consistency loss\nfor mean teachers compared to the commonly used cross-\nentropy. This is justified by our analysis with respect to\nthe (symmetric) cross-entropy’s gradient properties. To pull\nthe test feature space closer to the source domain, where\nthe pre-trained model is well posed, contrastive learning\nis leveraged.\nSince applications differ in their require-\nments, we address several settings, including having source\ndata available and the more challenging source-free setting.\nWe demonstrate the effectiveness of our proposed method\n“robust mean teacher“ (RMT) on the continual and grad-\nual corruption benchmarks CIFAR10C, CIFAR100C, and\nImagenet-C. We further consider ImageNet-R and propose a\nnew continual DomainNet-126 benchmark. State-of-the-art\nresults are achieved on all benchmarks. 1\n",
        "question": {
            "statement": "What is a key challenge in continual and gradual test-time adaptation?",
            "options": [
                "Model complexity",
                "Error accumulation",
                "Overfitting to the target domain",
                "Insufficient training data"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Self-Supervised Image-to-Point Distillation via Semantically Tolerant\nContrastive Loss\nAnas Mahmoud1, Jordan S. K. Hu1, Tianshu Kuai1, Ali Harakeh2, Liam Paull2, and Steven L. Waslander1\n1University of Toronto Robotics Institute, 2Mila, UniversitÂ\ne de MontrÂ\neal\nAbstract\nAn effective framework for learning 3D representations\nfor perception tasks is distilling rich self-supervised image\nfeatures via contrastive learning. However, image-to-point\nrepresentation learning for autonomous driving datasets\nfaces two main challenges:\n1) the abundance of self-\nsimilarity, which results in the contrastive losses pushing\naway semantically similar point and image regions and thus\ndisturbing the local semantic structure of the learned rep-\nresentations, and 2) severe class imbalance as pretraining\ngets dominated by over-represented classes. We propose to\nalleviate the self-similarity problem through a novel seman-\ntically tolerant image-to-point contrastive loss that takes\ninto consideration the semantic distance between positive\nand negative image regions to minimize contrasting seman-\ntically similar point and image regions. Additionally, we\naddress class imbalance by designing a class-agnostic bal-\nanced loss that approximates the degree of class imbalance\nthrough an aggregate sample-to-samples semantic similar-\nity measure. We demonstrate that our semantically-tolerant\ncontrastive loss with class balancing improves state-of-the-\nart 2D-to-3D representation learning in all evaluation set-\ntings on 3D semantic segmentation.\nOur method con-\nsistently outperforms state-of-the-art 2D-to-3D representa-\ntion learning frameworks across a wide range of 2D self-\nsupervised pretrained models.\n",
        "question": {
            "statement": "What is a major challenge in image-to-point representation learning for autonomous driving datasets?",
            "options": [
                "lack of annotated labels",
                "severe class imbalance",
                "inadequate computational resources",
                "insufficient training data"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "5",
                "0",
                "0"
            ]
        },
        "difference": 5,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Looking Through the Glass: Neural Surface Reconstruction Against\nHigh Specular Reflections\nJiaxiong Qiu1\nPeng-Tao Jiang2\nYifan Zhu1\nZe-Xin Yin1\nMing-Ming Cheng1\nBo Ren1*\n1VCIP, CS, Nankai University\n2Zhejiang University\nAbstract\nNeural implicit methods have achieved high-quality 3D\nobject surfaces under slight specular highlights. However,\nhigh specular reflections (HSR) often appear in front of tar-\nget objects when we capture them through glasses.\nThe\ncomplex ambiguity in these scenes violates the multi-view\nconsistency, then makes it challenging for recent methods\nto reconstruct target objects correctly. To remedy this is-\nsue, we present a novel surface reconstruction framework,\nNeuS-HSR, based on implicit neural rendering. In NeuS-\nHSR, the object surface is parameterized as an implicit\nsigned distance function (SDF). To reduce the interference\nof HSR, we propose decomposing the rendered image into\ntwo appearances: the target object and the auxiliary plane.\nWe design a novel auxiliary plane module by combining\nphysical assumptions and neural networks to generate the\nauxiliary plane appearance. Extensive experiments on syn-\nthetic and real-world datasets demonstrate that NeuS-HSR\noutperforms state-of-the-art approaches for accurate and\nrobust target surface reconstruction against HSR. Code is\navailable at https://github.com/JiaxiongQ/\nNeuS-HSR.\n",
        "question": {
            "statement": "What is a common challenge in reconstructing 3D object surfaces from images captured through glasses?",
            "options": [
                "Shadows",
                "Occlusions",
                "Specular reflections",
                "Motion blur"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "MCF: Mutual Correction Framework for Semi-Supervised Medical Image\nSegmentation\nYongchao Wang\nBin Xiao∗Xiuli Bi\nWeisheng Li\nXinbo Gao\nChongqing University of Posts and Telecommunications\nChongqing, China\nd200201023@stu.cqupt.edu.cn, {xiaobin,bixl,liws,gaoxb}@cqupt.edu.cn\nAbstract\nSemi-supervised learning is a promising method for\nmedical image segmentation under limited annotation.\nHowever, the model cognitive bias impairs the segmentation\nperformance, especially for edge regions.\nFurthermore,\ncurrent mainstream semi-supervised medical image seg-\nmentation (SSMIS) methods lack designs to handle model\nbias.\nThe neural network has a strong learning ability,\nbut the cognitive bias will gradually deepen during the\ntraining, and it is difﬁcult to correct itself.\nWe propose\na novel mutual correction framework (MCF) to explore\nnetwork bias correction and improve the performance of\nSSMIS. Inspired by the plain contrast idea, MCF intro-\nduces two different subnets to explore and utilize the dis-\ncrepancies between subnets to correct cognitive bias of the\nmodel.\nMore concretely, a contrastive difference review\n(CDR) module is proposed to ﬁnd out inconsistent pre-\ndiction regions and perform a review training. Addition-\nally, a dynamic competitive pseudo-label generation (DC-\nPLG) module is proposed to evaluate the performance of\nsubnets in real-time, dynamically selecting more reliable\npseudo-labels.\nExperimental results on two medical im-\nage databases with different modalities (CT and MRI) show\nthat our method achieves superior performance compared\nto several state-of-the-art methods. The code will be avail-\nable at https://github.com/WYC-321/MCF.\n",
        "question": {
            "statement": "What is a major limitation of current semi-supervised medical image segmentation methods?",
            "options": [
                "They are unable to generalize to new datasets",
                "They lack designs to handle model bias.",
                "They require large amounts of annotated data",
                "They are only applicable to specific medical imaging modalities"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Causally-Aware Intraoperative Imputation for Overall Survival Time Prediction\nXiang Li1,∗, Xuelin Qian1,∗, Litian Liang1,∗, Lingjie Kong1, Qiaole Dong1, Jiejun Chen2\nDingxia Liu2, Xiuzhong Yao2,†, Yanwei Fu1,†\n1School of Data Science, Fudan University\n2Department of Radiology, Zhongshan Hospital, Fudan University\n{li x20,xlqian,ltliang19,qldong18,jjchen20,yanweifu}@fudan.edu.cn\n{jkong22,dxliu21}@m.fudan.edu.cn\nyao.xiuzhong@zs-hospital.sh.cn\nCause: PLC\nHBV: (+)\nHCV: (-)\n… …\n109mths\n≥9yrs\n≥9yrs\n≤3yrs\nPre-operative \nindicators\nMRI\nT1ce-AP\nIntra-operative \nindicators\nAged 45\n16mths\nPred\nPred\nPred\nPred\nAged 39\nA\nB\nPatients\n23.15\n31.88\n33.83\n42.21\n20\n25\n30\n35\n40\n45\n50\nRandom\nMRI\nMRI+Pre\nOurs (CAWIM)\nOverall Performance Comparison of F1-score \nRandom\nMRI\nMRI+Pre\nOurs (CAWIM)\nA0: Clinicopathologic hepatocirrhosis\nA1: Clinicopathologic cancer embolus\nA2: G-score         \nA4: Cirrhosis nodules\nA3: S-score          \nA5: Hepatocirrhosis\nPatient A\nPatient B\nFigure 1. Our idea illustration. Patient A and B have different OS time (109 months and 16 months, respectively) but similar pre-operative\nimage-based patterns and indexes; thus, preoperative based model fails to distinguish A and B (their predicted OS time are both larger\nthan 9 years). However, their intra-operative attributes, which can describe the severity of disease in a more informative way, are very\ndifferent from each other (e.g., G-score, S-score, Clinicopathologic hepatocirrhosis). Therefore, it can help the model discriminate A and\nB. In the right image, we show that our CAWIM – that leverages intra-operative indexes in the training stage – can largely improve the\nprediction power than other methods. Our method managed to correctly classify these two cases, and the overall performance of our\nCAWIM surpasses other baseline models by approximately 10 points on 4-category classification task.\nAbstract\nPrevious efforts in vision community are mostly made on\nlearning good representations from visual patterns. Beyond\nthis, this paper emphasizes the high-level ability of causal\nreasoning. We thus present a case study of solving the chal-\nlenging task of Overall Survival (OS) time in primary liver\ncancers. Critically, the prediction of OS time at the early\nstage remains challenging, due to the unobvious image pat-\nterns of reflecting the OS. To this end, we propose a causal\ninference system by leveraging the intraoperative attributes\nand the correlation among them, as an intermediate super-\nvision to bridge the gap between the images and the final\nOS. Particularly, we build a causal graph, and train the im-\nages to estimate the intraoperative attributes for final OS\nprediction.\nWe present a novel Causally-aware Intraop-\nerative Imputation Model (CAWIM) that can sequentially\npredict each attribute using its parent nodes in the esti-\nmated causal graph. To determine the causal directions,\n*Equal contribution\n†Corresponding author\nwe propose a splitting-voting mechanism, which votes for\nthe direction for each pair of adjacent nodes among multi-\nple predictions obtained via causal discovery from hetero-\ngeneity. The practicability and effectiveness of our method\nare demonstrated by the promising results on liver cancer\ndataset of 361 patients with long-term observations.\n",
        "question": {
            "statement": "What is the main challenge in predicting Overall Survival time in primary liver cancers?",
            "options": [
                " Limited availability of MRI scans",
                "Insufficient patient population",
                "Unobvious image patterns",
                "Lack of intraoperative data"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Neural Intrinsic Embedding for Non-rigid Point Cloud Matching\nPuhua Jiang1,2\nMingze Sun1\nRuqi Huang1\n1Tsinghua Shenzhen International Graduate School\n2Peng Cheng Laboratory\n{jph21, smz22}@mails.tsinghua.edu.cn\nruqihuang@sz.tsinghua.edu.cn\nFigure 1. Given a point cloud, we select 5 landmarks (see red points on the right-most one) and assign each of the rest points to the\ncluster represented by its nearest neighbor among the landmarks in the respective embedded space. We compare our method to Euclidean\ncoordinates, LIE [28], and GPS [40]. Our method takes in only the point cloud and produces segmentation that is intrinsic geometry-aware.\nAbstract\nAs a primitive 3D data representation, point clouds\nare prevailing in 3D sensing, yet short of intrinsic struc-\ntural information of the underlying objects. Such discrep-\nancy poses great challenges in directly establishing corre-\nspondences between point clouds sampled from deformable\nshapes. In light of this, we propose Neural Intrinsic Embed-\nding (NIE) to embed each vertex into a high-dimensional\nspace in a way that respects the intrinsic structure. Based\nupon NIE, we further present a weakly-supervised learn-\ning framework for non-rigid point cloud registration. Un-\nlike the prior works, we do not require expansive and sen-\nsitive off-line basis construction (e.g., eigen-decomposition\nof Laplacians), nor do we require ground-truth correspon-\ndence labels for supervision. We empirically show that our\nframework performs on par with or even better than the\nstate-of-the-art baselines, which generally require more su-\npervision and/or more structural geometric input.\n",
        "question": {
            "statement": "What is a major challenge in establishing correspondences between point clouds sampled from deformable shapes?",
            "options": [
                "Lack of intrinsic structural information",
                "Limited number of points in the point cloud",
                "High dimensionality of the point clouds",
                "Noise in the point cloud data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "GrowSP: Unsupervised Semantic Segmentation of 3D Point Clouds\nZihui Zhang,\nBo Yang*\n,\nBing Wang,\nBo Li\nShenzhen Research Institute, The Hong Kong Polytechnic University\nvLAR Group, The Hong Kong Polytechnic University\nzihui.zhang@connect.polyu.hk, bo.yang@polyu.edu.hk\nInput Point Cloud\nInitial Superpoints\nProgressively Growing Superpoints\nPrediction by GrowSP\nGround Truth\nFigure 1. Given an input point cloud with complex structures from S3DIS dataset [2], our GrowSP automatically discovers accurate\nsemantic classes simply by progressively growing superpoints, without needing any human labels in training.\nAbstract\nWe study the problem of 3D semantic segmentation from\nraw point clouds. Unlike existing methods which primarily\nrely on a large amount of human annotations for training\nneural networks, we propose the first purely unsupervised\nmethod, called GrowSP, to successfully identify complex se-\nmantic classes for every point in 3D scenes, without need-\ning any type of human labels or pretrained models. The key\nto our approach is to discover 3D semantic elements via\nprogressive growing of superpoints. Our method consists\nof three major components, 1) the feature extractor to learn\nper-point features from input point clouds, 2) the superpoint\nconstructor to progressively grow the sizes of superpoints,\nand 3) the semantic primitive clustering module to group\nsuperpoints into semantic elements for the final semantic\nsegmentation. We extensively evaluate our method on mul-\ntiple datasets, demonstrating superior performance over all\nunsupervised baselines and approaching the classic fully-\nsupervised PointNet. We hope our work could inspire more\nadvanced methods for unsupervised 3D semantic learning.\n",
        "question": {
            "statement": "What is the primary advantage of the GrowSP method for 3D semantic segmentation?",
            "options": [
                "It does not require any human annotations or pre-trained models",
                "It can only be applied to simple structures",
                "It is limited to specific types of point clouds",
                "It requires a large amount of computational resources"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Few-Shot Referring Relationships in Videos\nYogesh Kumar and Anand Mishra\nIndian Institute of Technology Jodhpur\n{kumar.204, mishra}@iitj.ac.in\nhttps://vl2g.github.io/projects/refRelations/\n<helicopter, ﬂy above, train>\n<plane, ﬂy above, truck>\nSupport Set\nvideo-1\nvideo-2\nvideo-3\nvideo-4\nTest video\nOutput\nfall off\nFrequency\nPredicate\nAccuracy\n(a) Problem Setup\n(c) Accuracy Distribution\n<bird, ﬂy above, person>\n<plane, ﬂy above, plane>\n<plane, ﬂy above, person>\n(b) Predicate Distribution\nride\nFigure 1. The proposed problem setup. (a) Given a query visual relationship as <subject, predicate, object> and a test video, our goal is\nto localize the subject and object on the test video using a support set containing a few videos sharing the same predicate. In this example,\nthe goal is to spatiotemporally localize the plane (subject), and person (object) that are connected via fly above (predicate), using a support\nset containing only four videos sharing predicate fly above. It should be noted here that fly above is unseen during training. We refer to this\nproblem as few-shot referring relationship in videos. This problem setup is inspired by the real-world scenario where obtaining large-scale\nannotations for every visual relationship is practically infeasible. As shown in (b), a popular visual relationship video dataset, namely\nImageNet-VidVRD [27], contains many predicates with very few examples, i.e., it has long-tail distribution. Further, as shown in (c), the\nsuccess of a recent visual relationship localization technique (vRGV) [35] is clearly proportional to predicate distribution in the train set.\nThis calls for solving referring relationship tasks in a few-shot setup. We propose this task and present a novel principled solution.\nAbstract\nInterpreting visual relationships is a core aspect of com-\nprehensive video understanding. Given a query visual re-\nlationship as <subject, predicate, object> and a test video,\nour objective is to localize the subject and object that are\nconnected via the predicate. Given modern visio-lingual\nunderstanding capabilities, solving this problem is achiev-\nable, provided that there are large-scale annotated training\nexamples available. However, annotating for every combi-\nnation of subject, object, and predicate is cumbersome, ex-\npensive, and possibly infeasible. Therefore, there is a need\nfor models that can learn to spatially and temporally lo-\ncalize subjects and objects that are connected via an un-\nseen predicate using only a few support set videos shar-\ning the common predicate.\nWe address this challenging\nproblem, referred to as few-shot referring relationships in\nvideos for the first time. To this end, we pose the problem\nas a minimization of an objective function defined over a\nT-partite random field. Here, the vertices of the random\nfield correspond to candidate bounding boxes for the sub-\nject and object, and T represents the number of frames in\nthe test video. This objective function is composed of frame-\nlevel and visual relationship similarity potentials. To learn\nthese potentials, we use a relation network that takes query-\nconditioned translational relationship embedding as inputs\nand is meta-trained using support set videos in an episodic\nmanner. Further, the objective function is minimized using\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n2289\na belief propagation-based message passing on the random\nfield to obtain the spatiotemporal localization or subject\nand object trajectories. We perform extensive experiments\nusing two public benchmarks, namely ImageNet-VidVRD\nand VidOR, and compare the proposed approach with com-\npetitive baselines to assess its efficacy.\n",
        "question": {
            "statement": "What is the main challenge in localizing visual relationships in videos?",
            "options": [
                "difficulty in defining visual relationships",
                "computational power required for processing videos",
                "annotating for every combination of subject, object, and predicate is cumbersome, expensive, and possibly infeasible",
                "limited availability of video datasets"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Sliced Optimal Partial Transport\nYikun Bai∗\n1& Bernhard Schmitzer∗\n2& Matthew Thorpe3,4& Soheil Kolouri1\n1Department of Computer Science, Vanderbilt University\n2Institute of Computer Science, G¨\nottingen University\n3Department of Mathematics, University of Manchester\n4The Alan Turing Institute\nAbstract\nOptimal transport (OT) has become exceedingly popu-\nlar in machine learning, data science, and computer vision.\nThe core assumption in the OT problem is the equal to-\ntal amount of mass in source and target measures, which\nlimits its application. Optimal Partial Transport (OPT) is\na recently proposed solution to this limitation. Similar to\nthe OT problem, the computation of OPT relies on solving\na linear programming problem (often in high dimensions),\nwhich can become computationally prohibitive. In this pa-\nper, we propose an efﬁcient algorithm for calculating the\nOPT problem between two non-negative measures in one\ndimension. Next, following the idea of sliced OT distances,\nwe utilize slicing to deﬁne the Sliced OPT distance. Finally,\nwe demonstrate the computational and accuracy beneﬁts of\nthe Sliced OPT-based method in various numerical exper-\niments. In particular, we show an application of our pro-\nposed Sliced OPT problem in noisy point cloud registration\nand color adaptation. Our code is available at Github Link.\n",
        "question": {
            "statement": "What is the main limitation of the optimal transport problem in machine learning and data science?",
            "options": [
                "The inability to handle noisy data",
                "The need for high-dimensional data",
                "The requirement for linear programming",
                "The assumption that the total amount of mass in the source and target measures must be equal"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Generating Features with Increased Crop-related Diversity\nfor Few-Shot Object Detection\nJingyi Xu\nStony Brook University\njingyixu@cs.stonybrook.edu\nHieu Le\nEPFL\nminh.le@epfl.ch\nDimitris Samaras\nStony Brook University\nsamaras@cs.stonybrook.edu\nAbstract\nTwo-stage object detectors generate object proposals\nand classify them to detect objects in images. These pro-\nposals often do not contain the objects perfectly but overlap\nwith them in many possible ways, exhibiting great variabil-\nity in the difficulty levels of the proposals. Training a ro-\nbust classifier against this crop-related variability requires\nabundant training data, which is not available in few-shot\nsettings. To mitigate this issue, we propose a novel vari-\national autoencoder (VAE) based data generation model,\nwhich is capable of generating data with increased crop-\nrelated diversity. The main idea is to transform the latent\nspace such latent codes with different norms represent dif-\nferent crop-related variations. This allows us to generate\nfeatures with increased crop-related diversity in difficulty\nlevels by simply varying the latent norm. In particular, each\nlatent code is rescaled such that its norm linearly correlates\nwith the IoU score of the input crop w.r.t. the ground-truth\nbox. Here the IoU score is a proxy that represents the dif-\nficulty level of the crop. We train this VAE model on base\nclasses conditioned on the semantic code of each class and\nthen use the trained model to generate features for novel\nclasses. In our experiments our generated features con-\nsistently improve state-of-the-art few-shot object detection\nmethods on the PASCAL VOC and MS COCO datasets.\n",
        "question": {
            "statement": "What is the primary goal of using a variational autoencoder (VAE) in few-shot object detection?",
            "options": [
                "To generate features with increased diversity in difficulty levels",
                "To reduce the number of object proposals",
                "To improve the accuracy of the ground-truth box",
                "To eliminate the need for abundant training data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "1",
                "2",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Bridging Precision and Confidence:\nA Train-Time Loss for Calibrating Object Detection\nMuhammad Akhtar Munir1,2\nMuhammad Haris Khan1\nSalman Khan1,3\nFahad Shahbaz Khan1,4\n1Mohamed bin Zayed University of AI\n2Information Technology University\n3Australian National University\n4Link¨\noping University\n{akhtar.munir,muhammad.haris,salman.khan,fahad.khan}@mbzuai.ac.ae\nAbstract\nDeep neural networks (DNNs) have enabled astound-\ning progress in several vision-based problems.\nDespite\nshowing high predictive accuracy, recently, several works\nhave revealed that they tend to provide overconfident pre-\ndictions and thus are poorly calibrated. The majority of\nthe works addressing the miscalibration of DNNs fall un-\nder the scope of classification and consider only in-domain\npredictions. However, there is little to no progress in study-\ning the calibration of DNN-based object detection models,\nwhich are central to many vision-based safety-critical ap-\nplications. In this paper, inspired by the train-time calibra-\ntion methods, we propose a novel auxiliary loss formulation\nthat explicitly aims to align the class confidence of bound-\ning boxes with the accurateness of predictions (i.e. preci-\nsion). Since the original formulation of our loss depends\non the counts of true positives and false positives in a mini-\nbatch, we develop a differentiable proxy of our loss that can\nbe used during training with other application-specific loss\nfunctions. We perform extensive experiments on challeng-\ning in-domain and out-domain scenarios with six bench-\nmark datasets including MS-COCO, Cityscapes, Sim10k,\nand BDD100k. Our results reveal that our train-time loss\nsurpasses strong calibration baselines in reducing calibra-\ntion error for both in and out-domain scenarios. Our source\ncode and pre-trained models are available at https://\ngithub.com/akhtarvision/bpc_calibration\n",
        "question": {
            "statement": "What is a common issue with deep neural networks (DNNs) despite their high predictive accuracy?",
            "options": [
                "overconfidence in their predictions",
                "inability to handle complex tasks",
                "high computational requirements",
                "insufficient training data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on\nVisual Language Understanding\nMorris Alper∗, Michael Fiman∗, Hadar Averbuch-Elor\nTel Aviv University\nAlex gave Riley a present\nAlex gave Riley an ultimatum\nthe ocean is [*]-colored\na corn chip is [*]-shaped\nHow concrete are the words present and ultimatum?\nWhat word should be inserted in the blank?\nFigure 1. In this paper, we propose a suite of visual language understanding tasks for probing the visual reasoning capabilities of text\nencoder models. While we consider text-only tasks (i.e., processing only the textual descriptions above without associated imagery), we\nargue that they require visual imagination to complete and can thus benefit from vision-and-language pretraining. For instance, consider\nthe words present and ultimatum. A simple online query (that considers only freely-available images) roughly yields a coherent set of\nimages for the more concrete word (namely present), while the latter cannot be uniquely depicted. Likewise, selecting the most natural\ncolor or shape descriptors in cloze contexts as shown in the two examples on the right requires implicit knowledge of the appearance of the\nreferent under consideration (ocean and corn chip respectively).\nAbstract\nMost humans use visual imagination to understand and\nreason about language, but models such as BERT reason\nabout language using knowledge acquired during text-only\npretraining. In this work, we investigate whether vision-\nand-language pretraining can improve performance on text-\nonly tasks that involve implicit visual reasoning, focusing\nprimarily on zero-shot probing methods. We propose a suite\nof visual language understanding (VLU) tasks for probing\nthe visual reasoning abilities of text encoder models, as\nwell as various non-visual natural language understanding\n(NLU) tasks for comparison. We also contribute a novel\nzero-shot knowledge probing method, Stroop probing, for\napplying models such as CLIP to text-only tasks without\nneeding a prediction head such as the masked language\nmodelling head of models like BERT. We show that SOTA\nmultimodally trained text encoders outperform unimodally\ntrained text encoders on the VLU tasks while being under-\nperformed by them on the NLU tasks, lending new context\nto previously mixed results regarding the NLU capabilities\nof multimodal models. We conclude that exposure to images\nduring pretraining affords inherent visual reasoning knowl-\n*These authors contributed equally to this work\nedge that is reflected in language-only tasks that require im-\nplicit visual reasoning. Our findings bear importance in the\nbroader context of multimodal learning, providing princi-\npled guidelines for the choice of text encoders used in such\ncontexts1.\n",
        "question": {
            "statement": "What is the primary difference between how humans process language and how models like BERT process language?",
            "options": [
                "Both humans and models like BERT rely on visual imagination",
                "Humans use visual imagination, while models like BERT rely solely on text-based knowledge",
                "There is no significant difference between human and model language processing",
                "Models like BERT are superior at processing language than humans"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval\nKuniaki Saito1,2∗, Kihyuk Sohn3 , Xiang Zhang2 , Chun-Liang Li2 ,\nChen-Yu Lee2 , Kate Saenko1,4 , Tomas Pﬁster2\n{keisaito, saenko}@bu.edu\n{kihyuks,fancyzhx,chunliang,chenyulee,tpfister}@google.com\n1Boston University, 2Google Cloud AI Research, 3Google Research, 4MIT-IBM Watson AI Lab\nAbstract\nIn Composed Image Retrieval (CIR), a user combines\na query image with text to describe their intended target.\nExisting methods rely on supervised learning of CIR mod-\nels using labeled triplets consisting of the query image, text\nspeciﬁcation, and the target image. Labeling such triplets\nis expensive and hinders broad applicability of CIR. In this\nwork, we propose to study an important task, Zero-Shot\nComposed Image Retrieval (ZS-CIR), whose goal is to build\na CIR model without requiring labeled triplets for training.\nTo this end, we propose a novel method, called Pic2Word,\nthat requires only weakly labeled image-caption pairs and\nunlabeled image datasets to train. Unlike existing super-\nvised CIR models, our model trained on weakly labeled\nor unlabeled datasets shows strong generalization across\ndiverse ZS-CIR tasks, e.g., attribute editing, object com-\nposition, and domain conversion. Our approach outper-\nforms several supervised CIR methods on the common CIR\nbenchmark, CIRR and Fashion-IQ. Code will be made pub-\nlicly available at https://github.com/google-\nresearch/composed_image_retrieval\n",
        "question": {
            "statement": "What is the primary goal of Zero-Shot Composed Image Retrieval (ZS-CIR)?",
            "options": [
                "to improve the accuracy of existing CIR models",
                "to increase the size of image datasets",
                "to develop a new type of image captioning system",
                "to build a CIR model without requiring labeled triplets for training"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "A Unified Spatial-Angular Structured Light for Single-View Acquisition of\nShape and Reflectance\nXianmin Xu1* Yuxin Lin1* Haoyang Zhou1 Chong Zeng1 Yaxin Yu1 Kun Zhou1,2 † Hongzhi Wu1 †\n1State Key Lab of CAD&CG, Zhejiang University\n2 ZJU-FaceUnity Joint Lab of Intelligent Graphics\nAbstract\nWe propose a unified structured light, consisting of an\nLED array and an LCD mask, for high-quality acquisition\nof both shape and reflectance from a single view. For ge-\nometry, one LED projects a set of learned mask patterns to\naccurately encode spatial information; the decoded results\nfrom multiple LEDs are then aggregated to produce a final\ndepth map. For appearance, learned light patterns are cast\nthrough a transparent mask to efficiently probe angularly-\nvarying reflectance. Per-point BRDF parameters are differ-\nentiably optimized with respect to corresponding measure-\nments, and stored in texture maps as the final reflectance.\nWe establish a differentiable pipeline for the joint capture to\nautomatically optimize both the mask and light patterns to-\nwards optimal acquisition quality. The effectiveness of our\nlight is demonstrated with a wide variety of physical ob-\njects. Our results compare favorably with state-of-the-art\ntechniques.\n",
        "question": {
            "statement": "What is the purpose of using a transparent mask in the proposed structured light system?",
            "options": [
                "To store per-point BRDF parameters",
                "To accurately encode spatial information",
                "To project a set of learned mask patterns",
                "To efficiently probe angularly-varying reflectance"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "OvarNet: Towards Open-vocabulary Object Attribute Recognition\nKeyan Chen1⋆, Xiaolong Jiang2⋆, Yao Hu2, Xu Tang2, Yan Gao2, Jianqi Chen1, Weidi Xie3,4†\nBeihang University1,\nXiaohongshu Inc2,\nCMIC, Shanghai Jiao Tong University3,\nShanghai AI Laboratory4\nhttps://kyanchen.github.io/OvarNet\ncategory: \numbrella\npos attribute\ncurtainle\ncolorful\nleather\nblue\ncurled\nneg attribute\ncartoon\nburnt\nunlit\nteddy bear\nsports ball\nperson\ntennis racket\nperson\nskis\nObject Detection\nperson\nOpen-Vocabulary OAR\ncategory: \nperson\npos attribute: \ndilapidated\nrelaxing\nthin\nilluminated\nscruffy\nneg attribute: \nwearing red\ncloth\nreading\ncategory: \nsink\npos attribute: \nporcelain\nslightly open\nnew\nin the air\nwhite\nneg attribute: \ndirt\nbaby\ncrossed\ncategory: \nfrisbee\npos attribute: \ntaking photo\npurple\nwhite\nround\nplastic\nneg attribute: \ncurved\nstyrofoam\nhardwood\ncategory: \ncow\npos attribute: \nbrown\nhorned\nmulticolored\nwhite\neating\nneg attribute: \nhardwood\nneon\ntexting\ncategory: \ntie\npos attribute: \ndark\ncloth   \nmulticolored\nstriped \nblue\nneg attribute: \nposing\nthick\nlooking at camera\nObject Det. & Attribute Cls. \ncategory: \ncow\npos attribute: \nold\nresting\nfoggy\nleaf covered\ntree-covered\nneg attribute:\nwearing gray\nlooking up\nseen painted\nsports ball\ntennis racket\nteddy bear\nperson\nskis\nfurry,  fluffy, soft, tame\nperson\nsports ball\ntennis racket\nteddy bear\nperson\nskis\nperson\nsports ball\ntennis racket\nteddy bear\nfurry, hairy, fluffy, soft\nwalking, adult, striped\nmoving, metal, functional\nstanding, watching, holding, female\ncat\nperson\nsports ball\ntennis racket\nteddy bear\nperson\nskis\ncat\nFigure 1. The first row depicts the tasks of object detection and attribute classification in a close-set setting, i.e., train and test on the same\nvocabulary set. The second row gives qualitative results from our proposed OvarNet, which simultaneously localizes, categorizes, and\ncharacterizes arbitrary objects in an open-vocabulary scenario. We only show one object per image for ease of visualization, red denotes\nthe base category/attribute i.e., seen in the training set, while blue represents the novel category/attribute unseen in the training set.\nAbstract\nIn this paper, we consider the problem of simultaneously\ndetecting objects and inferring their visual attributes in an\nimage, even for those with no manual annotations provided\nat the training stage, resembling an open-vocabulary sce-\nnario. To achieve this goal, we make the following con-\ntributions: (i) we start with a na¨\nıve two-stage approach\nfor open-vocabulary object detection and attribute classi-\nfication, termed CLIP-Attr. The candidate objects are first\nproposed with an offline RPN and later classified for se-\nmantic category and attributes; (ii) we combine all avail-\nable datasets and train with a federated strategy to fine-\ntune the CLIP model, aligning the visual representation\nwith attributes, additionally, we investigate the efficacy of\nleveraging freely available online image-caption pairs un-\nder weakly supervised learning; (iii) in pursuit of efficiency,\nwe train a Faster-RCNN type model end-to-end with knowl-\nedge distillation, that performs class-agnostic object pro-\nposals and classification on semantic categories and at-\ntributes with classifiers generated from a text encoder; Fi-\nnally, (iv) we conduct extensive experiments on VAW, MS-\nCOCO, LSA, and OVAD datasets, and show that recog-\n⋆Equal contribution. † Corresponding author.\nnition of semantic category and attributes is complemen-\ntary for visual scene understanding, i.e., jointly training\nobject detection and attributes prediction largely outper-\nform existing approaches that treat the two tasks indepen-\ndently, demonstrating strong generalization ability to novel\nattributes and categories.\n",
        "question": {
            "statement": "What is the main advantage of jointly training object detection and attribute prediction models?",
            "options": [
                "Improved generalization to novel attributes and categories",
                "Increased accuracy for specific object categories",
                "Faster inference time",
                "Reduced requirement for annotated training data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "The Resource Problem of Using Linear Layer Leakage Attack in Federated\nLearning\nJoshua C. Zhao1, Ahmed Roushdy Elkordy2, Atul Sharma1, Yahya H. Ezzeldin2\nSalman Avestimehr2, Saurabh Bagchi1\n1Purdue University\n2University of Southern California\n{zhao1207,sharm438,sbagchi}@purdue.edu, {aelkordy,yessa,avestime}@usc.edu\nAbstract\nSecure aggregation promises a heightened level of pri-\nvacy in federated learning, maintaining that a server only\nhas access to a decrypted aggregate update. Within this\nsetting, linear layer leakage methods are the only data re-\nconstruction attacks able to scale and achieve a high leak-\nage rate regardless of the number of clients or batch size.\nThis is done through increasing the size of an injected fully-\nconnected (FC) layer. However, this results in a resource\noverhead which grows larger with an increasing number of\nclients. We show that this resource overhead is caused by\nan incorrect perspective in all prior work that treats an at-\ntack on an aggregate update in the same way as an individ-\nual update with a larger batch size. Instead, by attacking\nthe update from the perspective that aggregation is combin-\ning multiple individual updates, this allows the application\nof sparsity to alleviate resource overhead. We show that\nthe use of sparsity can decrease the model size overhead by\nover 327× and the computation time by 3.34× compared to\nSOTA while maintaining equivalent total leakage rate, 77%\neven with 1000 clients in aggregation.\n",
        "question": {
            "statement": "What is a major limitation of using linear layer leakage attack in federated learning?",
            "options": [
                "It is incompatible with sparse models",
                "It requires direct access to individual client updates",
                "It is unable to achieve a high leakage rate",
                "It causes significant resource overhead that increases with the number of clients"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Where We Are and What We’re Looking At: Query Based Worldwide Image\nGeo-localization Using Hierarchies and Scenes\nBrandon Clark*\n, Alec Kerrigan∗, Parth Parag Kulkarni, Vicente Vivanco Cepeda, Mubarak Shah\nCenter for Research in Computer Vision, University of Central Florida, Orlando, USA\n{brandonclark314, aleckerrigan, parthpkulkarni.pk, vicente.vivanco}@knights.ucf.edu,\nshah@crcv.ucf.edu\nAbstract\nDetermining the exact latitude and longitude that a\nphoto was taken is a useful and widely applicable task,\nyet it remains exceptionally difficult despite the acceler-\nated progress of other computer vision tasks. Most previ-\nous approaches have opted to learn single representations\nof query images, which are then classified at different lev-\nels of geographic granularity.\nThese approaches fail to\nexploit the different visual cues that give context to differ-\nent hierarchies, such as the country, state, and city level.\nTo this end, we introduce an end-to-end transformer-based\narchitecture that exploits the relationship between differ-\nent geographic levels (which we refer to as hierarchies)\nand the corresponding visual scene information in an im-\nage through hierarchical cross-attention. We achieve this by\nlearning a query for each geographic hierarchy and scene\ntype. Furthermore, we learn a separate representation for\ndifferent environmental scenes, as different scenes in the\nsame location are often defined by completely different vi-\nsual features. We achieve state of the art accuracy on 4\nstandard geo-localization datasets : Im2GPS, Im2GPS3k,\nYFCC4k, and YFCC26k, as well as qualitatively demon-\nstrate how our method learns different representations for\ndifferent visual hierarchies and scenes, which has not been\ndemonstrated in the previous methods. Above previous test-\ning datasets mostly consist of iconic landmarks or images\ntaken from social media, which makes the dataset a sim-\nple memory task, or makes it biased towards certain places.\nTo address this issue we introduce a much harder testing\ndataset, Google-World-Streets-15k, comprised of images\ntaken from Google Streetview covering the whole planet and\npresent state of the art results. Our code can be found at\nhttps://github.com/AHKerrigan/GeoGuessNet.\n*These authors contributed equally to the work\n",
        "question": {
            "statement": "What approach do most previous methods take when determining the geographic location of an image?",
            "options": [
                "Analyzing the EXIF data of the image",
                "Classifying the image based on object detection",
                "Learning a single representation of the query image",
                "Using GPS coordinates embedded in the image"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Regularized Vector Quantization for Tokenized Image Synthesis\nJiahui Zhang1\nFangneng Zhan2\nChristian Theobalt2\nShijian Lu*1\n1 Nanyang Technological University\n2 Max Planck Institute for Informatics\nAbstract\nQuantizing images into discrete representations has been\na fundamental problem in unified generative modeling.\nPredominant approaches learn the discrete representation\neither in a deterministic manner by selecting the best-\nmatching token or in a stochastic manner by sampling from\na predicted distribution. However, deterministic quantiza-\ntion suffers from severe codebook collapse and misalign-\nment with inference stage while stochastic quantization suf-\nfers from low codebook utilization and perturbed recon-\nstruction objective. This paper presents a regularized vec-\ntor quantization framework that allows to mitigate above\nissues effectively by applying regularization from two per-\nspectives.\nThe first is a prior distribution regularization\nwhich measures the discrepancy between a prior token dis-\ntribution and the predicted token distribution to avoid code-\nbook collapse and low codebook utilization. The second is\na stochastic mask regularization that introduces stochastic-\nity during quantization to strike a good balance between in-\nference stage misalignment and unperturbed reconstruction\nobjective. In addition, we design a probabilistic contrastive\nloss which serves as a calibrated metric to further miti-\ngate the perturbed reconstruction objective. Extensive ex-\nperiments show that the proposed quantization framework\noutperforms prevailing vector quantization methods con-\nsistently across different generative models including auto-\nregressive models and diffusion models.\n",
        "question": {
            "statement": "What is a major limitation of deterministic quantization in image synthesis?",
            "options": [
                "Codebook collapse and misalignment with inference stage",
                "High computational cost and memory requirements",
                "Insufficient training data and model complexity",
                "Low codebook utilization and perturbed reconstruction objective"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Make-A-Story: Visual Memory Conditioned Consistent Story Generation\nTanzila Rahman1,3\nHsin-Ying Lee2\nJian Ren2\nSergey Tulyakov2\nShweta Mahajan1,3\nLeonid Sigal1,3,4\n1University of British Columbia\n2Snap Inc.\n3Vector Institute for AI\n4Canada CIFAR AI Chair\nFred and Barney are\nlaughing at a robot in the\nquarry.\nFred is running while\npushing Barney through a\nroom.\nThey lean against a\ndoorway breathing heavy.\nThey are leaning against a\ndoorway.  Barney talks to\nFred.  They both look\nscared. \nLisa runs from lane left to\nright and jump to collect\ncoin and kill for snail and\nagain jump but bee killing\nfor me in Dirt.\nShe walks right on a ledge,\ncollects two coins and turns\naround and walks off a\nledge. It drops to a lower\nledge and is killed by a gear.\nShe runs left from right and\njumps over the gear to\ncollect the coin.\nShe jumps over a frog and\ngoes left. It gets a couple of\ncoins.\nFred and Barney are\nlaughing at a robot in the\nquarry.\nFred is running while\npushing Barney through a\nroom.\nFred and Barney lean\nagainst a doorway breathing\nheavy.\nFred and Barney are\nleaning against a doorway. \nBarney talks to Fred.  They\nboth look scared. \nLisa runs from lane left to\nright and jump to collect\ncoin and kill for snail and\nagain jump but bee killing\nfor me in Dirt.\nLisa walks right on a ledge,\ncollects two coins and turns\naround and walks off a\nledge. It drops to a lower\nledge and is killed by a gear.\nLisa runs left from right and\njumps over the gear to\ncollect the coin.\nShe jumps over a frog and\ngoes left. It gets a couple of\ncoins.\nText with\nreferences\nText without\nreferences\nFigure 1. Referential and consistent story visualization. Examples of more natural stories with references for the FlintstonesSV [16]\nand MUGEN [19] datasets (bottom text) compared to more typical but less natural story text (top). We extend the MUGEN dataset by\nintroducing additional two characters (e.g. Lisa and Jhon) and four backgrounds (e.g. Sand, Grass, Stone and Dirt).\nAbstract\nThere has been a recent explosion of impressive gen-\nerative models that can produce high quality images (or\nvideos) conditioned on text descriptions. However, all such\napproaches rely on conditional sentences that contain un-\nambiguous descriptions of scenes and main actors in them.\nTherefore employing such models for more complex task\nof story visualization, where naturally references and co-\nreferences exist, and one requires to reason about when\nto maintain consistency of actors and backgrounds across\nframes/scenes, and when not to, based on story progression,\nremains a challenge. In this work, we address the afore-\nmentioned challenges and propose a novel autoregressive\ndiffusion-based framework with a visual memory module\nthat implicitly captures the actor and background context\nacross the generated frames. Sentence-conditioned soft at-\ntention over the memories enables effective reference reso-\nlution and learns to maintain scene and actor consistency\nwhen needed. To validate the effectiveness of our approach,\nwe extend the MUGEN dataset [19] and introduce addi-\ntional characters, backgrounds and referencing in multi-\nsentence storylines. Our experiments for story generation\non the MUGEN, the PororoSV [30] and the FlintstonesSV\n[16] dataset show that our method not only outperforms\nprior state-of-the-art in generating frames with high visual\nquality, which are consistent with the story, but also models\nappropriate correspondences between the characters and\nthe background.\n",
        "question": {
            "statement": "What is a key challenge in using generative models for story visualization?",
            "options": [
                "producing conditional sentences",
                "resolving sentence ambiguity",
                "generating high-quality images",
                "maintaining consistency of actors and backgrounds across frames"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "2",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Image Super-Resolution Using T-Tetromino Pixels\nSimon Grosche, Andy Regensky, J¨\nurgen Seiler, and Andr´\ne Kaup\nChair of Multimedia Communications and Signal Processing,\nFriedrich-Alexander-Univerist¨\nat Erlangen-N¨\nurnberg,\nCauerstr. 7, 91058 Erlangen, Germany\nsimon.grosche@fau.de, andy.regensky@fau.de, juergen.seiler@fau.de, andre.kaup@fau.de\nAbstract\nFor modern high-resolution imaging sensors, pixel bin-\nning is performed in low-lighting conditions and in case\nhigh frame rates are required. To recover the original spa-\ntial resolution, single-image super-resolution techniques\ncan be applied for upscaling. To achieve a higher image\nquality after upscaling, we propose a novel binning con-\ncept using tetromino-shaped pixels.\nIt is embedded into\nthe field of compressed sensing and the coherence is cal-\nculated to motivate the sensor layouts used. Next, we inves-\ntigate the reconstruction quality using tetromino pixels for\nthe first time in literature. Instead of using different types\nof tetrominoes as proposed elsewhere, we show that using\na small repeating cell consisting of only four T-tetrominoes\nis sufficient. For reconstruction, we use a locally fully con-\nnected reconstruction (LFCR) network as well as two clas-\nsical reconstruction methods from the field of compressed\nsensing. Using the LFCR network in combination with the\nproposed tetromino layout, we achieve superior image qual-\nity in terms of PSNR, SSIM, and visually compared to con-\nventional single-image super-resolution using the very deep\nsuper-resolution (VDSR) network. For PSNR, a gain of up\nto +1.92 dB is achieved.\n",
        "question": {
            "statement": "What is the primary purpose of applying single-image super-resolution techniques to images captured with pixel binning?",
            "options": [
                "To reduce noise in low-light conditions",
                "To increase the frame rate",
                "To improve color accuracy",
                "To recover the original spatial resolution"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Multimodality Helps Unimodality:\nCross-Modal Few-Shot Learning with Multimodal Models\nZhiqiu Lin*\nSamuel Yu*\nZhiyi Kuang\nDeepak Pathak\nDeva Ramanan\nCarnegie Mellon University\n{zhiqiul,samuelyu,zkuang,dpathak,deva}@cs.cmu.edu\nAbstract\nThe ability to quickly learn a new task with minimal in-\nstruction – known as few-shot learning – is a central as-\npect of intelligent agents. Classical few-shot benchmarks\nmake use of few-shot samples from a single modality, but\nsuch samples may not be sufﬁcient to characterize an entire\nconcept class. In contrast, humans use cross-modal infor-\nmation to learn new concepts efﬁciently. In this work, we\ndemonstrate that one can indeed build a better visual dog\nclassiﬁer by reading about dogs and listening to them bark.\nTo do so, we exploit the fact that recent multimodal founda-\ntion models such as CLIP are inherently cross-modal, map-\nping different modalities to the same representation space.\nSpeciﬁcally, we propose a simple cross-modal adaptation\napproach that learns from few-shot examples spanning dif-\nferent modalities. By repurposing class names as additional\none-shot training samples, we achieve SOTA results with an\nembarrassingly simple linear classiﬁer for vision-language\nadaptation. Furthermore, we show that our approach can\nbeneﬁt existing methods such as preﬁx tuning, adapters, and\nclassiﬁer ensembling. Finally, to explore other modalities\nbeyond vision and language, we construct the ﬁrst (to our\nknowledge) audiovisual few-shot benchmark and use cross-\nmodal training to improve the performance of both image\nand audio classiﬁcation. Project site at link.\n",
        "question": {
            "statement": "What advantage do multimodal foundation models offer for few-shot learning?",
            "options": [
                "They enable mapping of different modalities to the same representation space",
                "They are limited to a single modality",
                "They require large amounts of labeled data",
                "They rely on manual feature engineering"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Detecting Backdoors in Pre-trained Encoders\nShiwei Feng, Guanhong Tao, Siyuan Cheng, Guangyu Shen,\nXiangzhe Xu, Yingqi Liu, Kaiyuan Zhang, Shiqing Ma†, Xiangyu Zhang\nPurdue University, †Rutgers University\n{feng292, taog, cheng535, shen447, xu1415, liu1751, zhan4057, xyzhang}@cs.purdue.edu\n†sm2283@cs.rutgers.edu\nAbstract\nSelf-supervised learning in computer vision trains on\nunlabeled data, such as images or (image, text) pairs, to\nobtain an image encoder that learns high-quality embed-\ndings for input data. Emerging backdoor attacks towards\nencoders expose crucial vulnerabilities of self-supervised\nlearning, since downstream classifiers (even further trained\non clean data) may inherit backdoor behaviors from en-\ncoders. Existing backdoor detection methods mainly fo-\ncus on supervised learning settings and cannot handle pre-\ntrained encoders especially when input labels are not avail-\nable. In this paper, we propose DECREE, the first back-\ndoor detection approach for pre-trained encoders, requir-\ning neither classifier headers nor input labels. We eval-\nuate DECREE on over 400 encoders trojaned under 3\nparadigms. We show the effectiveness of our method on im-\nage encoders pre-trained on ImageNet and OpenAI’s CLIP\n400 million image-text pairs. Our method consistently has\na high detection accuracy even if we have only limited or\nno access to the pre-training dataset. Code is available at\nhttps://github.com/GiantSeaweed/DECREE.\n",
        "question": {
            "statement": "What is a major concern in self-supervised learning in computer vision?",
            "options": [
                "Overfitting",
                "Biased datasets",
                "Underfitting",
                "Backdoor attacks"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art\nfor real-time object detectors\nChien-Yao Wang1, Alexey Bochkovskiy, and Hong-Yuan Mark Liao1\n1Institute of Information Science, Academia Sinica, Taiwan\nkinyiu@iis.sinica.edu.tw, alexeyab84@gmail.com, and liao@iis.sinica.edu.tw\nAbstract\nReal-time object detection is one of the most important\nresearch topics in computer vision. As new approaches re-\ngarding architecture optimization and training optimization\nare continually being developed, we have found two re-\nsearch topics that have spawned when dealing with these\nlatest state-of-the-art methods. To address the topics, we\npropose a trainable bag-of-freebies oriented solution. We\ncombine the ﬂexible and efﬁcient training tools with the\nproposed architecture and the compound scaling method.\nYOLOv7 surpasses all known object detectors in both speed\nand accuracy in the range from 5 FPS to 120 FPS and\nhas the highest accuracy 56.8% AP among all known real-\ntime object detectors with 30 FPS or higher on GPU V100.\nSource code is released in https://github.com/\nWongKinYiu/yolov7.\n",
        "question": {
            "statement": "What is the primary goal of real-time object detection research in computer vision?",
            "options": [
                "To achieve high accuracy and speed simultaneously",
                "To develop detectors that only work on specific hardware",
                "To focus solely on improving accuracy regardless of speed",
                "To prioritize speed over accuracy"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Unifying Layout Generation with a Decoupled Diffusion Model\nMude Hui1*\nZhizheng Zhang2\nXiaoyi Zhang2\nWenxuan Xie2\nYuwang Wang3\nYan Lu2\n1Xi’an Jiaotong University\n2Microsoft Research Asia\n3Tsinghua University\n{zhizzhang, xiaoyizhang, wenxie, yanlu}@microsoft.com\ntheflood@stu.xjtu.edu.cn\nwang-yuwang@mail.tsinghua.edu.cn\nAbstract\nLayout generation aims to synthesize realistic graphic\nscenes consisting of elements with different attributes in-\ncluding category, size, position, and between-element rela-\ntion. It is a crucial task for reducing the burden on heavy-\nduty graphic design works for formatted scenes, e.g., publi-\ncations, documents, and user interfaces (UIs). Diverse ap-\nplication scenarios impose a big challenge in unifying var-\nious layout generation subtasks, including conditional and\nunconditional generation. In this paper, we propose a Lay-\nout Diffusion Generative Model (LDGM) to achieve such\nuniﬁcation with a single decoupled diffusion model. LDGM\nviews a layout of arbitrary missing or coarse element at-\ntributes as an intermediate diffusion status from a com-\npleted layout. Since different attributes have their individ-\nual semantics and characteristics, we propose to decouple\nthe diffusion processes for them to improve the diversity of\ntraining samples and learn the reverse process jointly to ex-\nploit global-scope contexts for facilitating generation. As a\nresult, our LDGM can generate layouts either from scratch\nor conditional on arbitrary available attributes.\nExten-\nsive qualitative and quantitative experiments demonstrate\nour proposed LDGM outperforms existing layout genera-\ntion models in both functionality and performance.\n",
        "question": {
            "statement": "What is the main advantage of using a decoupled diffusion model in layout generation?",
            "options": [
                "It requires less training data than other models.",
                "It reduces the computational complexity of the model.",
                "It enables the generation of layouts with a fixed number of elements.",
                "It allows for the learning of individual attribute semantics and characteristics."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "2",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "ReCo: Region-Controlled Text-to-Image Generation\nZhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin,\nChenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang\nMicrosoft\n{zhengyang,jianfw,zhe.gan,lindsey.li,keli,chewu,nanduan,zliu,ce.liu,nzeng,lijuanw}@microsoft.com\n… … …\nFigure 1. (a) ReCo extends pre-trained text-to-image models (Stable Diffusion [34]) with an extra set of input position tokens (in dark\nblue color) that represent quantized spatial coordinates. Combining position and text tokens yields the region-controlled text input, which\ncan specify an open-ended regional description precisely for any image region. (b) With the region-controlled text input, ReCo can better\ncontrol the object count/relationship/size properties and improve the T2I semantic correctness. We empirically observe that position tokens\nare less likely to get overlooked than positional text words, especially when the input query is complicated or describes an unusual scene.\nAbstract\nRecently, large-scale text-to-image (T2I) models have\nshown impressive performance in generating high-fidelity\nimages, but with limited controllability, e.g., precisely spec-\nifying the content in a specific region with a free-form text\ndescription. In this paper, we propose an effective technique\nfor such regional control in T2I generation. We augment T2I\nmodels’ inputs with an extra set of position tokens, which\nrepresent the quantized spatial coordinates. Each region\nis specified by four position tokens to represent the top-left\nand bottom-right corners, followed by an open-ended nat-\nural language regional description. Then, we fine-tune a\npre-trained T2I model with such new input interface. Our\nmodel, dubbed as ReCo (Region-Controlled T2I), enables\nthe region control for arbitrary objects described by open-\nended regional texts rather than by object labels from a\nconstrained category set. Empirically, ReCo achieves bet-\nter image quality than the T2I model strengthened by posi-\ntional words (FID: 8.82 →7.36, SceneFID: 15.54 →6.51\non COCO), together with objects being more accurately\nplaced, amounting to a 20.40% region classification accu-\nracy improvement on COCO. Furthermore, we demonstrate\nthat ReCo can better control the object count, spatial re-\nlationship, and region attributes such as color/size, with\nthe free-form regional description. Human evaluation on\nPaintSkill shows that ReCo is +19.28% and +17.21% more\naccurate in generating images with correct object count and\nspatial relationship than the T2I model. Code is available\nat https://github.com/microsoft/ReCo.\n",
        "question": {
            "statement": "What is the primary advantage of using position tokens in text-to-image models?",
            "options": [
                "They improve the overall fidelity of generated images",
                "They reduce the complexity of natural language processing",
                "They allow for more precise specification of regional descriptions",
                "They enable the use of object labels from a constrained category set"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame\nNon-linear Interpolation\nZehan Zheng∗, Danni Wu∗, Ruisi Lu, Fan Lu, Guang Chen†, Changjun Jiang\nTongji University\n{zhengzehan, woodannie, lrs910, lufan, guangchen, cjjiang}@tongji.edu.cn\nAbstract\nIn recent years, there has been a significant increase in\nfocus on the interpolation task of computer vision. Despite\nthe tremendous advancement of video interpolation, point\ncloud interpolation remains insufficiently explored. Mean-\nwhile, the existence of numerous nonlinear large motions\nin real-world scenarios makes the point cloud interpolation\ntask more challenging. In light of these issues, we present\nNeuralPCI: an end-to-end 4D spatio-temporal Neural field\nfor 3D Point Cloud Interpolation, which implicitly inte-\ngrates multi-frame information to handle nonlinear large\nmotions for both indoor and outdoor scenarios. Further-\nmore, we construct a new multi-frame point cloud interpo-\nlation dataset called NL-Drive for large nonlinear motions\nin autonomous driving scenes to better demonstrate the su-\nperiority of our method. Ultimately, NeuralPCI achieves\nstate-of-the-art performance on both DHB (Dynamic Hu-\nman Bodies) and NL-Drive datasets. Beyond the interpola-\ntion task, our method can be naturally extended to point\ncloud extrapolation, morphing, and auto-labeling, which\nindicates its substantial potential in other domains. Codes\nare available at https://github.com/ispc-lab/NeuralPCI.\n",
        "question": {
            "statement": "What type of data does the NeuralPCI method primarily operate on?",
            "options": [
                "audio signals",
                "3D point clouds",
                "image sequences",
                "video frames"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Local-to-Global Registration for Bundle-Adjusting Neural Radiance Fields\nYue Chen1,2*\nXingyu Chen1,2*\nXuan Wang3†\nQi Zhang4\nYu Guo1,2†\nYing Shan4\nFei Wang1,2\n1 National Key Laboratory of Human-Machine Hybrid Augmented Intelligence\n2 IAIR, Xi’an Jiaotong University\n3Ant Group\n4Tencent AI Lab\nAbstract\nNeural Radiance Fields (NeRF) have achieved photo-\nrealistic novel views synthesis; however, the requirement\nof accurate camera poses limits its application. Despite\nanalysis-by-synthesis extensions for jointly learning neu-\nral 3D representations and registering camera frames exist,\nthey are susceptible to suboptimal solutions if poorly initial-\nized. We propose L2G-NeRF, a Local-to-Global registra-\ntion method for bundle-adjusting Neural Radiance Fields:\nﬁrst, a pixel-wise ﬂexible alignment, followed by a frame-\nwise constrained parametric alignment. Pixel-wise local\nalignment is learned in an unsupervised way via a deep\nnetwork which optimizes photometric reconstruction errors.\nFrame-wise global alignment is performed using differen-\ntiable parameter estimation solvers on the pixel-wise corre-\nspondences to ﬁnd a global transformation. Experiments on\nsynthetic and real-world data show that our method outper-\nforms the current state-of-the-art in terms of high-ﬁdelity\nreconstruction and resolving large camera pose misalign-\nment.\nOur module is an easy-to-use plugin that can be\napplied to NeRF variants and other neural ﬁeld applica-\ntions. The Code and supplementary materials are available\nat https://rover-xingyu.github.io/L2G-NeRF/.\n",
        "question": {
            "statement": "What is a common limitation of Neural Radiance Fields (NeRF) that can be addressed through registration methods?",
            "options": [
                "The need for large amounts of training data",
                "The requirement of accurate camera poses",
                "The inability to handle complex lighting conditions",
                "The lack of flexibility in rendering novel views"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "IMAGEBIND: One Embedding Space To Bind Them All\nRohit Girdhar∗\nAlaaeldin El-Nouby∗\nZhuang Liu\nMannat Singh\nKalyan Vasudev Alwala\nArmand Joulin\nIshan Misra∗\nFAIR, Meta AI\nhttps://facebookresearch.github.io/ImageBind\n1) Cross-Modal Retrieval\nCrackle of a Fire\nAudio\nText\n“A fire crackles while a pan of food is frying on \nthe fire.”\n“Fire is crackling then wind starts blowing.”\n“Firewood crackles then music...”\n“A baby is crying while a toddler is laughing.”\n“A baby is laughing while an adult is laughing.”\n“A baby laughs and something…”\nBaby Cooing\nWaves\n2) Embedding-Space Arithmetic\n3) Audio to Image Generation\nDog\nEngine\nFire\nImages\nVideos\nDepth\n&\nRain\nFigure 1. IMAGEBIND’s joint embedding space enables novel multimodal capabilities. By aligning six modalities’ embedding into a\ncommon space, IMAGEBIND enables: 1) Cross-Modal Retrieval, which shows emergent alignment of modalities such as audio, depth or\ntext, that aren’t observed together. 2) Adding embeddings from different modalities naturally composes their semantics. And 3) Audio-to-\nImage generation, by using our audio embeddings with a pre-trained DALLE-2 [60] decoder designed to work with CLIP text embeddings.\nAbstract\nWe present IMAGEBIND, an approach to learn a joint\nembedding across six different modalities - images, text, au-\ndio, depth, thermal, and IMU data. We show that all combi-\nnations of paired data are not necessary to train such a joint\nembedding, and only image-paired data is sufficient to bind\nthe modalities together. IMAGEBIND can leverage recent\nlarge scale vision-language models, and extends their zero-\nshot capabilities to new modalities just by using their natu-\nral pairing with images. It enables novel emergent applica-\ntions ‘out-of-the-box’ including cross-modal retrieval, com-\nposing modalities with arithmetic, cross-modal detection\nand generation. The emergent capabilities improve with the\nstrength of the image encoder and we set a new state-of-the-\nart on emergent zero-shot recognition tasks across modal-\nities, outperforming specialist supervised models. Finally,\nwe show strong few-shot recognition results outperforming\nprior work, and that IMAGEBIND serves as a new way to\nevaluate vision models for visual and non-visual tasks.\n∗Equal technical contribution.\n",
        "question": {
            "statement": "What is the main advantage of the IMAGEBIND approach in learning a joint embedding across multiple modalities?",
            "options": [
                "It uses a separate encoder for each modality",
                "It does not require paired data for all combinations of modalities",
                "It requires large amounts of labeled data",
                "It is limited to only two modalities"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "TarViS: A Unified Approach for Target-based Video Segmentation\nAli Athar1\nAlexander Hermans1\nJonathon Luiten1,2\nDeva Ramanan2\nBastian Leibe1\n1RWTH Aachen University, Germany\n2Carnegie Mellon University, USA\n{athar,hermans,luiten,leibe}@vision.rwth-aachen.de\ndeva@cs.cmu.edu\nAbstract\nThe general domain of video segmentation is currently\nfragmented into different tasks spanning multiple bench-\nmarks. Despite rapid progress in the state-of-the-art, cur-\nrent methods are overwhelmingly task-specific and cannot\nconceptually generalize to other tasks. Inspired by recent\napproaches with multi-task capability, we propose TarViS:\na novel, unified network architecture that can be applied to\nany task that requires segmenting a set of arbitrarily de-\nfined ‘targets’ in video. Our approach is flexible with re-\nspect to how tasks define these targets, since it models the\nlatter as abstract ‘queries’ which are then used to predict\npixel-precise target masks. A single TarViS model can be\ntrained jointly on a collection of datasets spanning differ-\nent tasks, and can hot-swap between tasks during infer-\nence without any task-specific retraining. To demonstrate\nits effectiveness, we apply TarViS to four different tasks,\nnamely Video Instance Segmentation (VIS), Video Panoptic\nSegmentation (VPS), Video Object Segmentation (VOS) and\nPoint Exemplar-guided Tracking (PET). Our unified, jointly\ntrained model achieves state-of-the-art performance on 5/7\nbenchmarks spanning these four tasks, and competitive per-\nformance on the remaining two. Code and model weights\nare available at: https://github.com/Ali2500/TarViS\n",
        "question": {
            "statement": "What is a key advantage of the TarViS approach in video segmentation?",
            "options": [
                "It can be trained jointly on multiple datasets and adapt to new tasks without retraining",
                "It is limited to segmenting objects of a specific class",
                "It is specifically designed for a single video segmentation task",
                "It requires manual annotation of targets in each video frame"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Multimodal Industrial Anomaly Detection via Hybrid Fusion\nYue Wang1*\n, Jinlong Peng2*\n, Jiangning Zhang2, Ran Yi1†\n, Yabiao Wang2, Chengjie Wang2,1\n1Shanghai Jiao Tong University, Shanghai, China; 2Youtu Lab, Tencent\n1{imwangyue,ranyi}@sjtu.edu.cn 2{jeromepeng,vtzhang,caseywang,jasoncjwang}@tencent.com\nAbstract\n2D-based Industrial Anomaly Detection has been widely\ndiscussed, however, multimodal industrial anomaly detec-\ntion based on 3D point clouds and RGB images still has\nmany untouched fields.\nExisting multimodal industrial\nanomaly detection methods directly concatenate the mul-\ntimodal features, which leads to a strong disturbance be-\ntween features and harms the detection performance. In this\npaper, we propose Multi-3D-Memory (M3DM), a novel\nmultimodal anomaly detection method with hybrid fusion\nscheme: firstly, we design an unsupervised feature fusion\nwith patch-wise contrastive learning to encourage the in-\nteraction of different modal features; secondly, we use a\ndecision layer fusion with multiple memory banks to avoid\nloss of information and additional novelty classifiers to\nmake the final decision. We further propose a point fea-\nture alignment operation to better align the point cloud and\nRGB features. Extensive experiments show that our multi-\nmodal industrial anomaly detection model outperforms the\nstate-of-the-art (SOTA) methods on both detection and seg-\nmentation precision on MVTec-3D AD dataset.\nCode at\ngithub.com/nomewang/M3DM.\n",
        "question": {
            "statement": "What is a limitation of existing multimodal industrial anomaly detection methods?",
            "options": [
                "Direct concatenation of multimodal features",
                "Inability to handle high-dimensional data",
                "Insufficient computational resources",
                "Lack of training data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Constrained Evolutionary Diffusion Filter for Monocular Endoscope Tracking\nXiongbiao Luo*\nDepartment of Computer Science and Technology, Xiamen University, Xiamen 361005, China\nNational Institute for Data Science in Health and Medicine, Xiamen University, Xiamen 361102, China\nxiongbiao.luo@gmail.com\nAbstract\nStochastic filtering is widely used to deal with nonlin-\near optimization problems such as 3-D and visual track-\ning in various computer vision and augmented reality ap-\nplications.\nMany current methods suffer from an imbal-\nance between exploration and exploitation due to their par-\nticle degeneracy and impoverishment, resulting in local op-\ntimums. To address this imbalance, this work proposes a\nnew constrained evolutionary diffusion filter for nonlinear\noptimization. Specifically, this filter develops spatial state\nconstraints and adaptive history-recall differential evolu-\ntion embedded evolutionary stochastic diffusion instead of\nsequential resampling to resolve the degeneracy and im-\npoverishment problem. With application to monocular en-\ndoscope 3-D tracking, the experimental results show that\nthe proposed filtering significantly improves the balance be-\ntween exploration and exploitation and certainly works bet-\nter than recent 3-D tracking methods. Particularly, the sur-\ngical tracking error was reduced from 4.03 mm to 2.59 mm.\n",
        "question": {
            "statement": "What is a common limitation of many stochastic filtering methods used in computer vision and augmented reality applications?",
            "options": [
                "Particle degeneracy and impoverishment leading to local optima",
                "Inability to handle high-dimensional data",
                "Lack of robustness to noise and outliers",
                "Insufficient computational resources"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Hidden Gems: 4D Radar Scene Flow Learning Using\nCross-Modal Supervision\nFangqiang Ding1\nAndras Palffy2\nDariu M. Gavrila2\nChris Xiaoxuan Lu1,*\n1University of Edinburgh\n{fding, xiaoxuan.lu}@ed.ac.uk\n2Delft University of Technology\n{a.palffy, d.m.gavrila}@tudelft.nl\nAbstract\nThis work proposes a novel approach to 4D radar-based\nscene flow estimation via cross-modal learning. Our ap-\nproach is motivated by the co-located sensing redundancy\nin modern autonomous vehicles. Such redundancy implic-\nitly provides various forms of supervision cues to the radar\nscene flow estimation. Specifically, we introduce a multi-\ntask model architecture for the identified cross-modal learn-\ning problem and propose loss functions to opportunistically\nengage scene flow estimation using multiple cross-modal\nconstraints for effective model training. Extensive experi-\nments show the state-of-the-art performance of our method\nand demonstrate the effectiveness of cross-modal super-\nvised learning to infer more accurate 4D radar scene flow.\nWe also show its usefulness to two subtasks - motion seg-\nmentation and ego-motion estimation. Our source code will\nbe available on https://github.com/Toytiny/CMFlow.\n",
        "question": {
            "statement": "What type of learning does the proposed approach utilize to estimate 4D radar scene flow?",
            "options": [
                "unsupervised learning",
                "reinforcement learning",
                "cross-modal supervised learning",
                "self-supervised learning"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": false,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Regularizing Second-Order Influences for Continual Learning\nZhicheng Sun1, Yadong Mu1,2*, Gang Hua3\n1Peking University, 2Peng Cheng Laboratory, 3Wormpex AI Research\n{sunzc,myd}@pku.edu.cn, ganghua@gmail.com\nAbstract\nContinual learning aims to learn on non-stationary data\nstreams without catastrophically forgetting previous knowl-\nedge. Prevalent replay-based methods address this chal-\nlenge by rehearsing on a small buffer holding the seen data,\nfor which a delicate sample selection strategy is required.\nHowever, existing selection schemes typically seek only to\nmaximize the utility of the ongoing selection, overlooking\nthe interference between successive rounds of selection.\nMotivated by this, we dissect the interaction of sequential\nselection steps within a framework built on influence func-\ntions. We manage to identify a new class of second-order\ninfluences that will gradually amplify incidental bias in the\nreplay buffer and compromise the selection process. To reg-\nularize the second-order effects, a novel selection objective\nis proposed, which also has clear connections to two widely\nadopted criteria. Furthermore, we present an efficient im-\nplementation for optimizing the proposed criterion. Exper-\niments on multiple continual learning benchmarks demon-\nstrate the advantage of our approach over state-of-the-art\nmethods. Code is available at https://github.com/\nfeifeiobama/InfluenceCL.\n",
        "question": {
            "statement": "What is a common challenge faced by continual learning methods that aim to learn from non-stationary data streams?",
            "options": [
                "Catastrophic forgetting of previous knowledge",
                "Slow adaptation to changes in the data distribution",
                "Insufficient exploration of new data",
                "Overfitting to the current batch of data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "HandsOff: Labeled Dataset Generation With\nNo Additional Human Annotations\nAustin Xu*\nGeorgia Institute of Technology\nMariya I. Vasileva\nAmazon AWS\nAchal Dave†\nToyota Research Institute\nArjun Seshadri\nAmazon Style\nAbstract\nRecent work leverages the expressive power of genera-\ntive adversarial networks (GANs) to generate labeled syn-\nthetic datasets.\nThese dataset generation methods often\nrequire new annotations of synthetic images, which forces\npractitioners to seek out annotators, curate a set of synthetic\nimages, and ensure the quality of generated labels. We in-\ntroduce the HandsOff framework, a technique capable of\nproducing an unlimited number of synthetic images and cor-\nresponding labels after being trained on less than 50 pre-\nexisting labeled images. Our framework avoids the practi-\ncal drawbacks of prior work by unifying the field of GAN in-\nversion with dataset generation. We generate datasets with\nrich pixel-wise labels in multiple challenging domains such\nas faces, cars, full-body human poses, and urban driving\nscenes. Our method achieves state-of-the-art performance\nin semantic segmentation, keypoint detection, and depth es-\ntimation compared to prior dataset generation approaches\nand transfer learning baselines. We additionally showcase\nits ability to address broad challenges in model develop-\nment which stem from fixed, hand-annotated datasets, such\nas the long-tail problem in semantic segmentation. Project\npage: austinxu87.github.io/handsoff.\n",
        "question": {
            "statement": "What is a major limitation of recent dataset generation methods using generative adversarial networks (GANs)?",
            "options": [
                "Requiring large amounts of computational resources",
                "Unable to produce diverse and realistic images",
                "Requiring new annotations of synthetic images",
                "Limited to generating datasets for simple objects"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Trade-off between Robustness and Accuracy of Vision Transformers\nYanxi Li, Chang Xu\nSchool of Computer Science, Faculty of Engineering, The University of Sydney, Australia\nyali0722@uni.sydney.edu.au, c.xu@sydney.edu.au\nAbstract\nAlthough deep neural networks (DNNs) have shown\ngreat successes in computer vision tasks, they are vulner-\nable to perturbations on inputs, and there exists a trade-off\nbetween the natural accuracy and robustness to such per-\nturbations, which is mainly caused by the existence of robust\nnon-predictive features and non-robust predictive features.\nRecent empirical analyses find Vision Transformers (ViTs)\nare inherently robust to various kinds of perturbations, but\nthe aforementioned trade-off still exists for them. In this\nwork, we propose Trade-off between Robustness and Accu-\nracy of Vision Transformers (TORA-ViTs), which aims to\nefficiently transfer ViT models pretrained on natural tasks\nfor both accuracy and robustness. TORA-ViTs consist of\ntwo major components, including a pair of accuracy and\nrobustness adapters to extract predictive and robust fea-\ntures, respectively, and a gated fusion module to adjust the\ntrade-off. The gated fusion module takes outputs of a pre-\ntrained ViT block as queries and outputs of our adapters\nas keys and values, and tokens from different adapters at\ndifferent spatial locations are compared with each other to\ngenerate attention scores for a balanced mixing of predic-\ntive and robust features. Experiments on ImageNet with\nvarious robust benchmarks show that our TORA-ViTs can\nefficiently improve the robustness of naturally pretrained\nViTs while maintaining competitive natural accuracy. Our\nmost balanced setting (TORA-ViTs with λ = 0.5) can main-\ntain 83.7% accuracy on clean ImageNet and reach 54.7%\nand 38.0% accuracy under FGSM and PGD white-box at-\ntacks, respectively. In terms of various ImageNet variants,\nit can reach 39.2% and 56.3% accuracy on ImageNet-A and\nImageNet-R and reach 34.4% mCE on ImageNet-C.\n",
        "question": {
            "statement": "What is the primary goal of the TORA-ViT approach?",
            "options": [
                "To analyze the robustness of Vision Transformers to various perturbations",
                "To achieve state-of-the-art performance on natural image classification tasks",
                "To efficiently balance the trade-off between robustness and accuracy in Vision Transformers",
                "To develop a new architecture for Vision Transformers"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "4"
            ]
        },
        "difference": 6,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "GCFAgg: Global and Cross-view Feature Aggregation for Multi-view Clustering\nWeiqing Yan1,5, Yuanyang Zhang1, Chenlei Lv2, Chang Tang3*\n, Guanghui Yue4, Liang Liao5, Weisi Lin5\n1School of Computer and Control Engineering, Yantai University, Yantai 264005, China\n2College of Computer Science and Software Engineering, Shenzhen University, Shenzhen 518060, China\n3 School of Computer, China University of Geosciences, Wuhan 430074, China\n4 School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen 518060, China\n5 School of Computer Science and Engineering, Nanyang Technological University, 639798, Singapore.\nAbstract\nMulti-view clustering can partition data samples into\ntheir categories by learning a consensus representation in\nunsupervised way and has received more and more atten-\ntion in recent years. However, most existing deep clustering\nmethods learn consensus representation or view-specific\nrepresentations from multiple views via view-wise aggre-\ngation way, where they ignore structure relationship of all\nsamples. In this paper, we propose a novel multi-view clus-\ntering network to address these problems, called Global\nand Cross-view Feature Aggregation for Multi-View Clus-\ntering (GCFAggMVC). Specifically, the consensus data pre-\nsentation from multiple views is obtained via cross-sample\nand cross-view feature aggregation, which fully explores the\ncomplementary of similar samples. Moreover, we align the\nconsensus representation and the view-specific representa-\ntion by the structure-guided contrastive learning module,\nwhich makes the view-specific representations from differ-\nent samples with high structure relationship similar. The\nproposed module is a flexible multi-view data representa-\ntion module, which can be also embedded to the incomplete\nmulti-view data clustering task via plugging our module\ninto other frameworks. Extensive experiments show that the\nproposed method achieves excellent performance in both\ncomplete multi-view data clustering tasks and incomplete\nmulti-view data clustering tasks.\n",
        "question": {
            "statement": "What is the main limitation of most existing deep clustering methods for multi-view clustering?",
            "options": [
                "They require labeled data for training",
                "They are unable to handle incomplete multi-view data",
                "They ignore the structure relationship between all samples",
                "They are limited to two views only"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "How you feelin’? Learning Emotions and Mental States in Movie Scenes\nDhruv Srivastava\nAditya Kumar Singh\nMakarand Tapaswi\nCVIT, IIIT Hyderabad, India\nhttps://katha-ai.github.io/projects/emotx\nAbstract\nMovie story analysis requires understanding characters’\nemotions and mental states.\nTowards this goal, we for-\nmulate emotion understanding as predicting a diverse and\nmulti-label set of emotions at the level of a movie scene\nand for each character. We propose EmoTx, a multimodal\nTransformer-based architecture that ingests videos, multi-\nple characters, and dialog utterances to make joint pre-\ndictions. By leveraging annotations from the MovieGraphs\ndataset [72], we aim to predict classic emotions (e.g. happy,\nangry) and other mental states (e.g. honest, helpful). We\nconduct experiments on the most frequently occurring 10\nand 25 labels, and a mapping that clusters 181 labels to\n26. Ablation studies and comparison against adapted state-\nof-the-art emotion recognition approaches shows the effec-\ntiveness of EmoTx. Analyzing EmoTx’s self-attention scores\nreveals that expressive emotions often look at character to-\nkens while other mental states rely on video and dialog cues.\n",
        "question": {
            "statement": "What is the primary goal of the EmoTx architecture?",
            "options": [
                "To recognize faces in a video",
                "To predict a diverse and multi-label set of emotions and mental states for each character in a movie scene",
                "To generate subtitles for a film",
                "To analyze the plot of a movie"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Parallel Diffusion Models of Operator and Image for Blind Inverse Problems\nHyungjin Chung1*\nJeongsol Kim1∗\nSehui Kim2\nJong Chul Ye2\n1Dept. of Bio & Brain Engineering,\n2Kim Jae Chul Graduate School of AI,\nKAIST\n{hj.chung, jeongsol, sehui.kim, jong.ye}@kaist.ac.kr\nFigure 1. Representative results and overall concept of the proposed method. (a) Results of blind deblurring. Both the image and the kernel\nin the bottom right corner are jointly estimated with the proposed method. (b) Results of imaging through turbulence. (c) Evolution of\njoint reconstruction with the proposed method. 1st, 2nd row illustrate the change of ˆ\nx0(xt) and ˆ\nk0(kt) through time as t = 1 →0, with\nthe measurement and the kernel initialization given on the first column.\nAbstract\nDiffusion\nmodel-based\ninverse\nproblem\nsolvers\nhave\ndemonstrated state-of-the-art performance in cases where\nthe forward operator is known (i.e. non-blind). However,\nthe applicability of the method to blind inverse problems\nThis work was supported by the National Research Foundation of Ko-\nrea under Grant NRF-2020R1A2B5B03001980, by the KAIST Key Re-\nsearch Institute (Interdisciplinary Research Group) Project, by the Insti-\ntute of Information & communications Technology Planning & Evaluation\n(IITP) grant funded by the Korea government (MSIT) (No. 2021-0-02068,\nArtificial Intelligence Innovation Hub), and by the IITP grant funded by\nthe Korea government(MSIT) (No. 2022-0-00984, Development of Arti-\nficial Intelligence Technology for Personalized Plug-and-Play Explanation\nand Verification of Explanation).\nhas yet to be explored.\nIn this work, we show that we\ncan indeed solve a family of blind inverse problems\nby constructing another diffusion prior for the forward\noperator.\nSpecifically, parallel reverse diffusion guided\nby gradients from the intermediate stages enables joint\noptimization of both the forward operator parameters as\nwell as the image, such that both are jointly estimated at the\nend of the parallel reverse diffusion procedure. We show\nthe efficacy of our method on two representative tasks —\nblind deblurring, and imaging through turbulence — and\nshow that our method yields state-of-the-art performance,\nwhile also being flexible to be applicable to general blind\ninverse problems when we know the functional forms. Code\navailable: https://github.com/BlindDPS/blind-dps\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n6059\n",
        "question": {
            "statement": "What is a key advantage of using parallel reverse diffusion in solving blind inverse problems?",
            "options": [
                "Ability to estimate the image but not the forward operator",
                "Guarantee of finding the global optimal solution",
                "Faster computation compared to other methods",
                "Flexibility to be applicable to general blind inverse problems when the functional forms are known."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Context-aware Alignment and Mutual Masking for 3D-Language Pre-training\nZhao Jin1\nMunawar Hayat2\nYuwei Yang1\nYulan Guo3\nYinjie Lei1,\n1Sichuan University\n2Monash University\n3Sun Yat-sen University\njinzhao@stu.scu.edu.cn\nmunawar.hayat@monash.edu\nyuwei@stu.scu.edu.cn\nguoyulan@sysu.edu.cn\nyinjie@scu.edu.cn\n3D point cloud\nContext aware Spatial-\nsemantic Alignment (Sec 3.2)\nMutual 3D-Language \nMasked modeling (Sec 3.3)\n3D-Language Pre-training\nThere is a white \ntable. It is in front \nof a couch.\nTextual description\nFine-tuning \nFeatures belong to \ndifferent semantics\nEnhanced features\n:\n:\nBlue :  \nLanguage \nfeature\nRed :\n3D feature\nMulti-modal \nEncoder\nMulti-modal \nEncoder\nPre-trained \nMulti-modal \nEncoder\nPre-trained \nMulti-modal \nEncoder\nDownstream \nTask \nDecoders\nDownstream \nTask \nDecoders\nPre-trained \nMulti-modal \nEncoder\nDownstream \nTask \nDecoders\n3D-Language \nFeatures \nAlignment\nCSA\n3D-Language \nFeatures \nEnhancement\nM3LM \nFigure 1. Illustration of our 3D-Language Pre-training framework. We ﬁrst semantically align 3D-language features (Sec. 3.2), and then\nfurther enhance their granularity using mutual masked learning (Sec. 3.3). Our learned multi-modal features generalize well across various\ndownstream tasks, including 3D visual grounding, 3D dense captioning and 3D question answering.\nAbstract\n3D visual language reasoning plays an important role\nin effective human-computer interaction. The current ap-\nproaches for 3D visual reasoning are task-speciﬁc, and lack\npre-training methods to learn generic representations that\ncan transfer across various tasks. Despite the encourag-\ning progress in vision-language pre-training for image-text\ndata, 3D-language pre-training is still an open issue due\nto limited 3D-language paired data, highly sparse and ir-\nregular structure of point clouds and ambiguities in spa-\ntial relations of 3D objects with viewpoint changes. In this\npaper, we present a generic 3D-language pre-training ap-\nproach, that tackles multiple facets of 3D-language rea-\nsoning by learning universal representations. Our learn-\ning objective constitutes two main parts. 1) Context aware\nspatial-semantic alignment to establish ﬁne-grained corre-\nspondence between point clouds and texts. It reduces rela-\ntional ambiguities by aligning 3D spatial relationships with\ntextual semantic context. 2) Mutual 3D-Language Masked\nmodeling to enable cross-modality information exchange.\nInstead of reconstructing sparse 3D points for which lan-\nguage can hardly provide cues, we propose masked pro-\nposal reasoning to learn semantic class and mask-invariant\nCorresponding Author: Yinjie Lei (yinjie@scu.edu.cn)\nrepresentations. Our proposed 3D-language pre-training\nmethod achieves promising results once adapted to vari-\nous downstream tasks, including 3D visual grounding, 3D\ndense captioning and 3D question answering. Our codes\nare available at https://github.com/leolyj/3D-VLP\n",
        "question": {
            "statement": "What is the primary goal of the context-aware spatial-semantic alignment step in 3D-language pre-training?",
            "options": [
                "Enable cross-modality information exchange",
                "Reconstruct sparse 3D points",
                "Learn semantic class and mask-invariant representations",
                "Establish fine-grained correspondence between point clouds and texts"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "MEDIC: Remove Model Backdoors via Importance Driven Cloning\nQiuling Xu, Guanhong Tao, Jean Honorio, Yingqi Liu, Shengwei An,\nGuangyu Shen, Siyuan Cheng, Xiangyu Zhang\nPurdue University\n{xu1230, taog, jhonorio, liu1751, an93, shen447, cheng535, xyzhang}@purdue.edu\nAbstract\nWe develop a novel method to remove injected backdoors\nin deep learning models. It works by cloning the benign\nbehaviors of a trojaned model to a new model of the same\nstructure. It trains the clone model from scratch on a very\nsmall subset of samples and aims to minimize a cloning loss\nthat denotes the differences between the activations of im-\nportant neurons across the two models. The set of important\nneurons varies for each input, depending on their magni-\ntude of activations and their impact on the classification\nresult. We theoretically show our method can better recover\nbenign functions of the backdoor model. Meanwhile, we\nprove our method can be more effective in removing back-\ndoors compared with fine-tuning. Our experiments show that\nour technique can effectively remove nine different types of\nbackdoors with minor benign accuracy degradation, outper-\nforming the state-of-the-art backdoor removal techniques\nthat are based on fine-tuning, knowledge distillation, and\nneuron pruning.1\n",
        "question": {
            "statement": "What approach does the MEDIC method use to remove backdoors from deep learning models?",
            "options": [
                "Adding noise to the training data",
                "Fine-tuning the existing model",
                "Pruning unimportant neurons from the model",
                "Cloning the benign behaviors of the trojaned model to a new model"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Harmonious Teacher for Cross-domain Object Detection\nJinhong Deng1\nDongli Xu2\nWen Li3*\nLixin Duan3,4\n1University of Electronic Science and Technology of China\n2University of Sydney\n3Shenzhen Institute for Advanced Study, UESTC\n4Sichuan Provincial People’s Hospital, UESTC\n{jhdengvision, dongliixu, liwenbnu, lxduan}@gmail.com\nAbstract\nSelf-training approaches recently achieved promising re-\nsults in cross-domain object detection, where people it-\neratively generate pseudo labels for unlabeled target do-\nmain samples with a model, and select high-confidence\nsamples to refine the model. In this work, we reveal that\nthe consistency of classification and localization predic-\ntions are crucial to measure the quality of pseudo labels,\nand propose a new Harmonious Teacher approach to im-\nprove the self-training for cross-domain object detection.\nIn particular, we first propose to enhance the quality of\npseudo labels by regularizing the consistency of the clas-\nsification and localization scores when training the detec-\ntion model.\nThe consistency losses are defined for both\nlabeled source samples and the unlabeled target samples.\nThen, we further remold the traditional sample selection\nmethod by a sample reweighing strategy based on the con-\nsistency of classification and localization scores to improve\nthe ranking of predictions.\nThis allows us to fully ex-\nploit all instance predictions from the target domain with-\nout abandoning valuable hard examples.\nWithout bells\nand whistles, our method shows superior performance in\nvarious cross-domain scenarios compared with the state-\nof-the-art baselines, which validates the effectiveness of\nour Harmonious Teacher. Our codes will be available at\nhttps://github.com/kinredon/Harmonious-Teacher.\n",
        "question": {
            "statement": "What is the main goal of the proposed Harmonious Teacher approach in cross-domain object detection?",
            "options": [
                "To use a fixed set of pre-defined pseudo labels",
                "To abandon hard examples in the target domain",
                "To improve the quality of pseudo labels by ensuring consistency between classification and localization predictions",
                "To focus solely on labeled source samples"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Class Attention Transfer Based Knowledge Distillation\nZiyao Guo1, Haonan Yan1,2,*, Hui Li1,*, Xiaodong Lin2\n1Xidian University, 2University of Guelph\ngzyaftermath@outlook.com\nAbstract\nPrevious knowledge distillation methods have shown\ntheir impressive performance on model compression tasks,\nhowever, it is hard to explain how the knowledge they trans-\nferred helps to improve the performance of the student net-\nwork. In this work, we focus on proposing a knowledge\ndistillation method that has both high interpretability and\ncompetitive performance. We first revisit the structure of\nmainstream CNN models and reveal that possessing the\ncapacity of identifying class discriminative regions of in-\nput is critical for CNN to perform classification. Further-\nmore, we demonstrate that this capacity can be obtained\nand enhanced by transferring class activation maps. Based\non our findings, we propose class attention transfer based\nknowledge distillation (CAT-KD). Different from previous\nKD methods, we explore and present several properties of\nthe knowledge transferred by our method, which not only\nimprove the interpretability of CAT-KD but also contribute\nto a better understanding of CNN. While having high inter-\npretability, CAT-KD achieves state-of-the-art performance\non multiple benchmarks. Code is available at: https:\n//github.com/GzyAftermath/CAT-KD.\n",
        "question": {
            "statement": "What is a key factor in enabling convolutional neural networks (CNNs) to perform classification tasks?",
            "options": [
                "Randomly initializing weights",
                "Having a large number of parameters",
                "Identifying class discriminative regions of input",
                "Using a specific type of activation function"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Continual Semantic Segmentation with Automatic Memory Sample Selection\nLanyun Zhu1*\nTianrun Chen2*\nJianxiong Yin3\nSimon See3\nJun Liu1†\nSingapore University of Technology and Design 1\nZhejiang University 2\nNVIDIA AI Tech Centre 3\nlanyun zhu@mymail.sutd.edu.sg\ntianrun.chen@zju.edu.cn\n{jianxiongy, ssee}@nvidia.com\njun liu@sutd.edu.sg\nAbstract\nContinual Semantic Segmentation (CSS) extends static\nsemantic segmentation by incrementally introducing new\nclasses for training. To alleviate the catastrophic forgetting\nissue in CSS, a memory buffer that stores a small number\nof samples from the previous classes is constructed for re-\nplay. However, existing methods select the memory samples\neither randomly or based on a single-factor-driven hand-\ncrafted strategy, which has no guarantee to be optimal. In\nthis work, we propose a novel memory sample selection\nmechanism that selects informative samples for effective re-\nplay in a fully automatic way by considering comprehen-\nsive factors including sample diversity and class perfor-\nmance. Our mechanism regards the selection operation as\na decision-making process and learns an optimal selection\npolicy that directly maximizes the validation performance\non a reward set. To facilitate the selection decision, we de-\nsign a novel state representation and a dual-stage action\nspace. Our extensive experiments on Pascal-VOC 2012 and\nADE 20K datasets demonstrate the effectiveness of our ap-\nproach with state-of-the-art (SOTA) performance achieved,\noutperforming the second-place one by 12.54% for the 6-\nstage setting on Pascal-VOC 2012.\n",
        "question": {
            "statement": "What is a major challenge in Continual Semantic Segmentation?",
            "options": [
                "catastrophic forgetting",
                "overfitting",
                "class imbalance",
                "underfitting"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "iQuery: Instruments as Queries for Audio-Visual Sound Separation\nJiaben Chen1, Renrui Zhang2, Dongze Lian3, Jiaqi Yang4, Ziyao Zeng4, Jianbo Shi5\n1UC San Diego\n2The Chinese University of Hong Kong\n3National University of Singapore\n4ShanghaiTech University\n5University of Pennsylvania\nAbstract\nCurrent audio-visual separation methods share a stan-\ndard architecture design where an audio encoder-decoder\nnetwork is fused with visual encoding features at the en-\ncoder bottleneck. This design confounds the learning of\nmulti-modal feature encoding with robust sound decod-\ning for audio separation. To generalize to a new instru-\nment, one must fine-tune the entire visual and audio net-\nwork for all musical instruments.\nWe re-formulate the\nvisual-sound separation task and propose Instruments as\nQueries (iQuery) with a flexible query expansion mech-\nanism.\nOur approach ensures cross-modal consistency\nand cross-instrument disentanglement. We utilize “visually\nnamed” queries to initiate the learning of audio queries\nand use cross-modal attention to remove potential sound\nsource interference at the estimated waveforms. To gen-\neralize to a new instrument or event class, drawing inspi-\nration from the text-prompt design, we insert additional\nqueries as audio prompts while freezing the attention mech-\nanism. Experimental results on three benchmarks demon-\nstrate that our iQuery improves audio-visual sound source\nseparation performance.\nCode is available at https:\n//github.com/JiabenChen/iQuery.\n",
        "question": {
            "statement": "What is a limitation of current audio-visual separation methods?",
            "options": [
                "They require fine-tuning the entire network for each new instrument",
                "They are only effective for separating vocals from instrumental tracks",
                "They rely solely on visual features for separation",
                "They are unable to handle multiple sound sources"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "BiFormer: Vision Transformer with Bi-Level Routing Attention\nLei Zhu1\nXinjiang Wang2\nZhanghan Ke1\nWayne Zhang2\nRynson Lau1†\n1 City University of Hong Kong\n2 SenseTime Research\n{lzhu68-c,zhanghake2-c}@my.cityu.edu.hk, {wangxinjiang,wayne.zhang}@sensetime.com\nRynson.Lau@cityu.edu.hk\nAbstract\nAs the core building block of vision transformers, atten-\ntion is a powerful tool to capture long-range dependency.\nHowever, such power comes at a cost: it incurs a huge\ncomputation burden and heavy memory footprint as pair-\nwise token interaction across all spatial locations is com-\nputed. A series of works attempt to alleviate this problem\nby introducing handcrafted and content-agnostic sparsity\ninto attention, such as restricting the attention operation to\nbe inside local windows, axial stripes, or dilated windows.\nIn contrast to these approaches, we propose a novel dy-\nnamic sparse attention via bi-level routing to enable a more\nﬂexible allocation of computations with content awareness.\nSpeciﬁcally, for a query, irrelevant key-value pairs are ﬁrst\nﬁltered out at a coarse region level, and then ﬁne-grained\ntoken-to-token attention is applied in the union of remain-\ning candidate regions (i.e., routed regions).\nWe provide\na simple yet effective implementation of the proposed bi-\nlevel routing attention, which utilizes the sparsity to save\nboth computation and memory while involving only GPU-\nfriendly dense matrix multiplications. Built with the pro-\nposed bi-level routing attention, a new general vision trans-\nformer, named BiFormer, is then presented. As BiFormer\nattends to a small subset of relevant tokens in a query adap-\ntive manner without distraction from other irrelevant ones,\nit enjoys both good performance and high computational\nefﬁciency, especially in dense prediction tasks. Empirical\nresults across several computer vision tasks such as image\nclassiﬁcation, object detection, and semantic segmentation\nverify the effectiveness of our design. Code is available at\nhttps://github.com/rayleizhu/BiFormer.\n",
        "question": {
            "statement": "What is the main advantage of using dynamic sparse attention in vision transformers?",
            "options": [
                "It reduces the number of parameters in the model",
                "It always attends to all tokens in the input sequence",
                "It enables flexible allocation of computations with content awareness",
                "It requires manual tuning of hyperparameters"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Mask DINO: Towards A Unified Transformer-based Framework for Object\nDetection and Segmentation\nFeng Li1,3*†, Hao Zhang1,3∗†, Huaizhe Xu1,3, Shilong Liu2,3,\nLei Zhang3‡, Lionel M. Ni1,4, Heung-Yeung Shum1,3\n1The Hong Kong University of Science and Technology.\n2Dept. of CST., BNRist Center, Institute for AI, Tsinghua University.\n3International Digital Economy Academy (IDEA).\n4The Hong Kong University of Science and Technology (Guangzhou).\n{fliay,hzhangcx,hxubr}@connect.ust.hk {liusl20}@mails.tsinghua.edu.cn {leizhang}@idea.edu.cn {ni,hshum}@ust.hk\nAbstract\nIn this paper we present Mask DINO, a unified object\ndetection and segmentation framework. Mask DINO extends\nDINO (DETR with Improved Denoising Anchor Boxes) by\nadding a mask prediction branch which supports all im-\nage segmentation tasks (instance, panoptic, and semantic).\nIt makes use of the query embeddings from DINO to dot-\nproduct a high-resolution pixel embedding map to predict\na set of binary masks. Some key components in DINO are\nextended for segmentation through a shared architecture\nand training process. Mask DINO is simple, efficient, and\nscalable, and it can benefit from joint large-scale detec-\ntion and segmentation datasets. Our experiments show that\nMask DINO significantly outperforms all existing special-\nized segmentation methods, both on a ResNet-50 backbone\nand a pre-trained model with SwinL backbone. Notably,\nMask DINO establishes the best results to date on instance\nsegmentation (54.5 AP on COCO), panoptic segmentation\n(59.4 PQ on COCO), and semantic segmentation (60.8 mIoU\non ADE20K) among models under one billion parameters.\nCode is available at https://github.com/IDEA-\nResearch/MaskDINO.\n",
        "question": {
            "statement": "What is the main advantage of the Mask DINO framework?",
            "options": [
                "It requires manual annotation of masks",
                "It can benefit from joint large-scale detection and segmentation datasets",
                "It uses a separate architecture for each task",
                "It is limited to specific types of image segmentation tasks"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "7",
                "0",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Collaborative Noisy Label Cleaner: Learning Scene-aware Trailers for\nMulti-modal Highlight Detection in Movies\nBei Gan\nXiujun Shu∗\nRuizhi Qiao∗\nHaoqian Wu\nKeyu Chen\nHanjun Li\nBo Ren\nTencent YouTu Lab\n{stylegan, xiujunshu, ruizhiqiao, linuswu, yolochen, hanjunli, timren}@tencent.com\nTrailers\nMovies\n…\n…\n…\n…\nComplete Highlights for audience1\nComplete Highlights for audience2\nIrrelevant clips\nFigure 1. As the preview of the movie, trailers are selected by professionals to grab an audience’s attention.However, trailers are usually\ncomposed with shots sparsely selected from movies to avoid spoilers, and the audience cannot get complete highlight information.Some\ntrailer clips convey the artistic style of the film only and lack movie storylines, disturbing the audience’s impressions.In addition, different\naudiences may be interested in different styles of clips, which makes it challenging to learn highlights from them.\nAbstract\nMovie highlights stand out of the screenplay for efficient\nbrowsing and play a crucial role on social media platforms.\nBased on existing efforts, this work has two observations:\n(1) For different annotators, labeling highlight has un-\ncertainty, which leads to inaccurate and time-consuming\nannotations. (2) Besides previous supervised or unsuper-\nvised settings, some existing video corpora can be useful,\ne.g., trailers, but they are often noisy and incomplete to\ncover the full highlights. In this work, we study a more\npractical and promising setting, i.e., reformulating high-\nlight detection as “learning with noisy labels”. This setting\ndoes not require time-consuming manual annotations and\ncan fully utilize existing abundant video corpora.\nFirst,\nbased on movie trailers, we leverage scene segmentation\nto obtain complete shots, which are regarded as noisy\nlabels.\nThen, we propose a Collaborative noisy Label\n*Corresponding author.\nCleaner (CLC) framework to learn from noisy highlight\nmoments. CLC consists of two modules: augmented cross-\npropagation (ACP) and multi-modality cleaning (MMC).\nThe former aims to exploit the closely related audio-visual\nsignals and fuse them to learn unified multi-modal repre-\nsentations.\nThe latter aims to achieve cleaner highlight\nlabels by observing the changes in losses among different\nmodalities. To verify the effectiveness of CLC, we further\ncollect a large-scale highlight dataset named MovieLights.\nComprehensive experiments on MovieLights and YouTube\nHighlights datasets demonstrate the effectiveness of our\napproach.\nCode has been made available at:https:\n/ / github . com / TencentYoutuResearch /\nHighlightDetection-CLC.\n",
        "question": {
            "statement": "What is a major challenge in learning highlight detection from movie trailers?",
            "options": [
                "Trailers are always accurate representations of the movie",
                "Manual annotation of highlights is a quick process",
                "Different audiences may be interested in different styles of clips",
                "Scene segmentation is not possible on movie trailers"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "8",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Dual-path Adaptation from Image to Video Transformers\nJungin Park1∗\nJiyoung Lee2∗\nKwanghoon Sohn1,3†\n1Yonsei University\n2NAVER AI Lab\n3Korea Institute of Science and Technology (KIST)\n{newrun, khsohn}@yonsei.ac.kr\nlee.j@navercorp.com\nAbstract\nIn this paper, we efficiently transfer the surpassing rep-\nresentation power of the vision foundation models, such as\nViT and Swin, for video understanding with only a few train-\nable parameters. Previous adaptation methods have simul-\ntaneously considered spatial and temporal modeling with a\nunified learnable module but still suffered from fully lever-\naging the representative capabilities of image transformers.\nWe argue that the popular dual-path (two-stream) architec-\nture in video models can mitigate this problem. We pro-\npose a novel DUALPATH adaptation separated into spatial\nand temporal adaptation paths, where a lightweight bottle-\nneck adapter is employed in each transformer block. Espe-\ncially for temporal dynamic modeling, we incorporate con-\nsecutive frames into a grid-like frameset to precisely imi-\ntate vision transformers’ capability that extrapolates rela-\ntionships between tokens. In addition, we extensively inves-\ntigate the multiple baselines from a unified perspective in\nvideo understanding and compare them with DUALPATH.\nExperimental results on four action recognition benchmarks\nprove that pretrained image transformers with DUALPATH\ncan be effectively generalized beyond the data domain.\n",
        "question": {
            "statement": "What approach is proposed to adapt vision foundation models, such as ViT and Swin, for video understanding?",
            "options": [
                "A hierarchical attention mechanism for selective focus on spatial or temporal features",
                "A single unified module for both spatial and temporal modeling",
                "A dual-path architecture with separate spatial and temporal adaptation paths",
                "A multi-task learning framework for simultaneous image and video processing"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Probability-based Global Cross-modal Upsampling for Pansharpening\nZeyu Zhu1, Xiangyong Cao1,2,3*, Man Zhou5, Junhao Huang1, Deyu Meng1,4\n1Xi’an Jiaotong University; 2School of Computer Science and Technology, Xi’an Jiaotong University\n3Ministry of Education Key Lab For Intelligent Networks and Network Security, Xi’an Jiaotong University\n4Macau University of Science and Technology\n5Nanyang Technological University\n{zeyuzhu2077, manzhountu, junhaoxjtu}@gmail.com\n{caoxiangyong, dymeng}@mail.xjtu.edu.cn\nAbstract\nPansharpening is an essential preprocessing step for re-\nmote sensing image processing.\nAlthough deep learning\n(DL) approaches performed well on this task, current up-\nsampling methods used in these approaches only utilize the\nlocal information of each pixel in the low-resolution multi-\nspectral (LRMS) image while neglecting to exploit its global\ninformation as well as the cross-modal information of the\nguiding panchromatic (PAN) image, which limits their per-\nformance improvement. To address this issue, this paper\ndevelops a novel probability-based global cross-modal up-\nsampling (PGCU) method for pan-sharpening. Precisely,\nwe first formulate the PGCU method from a probabilis-\ntic perspective and then design an efficient network mod-\nule to implement it by fully utilizing the information men-\ntioned above while simultaneously considering the chan-\nnel specificity. The PGCU module consists of three blocks,\ni.e., information extraction (IE), distribution and expecta-\ntion estimation (DEE), and fine adjustment (FA). Exten-\nsive experiments verify the superiority of the PGCU method\ncompared with other popular upsampling methods. Addi-\ntionally, experiments also show that the PGCU module can\nhelp improve the performance of existing SOTA deep learn-\ning pansharpening methods. The codes are available at\nhttps://github.com/Zeyu-Zhu/PGCU.\n",
        "question": {
            "statement": "What is the primary goal of pansharpening in remote sensing image processing?",
            "options": [
                "To reduce the noise in panchromatic images",
                "To change the color palette of the images",
                "To increase the number of spectral bands",
                "To enhance the resolution of multispectral images"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "IMP: Iterative Matching and Pose Estimation with Adaptive Pooling\nFei Xue\nIgnas Budvytis\nRoberto Cipolla\nUniversity of Cambridge\n{fx221, ib255, rc10001}@cam.ac.uk\nAbstract\nPrevious methods solve feature matching and pose esti-\nmation using a two-stage process by ﬁrst ﬁnding matches\nand then estimating the pose. As they ignore the geomet-\nric relationships between the two tasks, they focus on ei-\nther improving the quality of matches or ﬁltering poten-\ntial outliers, leading to limited efﬁciency or accuracy. In\ncontrast, we propose an iterative matching and pose es-\ntimation framework (IMP) leveraging the geometric con-\nnections between the two tasks: a few good matches are\nenough for a roughly accurate pose estimation; a roughly\naccurate pose can be used to guide the matching by pro-\nviding geometric constraints. To this end, we implement\na geometry-aware recurrent attention-based module which\njointly outputs sparse matches and camera poses. Specif-\nically, for each iteration, we ﬁrst implicitly embed geo-\nmetric information into the module via a pose-consistency\nloss, allowing it to predict geometry-aware matches pro-\ngressively. Second, we introduce an efﬁcient IMP, called\nEIMP, to dynamically discard keypoints without potential\nmatches, avoiding redundant updating and signiﬁcantly re-\nducing the quadratic time complexity of attention computa-\ntion in transformers. Experiments on YFCC100m, Scannet,\nand Aachen Day-Night datasets demonstrate that the pro-\nposed method outperforms previous approaches in terms\nof accuracy and efﬁciency. Code is available at https:\n//github.com/feixue94/imp-release\n",
        "question": {
            "statement": "What is the main advantage of the proposed Iterative Matching and Pose Estimation framework?",
            "options": [
                "It relies heavily on filtering potential outliers",
                "It leverages the geometric connections between feature matching and pose estimation",
                "It focuses solely on improving the quality of matches",
                "It uses a two-stage process to improve efficiency"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Generalized UAV Object Detection via Frequency Domain Disentanglement\nKunyu Wang, Xueyang Fu, Yukun Huang, Chengzhi Cao, Gege Shi, Zheng-Jun Zha∗\nUniversity of Science and Technology of China, China\n{kunyuwang@mail., xyfu@, kevinh@mail., chengzhicao@mail., shigg@mail., zhazj@}ustc.edu.cn\nAbstract\nWhen deploying the Unmanned Aerial Vehicles object\ndetection (UAV-OD) network to complex and unseen real-\nworld scenarios, the generalization ability is usually re-\nduced due to the domain shift. To address this issue, this\npaper proposes a novel frequency domain disentanglement\nmethod to improve the UAV-OD generalization.\nSpecifi-\ncally, we first verified that the spectrum of different bands\nin the image has different effects to the UAV-OD general-\nization. Based on this conclusion, we design two learnable\nfilters to extract domain-invariant spectrum and domain-\nspecific spectrum, respectively.\nThe former can be used\nto train the UAV-OD network and improve its capacity for\ngeneralization. In addition, we design a new instance-level\ncontrastive loss to guide the network training. This loss\nenables the network to concentrate on extracting domain-\ninvariant spectrum and domain-specific spectrum, so as\nto achieve better disentangling results. Experimental re-\nsults on three unseen target domains demonstrate that our\nmethod has better generalization ability than both the base-\nline method and state-of-the-art methods.\n",
        "question": {
            "statement": "What is the main challenge when deploying an unmanned aerial vehicle (UAV) object detection network to real-world scenarios?",
            "options": [
                "Sensor calibration",
                "Object occlusion",
                "Noise interference",
                "Domain shift"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": false,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "EVAL: Explainable Video Anomaly Localization\nAshish Singh1,2*\nMichael J. Jones2*\nErik G. Learned-Miller1,2\n1CICS, University of Massachusetts Amherst\n2Mitsubishi Electric Research Labs\nashishsingh@cs.umass.edu\nmjones@merl.com\nelm@cs.umass.edu\nAbstract\nWe\ndevelop\na\nnovel\nframework\nfor\nsingle-scene\nvideo\nanomaly\nlocalization\nthat\nallows\nfor\nhuman-\nunderstandable reasons for the decisions the system makes.\nWe first learn general representations of objects and their\nmotions (using deep networks) and then use these repre-\nsentations to build a high-level, location-dependent model\nof any particular scene. This model can be used to detect\nanomalies in new videos of the same scene. Importantly, our\napproach is explainable – our high-level appearance and\nmotion features can provide human-understandable rea-\nsons for why any part of a video is classified as normal\nor anomalous. We conduct experiments on standard video\nanomaly detection datasets (Street Scene, CUHK Avenue,\nShanghaiTech and UCSD Ped1, Ped2) and show significant\nimprovements over the previous state-of-the-art. All of our\ncode and extra datasets will be made publicly available.\n",
        "question": {
            "statement": "What characteristic of the EVAL framework allows humans to understand the reasoning behind its anomaly detection decisions?",
            "options": [
                "Accuracy",
                "Flexibility",
                "Explainability",
                "Scalability"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Robust Model-based Face Reconstruction through\nWeakly-Supervised Outlier Segmentation\nChunlu Li1,2\nAndreas Morel-Forster 2\nThomas Vetter 2\nBernhard Egger3,∗\nAdam Kortylewski4,5,∗\nlcl@seu.edu.cn\nbernhard.egger@fau.de\nakortyle@mpi-inf.mpg.de\n1 School of Automation, Southeast University\n2 Department of Mathematics and Computer Science, University of Basel\n3 Friedrich-Alexander-Universit¨\nat Erlangen-N¨\nurnberg\n4University of Freiburg\n5Max Planck Institute for Informatics\nAbstract\nIn this work, we aim to enhance model-based face re-\nconstruction by avoiding fitting the model to outliers, i.e.\nregions that cannot be well-expressed by the model such as\noccluders or make-up. The core challenge for localizing\noutliers is that they are highly variable and difficult to anno-\ntate. To overcome this challenging problem, we introduce a\njoint Face-autoencoder and outlier segmentation approach\n(FOCUS). In particular, we exploit the fact that the outliers\ncannot be fitted well by the face model and hence can be\nlocalized well given a high-quality model fitting. The main\nchallenge is that the model fitting and the outlier segmenta-\ntion are mutually dependent on each other, and need to be\ninferred jointly. We resolve this chicken-and-egg problem\nwith an EM-type training strategy, where a face autoen-\ncoder is trained jointly with an outlier segmentation net-\nwork. This leads to a synergistic effect, in which the seg-\nmentation network prevents the face encoder from fitting to\nthe outliers, enhancing the reconstruction quality. The im-\nproved 3D face reconstruction, in turn, enables the segmen-\ntation network to better predict the outliers. To resolve the\nambiguity between outliers and regions that are difficult to\nfit, such as eyebrows, we build a statistical prior from syn-\nthetic data that measures the systematic bias in model fit-\nting. Experiments on the NoW testset demonstrate that FO-\nCUS achieves SOTA 3D face reconstruction performance\namong all baselines trained without 3D annotation. More-\nover, our results on CelebA-HQ and AR database show that\nthe segmentation network can localize occluders accurately\n∗Denotes same contribution.\nCodes available at: github.com/unibas-gravis/Occlusion-Robust-MoFA\nC.Li is funded by the China Scholarship Council (CSC) from the Min-\nistry of Education of P.R. China.\nB.Egger was supported by a Post-\nDoc Mobility Grant, Swiss National Science Foundation P400P2 191110.\nA.Kortylewski acknowledges support via his Emmy Noether Research\nGroup funded by the German Science Foundation (DFG) under Grant No.\n468670075. Sincere gratitude to Tatsuro Koizumi and William A. P. Smith\nwho offered the MoFA re-implementation.\nFigure 1. FOCUS conducts face reconstruction and outlier seg-\nmentation jointly under weak supervision. Top to bottom: target\nimages, our reconstruction images, and estimated outlier masks.\ndespite being trained without any segmentation annotation.\n",
        "question": {
            "statement": "What is the main challenge in localizing outliers in model-based face reconstruction?",
            "options": [
                "They can be easily detected using simple image processing techniques",
                "They are always located in the center of the face",
                "They are highly variable and difficult to annotate",
                "They are only present in low-quality images"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Backdoor Defense via Adaptively Splitting Poisoned Dataset\nKuofeng Gao1*\n, Yang Bai 2*, Jindong Gu3†\n, Yong Yang4, Shu-Tao Xia15†\n1 Tsinghua University\n2 Tencent Security Zhuque Lab\n3 University of Oxford\n4 Tencent Security Platform Department\n5 Peng Cheng Laboratory\ngkf21@mails.tsinghua.edu.cn, {mavisbai,coolcyang}@tencent.com\njindong.gu@eng.ox.ac.uk, xiast@sz.tsinghua.edu.cn\nAbstract\nBackdoor defenses have been studied to alleviate the\nthreat of deep neural networks (DNNs) being backdoor\nattacked and thus maliciously altered. Since DNNs usu-\nally adopt some external training data from an untrusted\nthird party, a robust backdoor defense strategy during the\ntraining stage is of importance.\nWe argue that the core\nof training-time defense is to select poisoned samples and\nto handle them properly. In this work, we summarize the\ntraining-time defenses from a unified framework as split-\nting the poisoned dataset into two data pools.\nUnder\nour framework, we propose an adaptively splitting dataset-\nbased defense (ASD). Concretely, we apply loss-guided split\nand meta-learning-inspired split to dynamically update two\ndata pools.\nWith the split clean data pool and polluted\ndata pool, ASD successfully defends against backdoor at-\ntacks during training. Extensive experiments on multiple\nbenchmark datasets and DNN models against six state-of-\nthe-art backdoor attacks demonstrate the superiority of our\nASD. Our code is available at https://github.com/\nKuofengGao/ASD.\n",
        "question": {
            "statement": "What is the main goal of a backdoor defense strategy during the training stage of deep neural networks?",
            "options": [
                "To improve the accuracy of the network on a specific task",
                "To increase the interpretability of the network's decisions",
                "To prevent the network from being maliciously altered by untrusted third-party training data",
                "To reduce the computational resources required for training"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding\nTal Shaharabany\nTel Aviv University\nshaharabany@mail.tau.ac.il\nLior Wolf\nTel Aviv University\nwolf@cs.tau.ac.il\nAbstract\nA phrase grounding model receives an input image and\na text phrase and outputs a suitable localization map. We\npresent an effective way to refine a phrase ground model\nby considering self-similarity maps extracted from the la-\ntent representation of the model’s image encoder.\nOur\nmain insights are that these maps resemble localization\nmaps and that by combining such maps, one can obtain\nuseful pseudo-labels for performing self-training. Our re-\nsults surpass, by a large margin, the state of the art in\nweakly supervised phrase grounding. A similar gap in per-\nformance is obtained for a recently proposed downstream\ntask called WWbL, in which only the image is input, without\nany text. Our code is available at https://github.\ncom/talshaharabany/Similarity-Maps-for-\nSelf-Training-Weakly-Supervised-Phrase-\nGrounding.\n",
        "question": {
            "statement": "What type of information does a phrase grounding model receive as input?",
            "options": [
                "Only a text phrase",
                "A video and a sentence",
                "An image and a text phrase",
                "Only an image"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Efficient Mask Correction for Click-Based Interactive Image Segmentation\nFei Du, Jianlong Yuan, Zhibin Wang, Fan Wang\nAlibaba Group\n{dufei.df, gongyuan.yjl, zhibin.waz, fan.w}@alibaba-inc.com\nAbstract\nThe goal of click-based interactive image segmentation\nis to extract target masks with the input of positive/negative\nclicks. Every time a new click is placed, existing methods\nrun the whole segmentation network to obtain a corrected\nmask, which is inefficient since several clicks may be needed\nto reach satisfactory accuracy.\nTo this end, we propose\nan efficient method to correct the mask with a lightweight\nmask correction network. The whole network remains a\nlow computational cost from the second click, even if we\nhave a large backbone. However, a simple correction net-\nwork with limited capacity is not likely to achieve com-\nparable performance with a classic segmentation network.\nThus, we propose a click-guided self-attention module and\na click-guided correlation module to effectively exploits the\nclick information to boost performance. First, several tem-\nplates are selected based on the semantic similarity with\nclick features. Then the self-attention module propagates\nthe template information to other pixels, while the correla-\ntion module directly uses the templates to obtain target out-\nlines. With the efficient architecture and two click-guided\nmodules, our method shows preferable performance and ef-\nficiency compared to existing methods. The code will be\nreleased at https://github.com/feiaxyt/EMC-\nClick.\n",
        "question": {
            "statement": "What is the main challenge in traditional click-based interactive image segmentation methods?",
            "options": [
                "Running the entire segmentation network every time a new click is added",
                "Insufficient semantic similarity between click features and templates",
                "Inability to exploit click information effectively",
                "Limited capacity of the correction network"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Learning Analytical Posterior Probability for Human Mesh Recovery\nQi Fang1\nKang Chen1\nYinghui Fan1\nQing Shuai2\nJiefeng Li3\nWeidong Zhang1\n1NetEase Games AI Lab\n2Zhejiang University\n3Shanghai Jiao Tong University\nAbstract\nDespite various probabilistic methods for modeling the\nuncertainty and ambiguity in human mesh recovery, their\noverall precision is limited because existing formulations\nfor joint rotations are either not constrained to SO(3) or\ndifficult to learn for neural networks. To address such an\nissue, we derive a novel analytical formulation for learn-\ning posterior probability distributions of human joint rota-\ntions conditioned on bone directions in a Bayesian man-\nner, and based on this, we propose a new posterior-guided\nframework for human mesh recovery. We demonstrate that\nour framework is not only superior to existing SOTA base-\nlines on multiple benchmarks but also flexible enough to\nseamlessly incorporate with additional sensors due to its\nBayesian nature.\nThe code is available at https://\ngithub.com/NetEase-GameAI/ProPose.\n",
        "question": {
            "statement": "What is a limitation of existing probabilistic methods for human mesh recovery?",
            "options": [
                "Lack of data for training",
                "Inability to model uncertainty and ambiguity",
                "Joint rotation formulations are either not constrained to SO(3) or difficult to learn for neural networks.",
                "Insufficient computational resources"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Generating Part-Aware Editable 3D Shapes without 3D Supervision\nKonstantinos Tertikas*1,3\nDespoina Paschalidou2\nBoxiao Pan2\nJeong Joon Park2\nMikaela Angelina Uy2\nIoannis Emiris3,1\nYannis Avrithis4\nLeonidas Guibas2\n1National and Kapodistrian University of Athens\n2Stanford University\n3Athena RC, Greece\n4Institute of Advanced Research in Artificial Intelligence (IARAI)\nFigure 1. Part-Aware Controllable 3D Shape Generation and Editing. We address the task of part-aware 3D shape generation and\nediting without explicit 3D supervision. Prior part-aware generative models [34, 40] assume 3D supervision, at training, and only allow\nchanging the shape of the object. In this work, we introduce PartNeRF, a generative model capable of editing the shape and appearance of\ngenerated shapes that are parametrized as a collection of locally defined NeRFs.\nAbstract\nImpressive progress in generative models and implicit\nrepresentations gave rise to methods that can generate 3D\nshapes of high quality. However, being able to locally con-\ntrol and edit shapes is another essential property that can\nunlock several content creation applications. Local control\ncan be achieved with part-aware models, but existing meth-\nods require 3D supervision and cannot produce textures. In\nthis work, we devise PartNeRF, a novel part-aware gener-\native model for editable 3D shape synthesis that does not\nrequire any explicit 3D supervision. Our model generates\nobjects as a set of locally defined NeRFs, augmented with\nan affine transformation. This enables several editing op-\nerations such as applying transformations on parts, mixing\n*Work done during internship at Stanford.\nparts from different objects etc. To ensure distinct, manip-\nulable parts we enforce a hard assignment of rays to parts\nthat makes sure that the color of each ray is only determined\nby a single NeRF. As a result, altering one part does not af-\nfect the appearance of the others. Evaluations on various\nShapeNet categories demonstrate the ability of our model to\ngenerate editable 3D objects of improved fidelity, compared\nto previous part-based generative approaches that require\n3D supervision or models relying on NeRFs.\n",
        "question": {
            "statement": "What is a key advantage of using part-aware generative models in 3D shape generation?",
            "options": [
                "They can only generate low-quality shapes",
                "They require explicit 3D supervision",
                "They enable local control and editing of generated shapes",
                "They are limited to generating shapes with fixed appearances"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "DPF: Learning Dense Prediction Fields with Weak Supervision\nXiaoxue Chen1, Yuhang Zheng2, Yupeng Zheng3\nQiang Zhou1, Hao Zhao1, Guyue Zhou1, Ya-Qin Zhang1\n1AIR, Tsinghua University 2BUAA 3CASIA\n{chenxiaoxue, zhaohao}@air.tsinghua.edu.cn, zyh 021@buaa.edu.cn\nAbstract\nNowadays, many visual scene understanding problems\nare addressed by dense prediction networks. But pixel-wise\ndense annotations are very expensive (e.g., for scene pars-\ning) or impossible (e.g., for intrinsic image decomposition),\nmotivating us to leverage cheap point-level weak supervi-\nsion. However, existing pointly-supervised methods still use\nthe same architecture designed for full supervision. In stark\ncontrast to them, we propose a new paradigm that makes\npredictions for point coordinate queries, as inspired by the\nrecent success of implicit representations, like distance or\nradiance fields. As such, the method is named as dense pre-\ndiction fields (DPFs). DPFs generate expressive interme-\ndiate features for continuous sub-pixel locations, thus al-\nlowing outputs of an arbitrary resolution. DPFs are nat-\nurally compatible with point-level supervision. We show-\ncase the effectiveness of DPFs using two substantially dif-\nferent tasks: high-level semantic parsing and low-level in-\ntrinsic image decomposition. In these two cases, supervi-\nsion comes in the form of single-point semantic category\nand two-point relative reflectance, respectively. As bench-\nmarked by three large-scale public datasets PASCALCon-\ntext, ADE20K and IIW, DPFs set new state-of-the-art per-\nformance on all of them with significant margins. Code can\nbe accessed at https://github.com/cxx226/DPF.\n",
        "question": {
            "statement": "What is the main advantage of using Dense Prediction Fields (DPFs) over traditional dense prediction networks?",
            "options": [
                "They can be trained with point-level weak supervision",
                "They require less computational resources",
                "They can only be applied to high-level semantic parsing tasks",
                "They produce lower-resolution output"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Align and Attend: Multimodal Summarization with Dual Contrastive Losses\nBo He1*\n, Jun Wang1, Jielin Qiu2, Trung Bui3, Abhinav Shrivastava1, Zhaowen Wang3\n1University of Maryland, College Park\n2Carnegie Mellon University\n3Adobe Research\n{bohe,abhinav}@cs.umd.edu, junwong@umd.edu, jielinq@andrew.cmu.edu, {bui,zhawang}@adobe.com\nAbstract\nThe goal of multimodal summarization is to extract\nthe most important information from different modalities\nto form summaries. Unlike unimodal summarization, the\nmultimodal summarization task explicitly leverages cross-\nmodal information to help generate more reliable and high-\nquality summaries. However, existing methods fail to lever-\nage the temporal correspondence between different modal-\nities and ignore the intrinsic correlation between differ-\nent samples.\nTo address this issue, we introduce Align\nand Attend Multimodal Summarization (A2Summ), a uni-\nfied multimodal transformer-based model which can ef-\nfectively align and attend the multimodal input.\nIn ad-\ndition, we propose two novel contrastive losses to model\nboth inter-sample and intra-sample correlations. Exten-\nsive experiments on two standard video summarization\ndatasets (TVSum and SumMe) and two multimodal sum-\nmarization datasets (Daily Mail and CNN) demonstrate the\nsuperiority of A2Summ, achieving state-of-the-art perfor-\nmances on all datasets. Moreover, we collected a large-scale\nmultimodal summarization dataset BLiSS, which contains\nlivestream videos and transcribed texts with annotated sum-\nmaries. Our code and dataset are publicly available at\nhttps://boheumd.github.io/A2Summ/.\n",
        "question": {
            "statement": "What is the main advantage of multimodal summarization over unimodal summarization?",
            "options": [
                "It is only applicable to text data",
                "It generates more reliable and high-quality summaries by leveraging cross-modal information",
                "It requires less data than unimodal summarization",
                "It is faster than unimodal summarization"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "MHPL: Minimum Happy Points Learning for Active Source Free\nDomain Adaptation\nFan Wang1, Zhongyi Han1, Zhiyan Zhang1, Rundong He1, Yilong Yin1*\n1 School of Software, Shandong University, China\n{fanwang,rundong he}@mail.sdu.edu.cn, {hanzhongyicn,zyzhangcs}@gmail.com, ylyin@sdu.edu.cn\nAbstract\nSource free domain adaptation (SFDA) aims to transfer\na trained source model to the unlabeled target domain with-\nout accessing the source data. However, the SFDA setting\nfaces a performance bottleneck due to the absence of source\ndata and target supervised information, as evidenced by\nthe limited performance gains of the newest SFDA meth-\nods. Active source free domain adaptation (ASFDA) can\nbreak through the problem by exploring and exploiting a\nsmall set of informative samples via active learning.\nIn\nthis paper, we first find that those satisfying the proper-\nties of neighbor-chaotic, individual-different, and source-\ndissimilar are the best points to select. We define them as the\nminimum happy (MH) points challenging to explore with ex-\nisting methods. We propose minimum happy points learning\n(MHPL) to explore and exploit MH points actively. We de-\nsign three unique strategies: neighbor environment uncer-\ntainty, neighbor diversity relaxation, and one-shot query-\ning, to explore the MH points. Further, to fully exploit MH\npoints in the learning process, we design a neighbor focal\nloss that assigns the weighted neighbor purity to the cross\nentropy loss of MH points to make the model focus more on\nthem. Extensive experiments verify that MHPL remarkably\nexceeds the various types of baselines and achieves signifi-\ncant performance gains at a small cost of labeling.\n",
        "question": {
            "statement": "What is the main advantage of Active Source-Free Domain Adaptation (ASFDA) over Source-Free Domain Adaptation (SFDA)?",
            "options": [
                "It can adapt to any type of target domain without additional training.",
                "It eliminates the need for a pre-trained source model.",
                "It does not require any labeled data from the target domain.",
                "It can break through the performance bottleneck by selecting and utilizing a small set of informative samples."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Shape, Pose, and Appearance from a Single Image\nvia Bootstrapped Radiance Field Inversion\nDario Pavllo1,2∗\nDavid Joseph Tan2\nMarie-Julie Rakotosaona2\nFederico Tombari2,3\n1ETH Zurich\n2Google\n3TU Munich\nReconstruction\nmodel\nImage\ncollection\nShape (SDF)\nPose\nAppearance\nInput image\nTriangle mesh\nTrain\nReal images\nSynthetic images\nInput\nOutput\nSurface\nNormals\nNovel\nView\nFigure 1. Given a collection of 2D images representing a speciﬁc category (e.g. cars), we learn a model that can fully recover shape, pose,\nand appearance from a single image, without leveraging multiple views during training. The 3D shape is parameterized as a signed distance\nfunction (SDF), which facilitates its transformation to a triangle mesh for further downstream applications.\nAbstract\nNeural Radiance Fields (NeRF) coupled with GANs rep-\nresent a promising direction in the area of 3D reconstruc-\ntion from a single view, owing to their ability to efﬁciently\nmodel arbitrary topologies. Recent work in this area, how-\never, has mostly focused on synthetic datasets where ex-\nact ground-truth poses are known, and has overlooked\npose estimation, which is important for certain down-\nstream applications such as augmented reality (AR) and\nrobotics. We introduce a principled end-to-end reconstruc-\ntion framework for natural images, where accurate ground-\ntruth poses are not available. Our approach recovers an\nSDF-parameterized 3D shape, pose, and appearance from a\nsingle image of an object, without exploiting multiple views\nduring training. More speciﬁcally, we leverage an uncondi-\ntional 3D-aware generator, to which we apply a hybrid in-\nversion scheme where a model produces a ﬁrst guess of the\nsolution which is then reﬁned via optimization. Our frame-\nwork can de-render an image in as few as 10 steps, enabling\nits use in practical scenarios. We demonstrate state-of-the-\nart results on a variety of real and synthetic benchmarks.\n∗Work done during an internship at Google.\n",
        "question": {
            "statement": "What is the primary advantage of using a Signed Distance Function (SDF) to represent 3D shapes?",
            "options": [
                "It improves the quality of generated images",
                "It enables more accurate pose estimation",
                "It reduces the computational cost of radiance field inversion",
                "It allows for efficient transformation into a triangle mesh"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "RGB no more: Minimally-decoded JPEG Vision Transformers\nJeongsoo Park\nJustin Johnson\nUniversity of Michigan\n{jespark, justincj}@umich.edu\nAbstract\nMost neural networks for computer vision are designed\nto infer using RGB images. However, these RGB images are\ncommonly encoded in JPEG before saving to disk; decoding\nthem imposes an unavoidable overhead for RGB networks.\nInstead, our work focuses on training Vision Transformers\n(ViT) directly from the encoded features of JPEG. This way,\nwe can avoid most of the decoding overhead, accelerating\ndata load. Existing works have studied this aspect but they\nfocus on CNNs. Due to how these encoded features are\nstructured, CNNs require heavy modification to their archi-\ntecture to accept such data. Here, we show that this is not\nthe case for ViTs. In addition, we tackle data augmentation\ndirectly on these encoded features, which to our knowledge,\nhas not been explored in-depth for training in this setting.\nWith these two improvements – ViT and data augmentation\n– we show that our ViT-Ti model achieves up to 39.2% faster\ntraining and 17.9% faster inference with no accuracy loss\ncompared to the RGB counterpart. 1\n",
        "question": {
            "statement": "What advantage does training Vision Transformers (ViT) directly from the encoded features of JPEG images offer compared to traditional approaches?",
            "options": [
                "Improved image quality",
                "Increased model complexity",
                "Enhanced feature extraction capabilities",
                "Faster data loading and reduced decoding overhead"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "2",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Real-Time Neural Light Field on Mobile Devices\nJunli Cao1\nHuan Wang2\nPavlo Chemerys1\nVladislav Shakhrai1\nJu Hu1\nYun Fu2\nDenys Makoviichuk1\nSergey Tulyakov1\nJian Ren1\n1Snap Inc.\n2Northeastern University\nAbstract\nRecent efforts in Neural Rendering Fields (NeRF) have\nshown impressive results on novel view synthesis by utiliz-\ning implicit neural representation to represent 3D scenes.\nDue to the process of volumetric rendering, the inference\nspeed for NeRF is extremely slow, limiting the application\nscenarios of utilizing NeRF on resource-constrained hard-\nware, such as mobile devices. Many works have been con-\nducted to reduce the latency of running NeRF models. How-\never, most of them still require high-end GPU for accel-\neration or extra storage memory, which is all unavailable\non mobile devices. Another emerging direction utilizes the\nneural light ﬁeld (NeLF) for speedup, as only one forward\npass is performed on a ray to predict the pixel color. Nev-\nertheless, to reach a similar rendering quality as NeRF, the\nnetwork in NeLF is designed with intensive computation,\nwhich is not mobile-friendly. In this work, we propose an\nefﬁcient network that runs in real-time on mobile devices\nfor neural rendering. We follow the setting of NeLF to train\nour network. Unlike existing works, we introduce a novel\nnetwork architecture that runs efﬁciently on mobile devices\nwith low latency and small size, i.e., saving 15⇥⇠24⇥\nstorage compared with MobileNeRF. Our model achieves\nhigh-resolution generation while maintaining real-time in-\nference for both synthetic and real-world scenes on mo-\nbile devices, e.g., 18.04ms (iPhone 13) for rendering one\n1008 ⇥756 image of real 3D scenes.\nAdditionally, we\nachieve similar image quality as NeRF and better quality\nthan MobileNeRF (PSNR 26.15 vs. 25.91 on the real-world\nforward-facing dataset)1.\n",
        "question": {
            "statement": "What is the main limitation of using Neural Rendering Fields (NeRF) on mobile devices?",
            "options": [
                "The network architecture is too complex",
                "The inference speed is extremely slow",
                "The training data is insufficient",
                "The required storage memory is too large"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "OCELOT: Overlapped Cell on Tissue Dataset for Histopathology\nJeongun Ryu∗Aaron Valero Puche∗JaeWoong Shin∗Seonwook Park Biagio Brattoli Jinhee Lee\nWonkyung Jung Soo Ick Cho Kyunghyun Paeng Chan-Young Ock Donggeun Yoo Sérgio Pereira\nLunit Inc.\n{rjw0205, aaron.valero, jwoong.shin, spark, biagio, jinhee.lee,\nwkjung, sooickcho, khpaeng, ock.chanyoung, dgyoo, sergio}@lunit.io\nAbstract\nCell detection is a fundamental task in computational\npathology that can be used for extracting high-level medical\ninformation from whole-slide images. For accurate cell de-\ntection, pathologists often zoom out to understand the tissue-\nlevel structures and zoom in to classify cells based on their\nmorphology and the surrounding context. However, there is\na lack of eﬀorts to reﬂect such behaviors by pathologists in\nthe cell detection models, mainly due to the lack of datasets\ncontaining both cell and tissue annotations with overlap-\nping regions. To overcome this limitation, we propose and\npublicly release OCELOT, a dataset purposely dedicated\nto the study of cell-tissue relationships for cell detection in\nhistopathology. OCELOT provides overlapping cell and tis-\nsue annotations on images acquired from multiple organs.\nWithin this setting, we also propose multi-task learning ap-\nproaches that beneﬁt from learning both cell and tissue tasks\nsimultaneously. When compared against a model trained\nonly for the cell detection task, our proposed approaches\nimprove cell detection performance on 3 datasets: proposed\nOCELOT, public TIGER, and internal CARP datasets. On\nthe OCELOT test set in particular, we show up to 6.79\nimprovement in F1-score. We believe the contributions of\nthis paper, including the release of the OCELOT dataset\nat https://lunit-io.github.io/research/\npublications/ocelot are a crucial starting point to-\nward the important research direction of incorporating cell-\ntissue relationships in computation pathology.\n",
        "question": {
            "statement": "What is the main goal of creating the OCELOT dataset in histopathology?",
            "options": [
                "To focus solely on detecting cells in whole-slide images",
                "To develop a new approach for classifying tissues",
                "To improve cell detection performance by considering cell-tissue relationships",
                "To create a comprehensive database of tissue-level structures"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Improving Table Structure Recognition with Visual-Alignment Sequential\nCoordinate Modeling\nYongshuai Huang*,1\nNing Lu∗,1\nDapeng Chen1\nYibo Li2\nZecheng Xie1\nShenggao Zhu1\nLiangcai Gao2\nWei Peng1\n1 Huawei Technologies Ltd.\n2 Peking University\n{huangyongshuai1,luning12,chendapeng8,xiezecheng1,zhushenggao,peng.wei1}@huawei.com\n{yiboli,gaoliangcai}@pku.edu.cn\nAbstract\nTable structure recognition aims to extract the logical\nand physical structure of unstructured table images into\na machine-readable format. The latest end-to-end image-\nto-text approaches simultaneously predict the two struc-\ntures by two decoders, where the prediction of the physi-\ncal structure (the bounding boxes of the cells) is based on\nthe representation of the logical structure. However, the\nprevious methods struggle with imprecise bounding boxes\nas the logical representation lacks local visual informa-\ntion. To address this issue, we propose an end-to-end se-\nquential modeling framework for table structure recogni-\ntion called VAST. It contains a novel coordinate sequence\ndecoder triggered by the representation of the non-empty\ncell from the logical structure decoder. In the coordinate se-\nquence decoder, we model the bounding box coordinates as\na language sequence, where the left, top, right and bottom\ncoordinates are decoded sequentially to leverage the inter-\ncoordinate dependency. Furthermore, we propose an auxil-\niary visual-alignment loss to enforce the logical representa-\ntion of the non-empty cells to contain more local visual de-\ntails, which helps produce better cell bounding boxes. Ex-\ntensive experiments demonstrate that our proposed method\ncan achieve state-of-the-art results in both logical and phys-\nical structure recognition.\nThe ablation study also vali-\ndates that the proposed coordinate sequence decoder and\nthe visual-alignment loss are the keys to the success of our\nmethod.\n",
        "question": {
            "statement": "What is a major limitation of previous end-to-end image-to-text approaches for table structure recognition?",
            "options": [
                "Inability to handle complex tables",
                "Logical representation lacks local visual information",
                "Difficulty in distinguishing between different types of tables",
                "Insufficient training data"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Reproducible scaling laws for contrastive language-image learning\nMehdi Cherti1,5 §§\nRomain Beaumont1 §§\nRoss Wightman1,3 §§\nMitchell Wortsman4 §§\nGabriel Ilharco4 §§\nCade Gordon2\nChristoph Schuhmann1\nLudwig Schmidt1,4 °°\nJenia Jitsev1,5 §§°°\nLAION1\nUC Berkeley2\nHuggingFace3\nUniversity of Washington4\nJuelich Supercomputing Center (JSC), Research Center Juelich (FZJ)5\ncontact@laion.ai, {m.cherti,j.jitsev}@fz-juelich.de\n§§ Equal first contributions, °° Equal senior contributions\nAbstract\nScaling up neural networks has led to remarkable perfor-\nmance across a wide range of tasks. Moreover, performance\noften follows reliable scaling laws as a function of training\nset size, model size, and compute, which offers valuable guid-\nance as large-scale experiments are becoming increasingly\nexpensive. However, previous work on scaling laws has pri-\nmarily used private data & models or focused on uni-modal\nlanguage or vision learning. To address these limitations, we\ninvestigate scaling laws for contrastive language-image pre-\ntraining (CLIP) with the public LAION dataset and the open-\nsource OpenCLIP repository. Our large-scale experiments\ninvolve models trained on up to two billion image-text pairs\nand identify power law scaling for multiple downstream tasks\nincluding zero-shot classification, retrieval, linear probing,\nand end-to-end fine-tuning. We find that the training dis-\ntribution plays a key role in scaling laws as the OpenAI\nand OpenCLIP models exhibit different scaling behavior\ndespite identical model architectures and similar training\nrecipes. We open-source our evaluation workflow and all\nmodels, including the largest public CLIP models, to en-\nsure reproducibility and make scaling laws research more\naccessible. Source code and instructions to reproduce this\nstudy is available at https://github.com/LAION-\nAI/scaling-laws-openclip.\n",
        "question": {
            "statement": "What type of learning involves both language and image data?",
            "options": [
                "contrastive language-image",
                "uni-modal language",
                "end-to-end fine-tuning",
                "vision-only"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "AnchorFormer: Point Cloud Completion from Discriminative Nodes\nZhikai Chen, Fuchen Long, Zhaofan Qiu, Ting Yao, Wengang Zhou, Jiebo Luo and Tao Mei\nUniversity of Science and Technology of China, Hefei, China\nUniversity of Rochester, Rochester, NY USA\nHiDream.ai Inc.\nczk654@mail.ustc.edu.cn,\n{longfc.ustc, zhaofanqiu, tingyao.ustc}@gmail.com\nzhwg@ustc.edu.cn,\njluo@cs.rochester.edu,\ntmei@hidream.ai\nAbstract\nPoint cloud completion aims to recover the completed 3D\nshape of an object from its partial observation. A common\nstrategy is to encode the observed points to a global fea-\nture vector and then predict the complete points through a\ngenerative process on this vector. Nevertheless, the results\nmay suffer from the high-quality shape generation problem\ndue to the fact that a global feature vector cannot sufficient-\nly characterize diverse patterns in one object. In this pa-\nper, we present a new shape completion architecture, name-\nly AnchorFormer, that innovatively leverages pattern-aware\ndiscriminative nodes, i.e., anchors, to dynamically capture\nregional information of objects. Technically, AnchorFormer\nmodels the regional discrimination by learning a set of an-\nchors based on the point features of the input partial ob-\nservation. Such anchors are scattered to both observed and\nunobserved locations through estimating particular offset-\ns, and form sparse points together with the down-sampled\npoints of the input observation.\nTo reconstruct the fine-\ngrained object patterns, AnchorFormer further employs a\nmodulation scheme to morph a canonical 2D grid at in-\ndividual locations of the sparse points into a detailed 3D\nstructure. Extensive experiments on the PCN, ShapeNet-\n55/34 and KITTI datasets quantitatively and qualitatively\ndemonstrate the efficacy of AnchorFormer over the state-of-\nthe-art point cloud completion approaches. Source code is\navailable at https://github.com/chenzhik/AnchorFormer.\n",
        "question": {
            "statement": "What is the main limitation of traditional point cloud completion methods that use a global feature vector?",
            "options": [
                "They require a large amount of computational resources",
                "They rely heavily on manual annotation",
                "They are limited to specific types of objects",
                "They struggle to capture diverse patterns within an object"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "3",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "CiCo: Domain-Aware Sign Language Retrieval via\nCross-Lingual Contrastive Learning\nYiting Cheng1,\nFangyun Wei2*\n,\nJianmin Bao2,\nDong Chen2,\nWenqiang Zhang1*\n1School of Computer Science, Fudan University,\n2Microsoft Research Asia\n{ytcheng18, wqzhang}@fudan.edu.cn,\n{fawe, jianbao, doch}@microsoft.com\nAbstract\nThis work focuses on sign language retrieval—a recently\nproposed task for sign language understanding. Sign lan-\nguage retrieval consists of two sub-tasks: text-to-sign-video\n(T2V) retrieval and sign-video-to-text (V2T) retrieval. Dif-\nferent from traditional video-text retrieval, sign language\nvideos, not only contain visual signals but also carry abun-\ndant semantic meanings by themselves due to the fact that\nsign languages are also natural languages. Considering\nthis character, we formulate sign language retrieval as a\ncross-lingual retrieval problem as well as a video-text re-\ntrieval task. Concretely, we take into account the linguistic\nproperties of both sign languages and natural languages,\nand simultaneously identify the fine-grained cross-lingual\n(i.e., sign-to-word) mappings while contrasting the texts\nand the sign videos in a joint embedding space. This pro-\ncess is termed as cross-lingual contrastive learning. An-\nother challenge is raised by the data scarcity issue—sign\nlanguage datasets are orders of magnitude smaller in scale\nthan that of speech recognition. We alleviate this issue by\nadopting a domain-agnostic sign encoder pre-trained on\nlarge-scale sign videos into the target domain via pseudo-\nlabeling. Our framework, termed as domain-aware sign\nlanguage retrieval via Cross-lingual Contrastive learning\nor CiCo for short, outperforms the pioneering method\nby large margins on various datasets, e.g., +22.4 T2V\nand +28.0 V2T R@1 improvements on How2Sign dataset,\nand +13.7 T2V and +17.1 V2T R@1 improvements on\nPHOENIX-2014T dataset. Code and models are available\nat: https://github.com/FangyunWei/SLRT.\n",
        "question": {
            "statement": "What characteristic of sign languages makes them particularly challenging for video-text retrieval tasks?",
            "options": [
                "They carry abundant semantic meanings by themselves",
                "They do not involve hand gestures",
                "They are not considered natural languages",
                "They are only used by deaf individuals"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Visual Prompt Tuning for Generative Transfer Learning\nKihyuk Sohn, Huiwen Chang, Jos´\ne Lezama, Luisa Polania,\nHan Zhang, Yuan Hao, Irfan Essa, Lu Jiang\nGoogle Research\nFigure 1. Image synthesis by knowledge transfer. Unlike previous works using GANs as base model and test transfer on relatively narrow\nvisual domains, we transfer knowledge of generative vision transformers [7, 15] to a wide range of visual domains, including natural\n(e.g., scene, ﬂower), specialized (e.g., satellite, medical), and structured (e.g., road scenes, infograph, sketch) with a few training images.\nNotably, the prompt tuning signiﬁcantly improves the prior best FID on two benchmarks ImageNet (85.9!16.3) and Places (71.3!24.2).\nAbstract\nLearning generative image models from various domains\nefﬁciently needs transferring knowledge from an image syn-\nthesis model trained on a large dataset. We present a recipe\nfor learning vision transformers by generative knowledge\ntransfer. We base our framework on generative vision trans-\nformers representing an image as a sequence of visual to-\nkens with the autoregressive or non-autoregressive trans-\nformers. To adapt to a new domain, we employ prompt tun-\ning, which prepends learnable tokens called prompts to the\nimage token sequence and introduces a new prompt design\nfor our task. We study on a variety of visual domains with\nvarying amounts of training images. We show the effective-\nness of knowledge transfer and a signiﬁcantly better image\ngeneration quality.1\n",
        "question": {
            "statement": "What technique is used to adapt a generative vision transformer to a new visual domain?",
            "options": [
                "autoregressive transformation",
                "prompt tuning",
                "knowledge distillation",
                "non-autoregressive transformation"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "L-CoIns: Language-based Colorization with Instance Awareness\nZheng Chang#1\nShuchen Weng#2,3\nPeixuan Zhang1\nYu Li4\nSi Li∗1\nBoxin Shi2,3\n1School of Artiﬁcial Intelligence, Beijing University of Posts and Telecommunications\n2National Key Laboratory for Multimedia Information Processing, School of Computer Science, Peking University\n3National Engineering Research Center of Visual Technology, School of Computer Science, Peking University\n4International Digital Economy Academy\n{zhengchang98,pxzhang,lisi}@bupt.edu.cn, {shuchenweng,shiboxin}@pku.edu.cn, liyu@idea.edu.cn\nThe right woman is dressed in shirt of violet color.\nL-CoDe\nL-CoDer\nOurs\nML2018\nThe middle woman is dressed in orange and the right woman in yellow.\nL-CoDe\nL-CoDer\nOurs\nML2018\nThree women are dressed in pink.\nL-CoDe\nL-CoDer\nOurs\nML2018\nThe middle woman is dressed in yellow, the left woman in blue, the right woman in red.\nL-CoDe\nL-CoDer\nOurs\nML2018\nGrayscale\nFigure 1. Language-based colorization results given four different language descriptions, compared with ML2018 [29], L-CoDe [42], and\nL-CoDer [6]. Top left: For the description that has clear correspondences between color words and object words, our method correctly\ncolorizes all corresponding regions. Top right: For the description that assigns distinct colors for every instance corresponding to the\nsame object words, our model predicts the exact correspondence between the instance region and the color word. Botton left:\nFor\nthe description that includes unobserved correspondences between color words and object words, our method could adaptively parse the\nsentence and determine the correct semantics for colorization. Bottom right: For the description that is against the statistical correlation\nbetween luminance and color words, our method shows the robustness and colorize description-consistent results.\nAbstract\nLanguage-based colorization produces plausible col-\nors consistent with the language description provided by\nthe user.\nRecent studies introduce additional annotation\nto prevent color-object coupling and mismatch issues, but\nthey still have difﬁculty in distinguishing instances corre-\nsponding to the same object words. In this paper, we pro-\npose a transformer-based framework to automatically ag-\ngregate similar image patches and achieve instance aware-\nness without any additional knowledge. By applying our\npresented luminance augmentation and counter-color loss\nto break down the statistical correlation between luminance\nand color words, our model is driven to synthesize colors\nwith better descriptive consistency.\nWe further collect a\ndataset to provide distinctive visual characteristics and de-\ntailed language descriptions for multiple instances in the\n# Equal contributions. * Corresponding author.\nsame image. Extensive experiments demonstrate our ad-\nvantages of synthesizing visually pleasing and description-\nconsistent results of instance-aware colorization.\n",
        "question": {
            "statement": "What is the main goal of language-based colorization?",
            "options": [
                "To generate random colors for an image",
                "To identify objects in an image based on their color",
                "Produce plausible colors consistent with the language description provided by the user.",
                "To convert a colored image into grayscale"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "3D Line Mapping Revisited\nShaohui Liu1\nYifan Yu1\nR´\nemi Pautrat1\nMarc Pollefeys1,2\nViktor Larsson3\n1Department of Computer Science, ETH Zurich\n2Microsoft\n3Lund University\nAbstract\nIn contrast to sparse keypoints, a handful of line segments\ncan concisely encode the high-level scene layout, as they\noften delineate the main structural elements. In addition to\noffering strong geometric cues, they are also omnipresent in\nurban landscapes and indoor scenes. Despite their appar-\nent advantages, current line-based reconstruction methods\nare far behind their point-based counterparts. In this paper\nwe aim to close the gap by introducing LIMAP, a library\nfor 3D line mapping that robustly and efficiently creates\n3D line maps from multi-view imagery. This is achieved\nthrough revisiting the degeneracy problem of line triangu-\nlation, carefully crafted scoring and track building, and\nexploiting structural priors such as line coincidence, paral-\nlelism, and orthogonality. Our code integrates seamlessly\nwith existing point-based Structure-from-Motion methods\nand can leverage their 3D points to further improve the line\nreconstruction. Furthermore, as a byproduct, the method\nis able to recover 3D association graphs between lines and\npoints / vanishing points (VPs). In thorough experiments,\nwe show that LIMAP significantly outperforms existing ap-\nproaches for 3D line mapping. Our robust 3D line maps\nalso open up new research directions. We show two exam-\nple applications: visual localization and bundle adjustment,\nwhere integrating lines alongside points yields the best re-\nsults. Code is available at https://github.com/cvg/limap.\n",
        "question": {
            "statement": "What is a key advantage of using line segments instead of sparse keypoints in 3D scene reconstruction?",
            "options": [
                "They require less computational resources.",
                "They are more abundant in natural scenes.",
                "They provide more accurate depth information.",
                "They often delineate the main structural elements."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "3",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Weakly Supervised Semantic Segmentation via\nAdversarial Learning of Classifier and Reconstructor\nHyeokjun Kweon∗, Sung-Hoon Yoon∗, and Kuk-Jin Yoon\nVisual Intelligence Lab., KAIST, Korea\n{0327june,yoon307,kjyoon}@kaist.ac.kr\nAbstract\nIn Weakly Supervised Semantic Segmentation (WSSS),\nClass Activation Maps (CAMs) usually 1) do not cover\nthe whole object and 2) be activated on irrelevant regions.\nTo address the issues, we propose a novel WSSS frame-\nwork via adversarial learning of a classifier and an im-\nage reconstructor. When an image is perfectly decomposed\ninto class-wise segments, information (i.e., color or tex-\nture) of a single segment could not be inferred from the\nother segments.\nTherefore, inferability between the seg-\nments can represent the preciseness of segmentation. We\nquantify the inferability as a reconstruction quality of one\nsegment from the other segments.\nIf one segment could\nbe reconstructed from the others, then the segment would\nbe imprecise. To bring this idea into WSSS, we simulta-\nneously train two models: a classifier generating CAMs\nthat decompose an image into segments and a reconstruc-\ntor that measures the inferability between the segments.\nAs in GANs, while being alternatively trained in an ad-\nversarial manner, two networks provide positive feedback\nto each other. We verify the superiority of the proposed\nframework with extensive ablation studies.\nOur method\nachieves new state-of-the-art performances on both PAS-\nCAL VOC 2012 and MS COCO 2014. The code is available\nat https://github.com/sangrockEG/ACR.\n",
        "question": {
            "statement": "What is the main goal of using the concept of 'inferability' in weakly supervised semantic segmentation?",
            "options": [
                "To reconstruct the original image",
                "To classify objects in the image",
                "To measure the preciseness of segmentation",
                "To generate Class Activation Maps (CAMs)"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "2",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Watch or Listen: Robust Audio-Visual Speech Recognition\nwith Visual Corruption Modeling and Reliability Scoring\nJoanna Hong*\nMinsu Kim*\nJeongsoo Choi\nYong Man Ro†\nImage and Video Systems Lab, KAIST\n{joanna2587, ms.k, jeongsoo.choi, ymro}@kaist.ac.kr\nAbstract\nThis paper deals with Audio-Visual Speech Recognition\n(AVSR) under multimodal input corruption situations where\naudio inputs and visual inputs are both corrupted, which is\nnot well addressed in previous research directions. Previ-\nous studies have focused on how to complement the cor-\nrupted audio inputs with the clean visual inputs with the\nassumption of the availability of clean visual inputs. How-\never, in real life, clean visual inputs are not always acces-\nsible and can even be corrupted by occluded lip regions or\nnoises. Thus, we firstly analyze that the previous AVSR mod-\nels are not indeed robust to the corruption of multimodal\ninput streams, the audio and the visual inputs, compared to\nuni-modal models. Then, we design multimodal input cor-\nruption modeling to develop robust AVSR models. Lastly,\nwe propose a novel AVSR framework, namely Audio-Visual\nReliability Scoring module (AV-RelScore), that is robust to\nthe corrupted multimodal inputs. The AV-RelScore can de-\ntermine which input modal stream is reliable or not for the\nprediction and also can exploit the more reliable streams\nin prediction.\nThe effectiveness of the proposed method\nis evaluated with comprehensive experiments on popular\nbenchmark databases, LRS2 and LRS3. We also show that\nthe reliability scores obtained by AV-RelScore well reflect\nthe degree of corruption and make the proposed model fo-\ncus on the reliable multimodal representations.\n",
        "question": {
            "statement": "What is a limitation of traditional Audio-Visual Speech Recognition models?",
            "options": [
                "They are not effective in noisy environments",
                "They are limited to recognizing single-speaker audio",
                "They require manual annotation of audio inputs",
                "They assume clean visual inputs are always available."
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Neural Lens Modeling\nWenqi Xian1,∗\nAljaˇ\nz Boˇ\nziˇ\nc2\nNoah Snavely3\nChristoph Lassner4\nMeta Reality Labs Research1,2,4\nCornell University1,3\nwx97@cornell.edu1, aljaz@meta.com2, snavely@cs.cornell.edu3, classner@meta.com4\nAbstract\nRecent methods for 3D reconstruction and rendering in-\ncreasingly benefit from end-to-end optimization of the entire\nimage formation process. However, this approach is cur-\nrently limited: effects of the optical hardware stack and in\nparticular lenses are hard to model in a unified way. This\nlimits the quality that can be achieved for camera calibra-\ntion and the fidelity of the results of 3D reconstruction. In\nthis paper, we propose NeuroLens, a neural lens model for\ndistortion and vignetting that can be used for point projec-\ntion and ray casting and can be optimized through both\noperations. This means that it can (optionally) be used to\nperform pre-capture calibration using classical calibration\ntargets, and can later be used to perform calibration or re-\nfinement during 3D reconstruction, e.g., while optimizing a\nradiance field. To evaluate the performance of our proposed\nmodel, we create a comprehensive dataset assembled from\nthe Lensfun database with a multitude of lenses. Using this\nand other real-world datasets, we show that the quality of\nour proposed lens model outperforms standard packages\nas well as recent approaches while being much easier to\nuse and extend. The model generalizes across many lens\ntypes and is trivial to integrate into existing 3D reconstruc-\ntion and rendering systems. Visit our project website at:\nhttps://neural-lens.github.io.\n",
        "question": {
            "statement": "What is a limitation of current end-to-end optimization methods for 3D reconstruction and rendering?",
            "options": [
                "Lack of data for training",
                "Effects of optical hardware stack and lenses are hard to model",
                "Inability to handle complex scenes",
                "Insufficient computing power"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "EFEM: Equivariant Neural Field Expectation Maximization\nfor 3D Object Segmentation Without Scene Supervision\nJiahui Lei1\nCongyue Deng2\nKarl Schmeckpeper1\nLeonidas Guibas2\nKostas Daniilidis1\n1 University of Pennsylvania\n2 Stanford University\n{leijh, karls, kostas}@cis.upenn.edu, {congyue, guibas}@cs.stanford.edu\nAbstract\nWe introduce Equivariant Neural Field Expectation\nMaximization (EFEM), a simple, effective, and robust ge-\nometric algorithm that can segment objects in 3D scenes\nwithout annotations or training on scenes.\nWe achieve\nsuch unsupervised segmentation by exploiting single ob-\nject shape priors. We make two novel steps in that direc-\ntion. First, we introduce equivariant shape representations\nto this problem to eliminate the complexity induced by the\nvariation in object configuration. Second, we propose a\nnovel EM algorithm that can iteratively refine segmenta-\ntion masks using the equivariant shape prior. We collect\na novel real dataset Chairs and Mugs that contains vari-\nous object configurations and novel scenes in order to verify\nthe effectiveness and robustness of our method. Experimen-\ntal results demonstrate that our method achieves consis-\ntent and robust performance across different scenes where\nthe (weakly) supervised methods may fail. Code and data\navailable at https://www.cis.upenn.edu/˜leijh/\nprojects/efem\n",
        "question": {
            "statement": "What is the main advantage of using equivariant shape representations in 3D object segmentation?",
            "options": [
                "They improve the accuracy of scene supervision",
                "They eliminate the complexity induced by variation in object configuration",
                "They require less training data",
                "They enable faster processing times"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "3",
                "2"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Movies2Scenes: Using Movie Metadata to Learn Scene Representation\nShixing Chen\nChun-Hao Liu\nXiang Hao\nXiaohan Nie\nMaxim Arap\nRaffay Hamid\nAmazon Prime Video\n{shixic, chunhaol, xianghao, nxiaohan, maxarap, raffay}@amazon.com\nAbstract\nUnderstanding scenes in movies is crucial for a variety of\napplications such as video moderation, search, and recom-\nmendation. However, labeling individual scenes is a time-\nconsuming process. In contrast, movie level metadata (e.g.,\ngenre, synopsis, etc.) regularly gets produced as part of\nthe film production process, and is therefore significantly\nmore commonly available. In this work, we propose a novel\ncontrastive learning approach that uses movie metadata to\nlearn a general-purpose scene representation. Specifically,\nwe use movie metadata to define a measure of movie sim-\nilarity, and use it during contrastive learning to limit our\nsearch for positive scene-pairs to only the movies that are\nconsidered similar to each other. Our learned scene repre-\nsentation consistently outperforms existing state-of-the-art\nmethods on a diverse set of tasks evaluated using multiple\nbenchmark datasets. Notably, our learned representation\noffers an average improvement of 7.9% on the seven classi-\nfication tasks and 9.7% improvement on the two regression\ntasks in LVU dataset. Furthermore, using a newly collected\nmovie dataset, we present comparative results of our scene\nrepresentation on a set of video moderation tasks to demon-\nstrate its generalizability on previously less explored tasks.\n",
        "question": {
            "statement": "What is the main advantage of using movie-level metadata for scene representation?",
            "options": [
                "It provides more detailed information about individual scenes",
                "It requires less computational resources",
                "It is more readily available",
                "It is more accurate than manual labeling"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Meta-Learning with a Geometry-Adaptive Preconditioner\nSuhyun Kang1*\n, Duhun Hwang1*\n, Moonjung Eo1, Taesup Kim2, Wonjong Rhee1,3†\n1 Department of Intelligence and Information, Seoul National University\n2 Graduate School of Data Science, Seoul National University\n3 IPAI and AIIS, Seoul National University\n{su_hyun, yelobean, eod87, taesup.kim, wrhee}@snu.ac.kr\nAbstract\nModel-agnostic meta-learning (MAML) is one of the most\nsuccessful meta-learning algorithms. It has a bi-level opti-\nmization structure where the outer-loop process learns a\nshared initialization and the inner-loop process optimizes\ntask-specific weights. Although MAML relies on the stan-\ndard gradient descent in the inner-loop, recent studies have\nshown that controlling the inner-loop’s gradient descent with\na meta-learned preconditioner can be beneficial. Existing\npreconditioners, however, cannot simultaneously adapt in\na task-specific and path-dependent way. Additionally, they\ndo not satisfy the Riemannian metric condition, which can\nenable the steepest descent learning with preconditioned\ngradient. In this study, we propose Geometry-Adaptive Pre-\nconditioned gradient descent (GAP) that can overcome the\nlimitations in MAML; GAP can efficiently meta-learn a\npreconditioner that is dependent on task-specific param-\neters, and its preconditioner can be shown to be a Rieman-\nnian metric. Thanks to the two properties, the geometry-\nadaptive preconditioner is effective for improving the inner-\nloop optimization. Experiment results show that GAP out-\nperforms the state-of-the-art MAML family and precondi-\ntioned gradient descent-MAML (PGD-MAML) family in a\nvariety of few-shot learning tasks. Code is available at:\nhttps://github.com/Suhyun777/CVPR23-GAP.\n",
        "question": {
            "statement": "What is a key limitation of existing preconditioners in model-agnostic meta-learning?",
            "options": [
                "They require manual tuning of hyperparameters",
                "They cannot adapt simultaneously in a task-specific and path-dependent way",
                "They are only applicable to computer vision tasks",
                "They are incompatible with stochastic gradient descent"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Task Difficulty Aware Parameter Allocation & Regularization for Lifelong\nLearning\nWenjin Wang, Yunqing Hu, Qianglong Chen, Yin Zhang*\nZhejiang University, Hangzhou, China\n{wangwenjin,yunqinghu,chenqianglong,zhangyin98}@zju.edu.cn\nAbstract\nParameter regularization or allocation methods are ef-\nfective in overcoming catastrophic forgetting in lifelong\nlearning. However, they solve all tasks in a sequence uni-\nformly and ignore the differences in the learning difficulty of\ndifferent tasks. So parameter regularization methods face\nsignificant forgetting when learning a new task very dif-\nferent from learned tasks, and parameter allocation meth-\nods face unnecessary parameter overhead when learning\nsimple tasks.\nIn this paper, we propose the Parameter\nAllocation & Regularization (PAR), which adaptively select\nan appropriate strategy for each task from parameter allo-\ncation and regularization based on its learning difficulty.\nA task is easy for a model that has learned tasks related\nto it and vice versa. We propose a divergence estimation\nmethod based on the Nearest-Prototype distance to mea-\nsure the task relatedness using only features of the new task.\nMoreover, we propose a time-efficient relatedness-aware\nsampling-based architecture search strategy to reduce the\nparameter overhead for allocation. Experimental results\non multiple benchmarks demonstrate that, compared with\nSOTAs, our method is scalable and significantly reduces\nthe model’s redundancy while improving the model’s per-\nformance. Further qualitative analysis indicates that PAR\nobtains reasonable task-relatedness.\n",
        "question": {
            "statement": "What is a limitation of traditional parameter regularization and allocation methods in lifelong learning?",
            "options": [
                "They require a fixed number of parameters for all tasks",
                "They fail to account for differences in learning difficulty between tasks",
                "They cannot handle tasks with varying input dimensions",
                "They are only effective for simple tasks"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "7",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 3,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Adaptive Sparse Pairwise Loss for Object Re-Identification\nXiao Zhou1\nYujie Zhong2†\nZhen Cheng1†\nFan Liang2\nLin Ma2\n1Department of Automation, BNRist, Tsinghua University, Beijing 100084\n2 Meituan Inc.\nzhouxiao17@mails.tsinghua.edu.cn, jaszhong@hotmail.com, zcheng@mail.tsinghua.edu.cn\nAbstract\nObject re-identification (ReID) aims to find instances\nwith the same identity as the given probe from a large\ngallery. Pairwise losses play an important role in training\na strong ReID network. Existing pairwise losses densely\nexploit each instance as an anchor and sample its triplets\nin a mini-batch. This dense sampling mechanism inevitably\nintroduces positive pairs that share few visual similarities,\nwhich can be harmful to the training. To address this prob-\nlem, we propose a novel loss paradigm termed Sparse Pair-\nwise (SP) loss that only leverages few appropriate pairs\nfor each class in a mini-batch, and empirically demon-\nstrate that it is sufficient for the ReID tasks.\nBased on\nthe proposed loss framework, we propose an adaptive pos-\nitive mining strategy that can dynamically adapt to diverse\nintra-class variations. Extensive experiments show that SP\nloss and its adaptive variant AdaSP loss outperform other\npairwise losses, and achieve state-of-the-art performance\nacross several ReID benchmarks.\nCode is available at\nhttps://github.com/Astaxanthin/AdaSP.\n",
        "question": {
            "statement": "What is a limitation of traditional pairwise losses in object re-identification tasks?",
            "options": [
                "They are sensitive to hyperparameter tuning",
                "They introduce positive pairs with few visual similarities",
                "They require large amounts of annotated data",
                "They are computationally expensive"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "2",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Affection: Learning Affective Explanations for Real-World Visual Data\nPanos Achlioptas1,3\nMaks Ovsjanikov2\nLeonidas Guibas3\nSergey Tulyakov1\n1Snap Inc.\n2LIX, Ecole Polytechnique, IP Paris\n3Stanford University\nAbstract\nIn this work, we explore the space of emotional reac-\ntions induced by real-world images. For this, we first in-\ntroduce a large-scale dataset that contains both categorical\nemotional reactions and free-form textual explanations for\n85,007 publicly available images, analyzed by 6,283 annota-\ntors who were asked to indicate and explain how and why\nthey felt when observing a particular image, with a total\nof 526,749 responses. Although emotional reactions are\nsubjective and sensitive to context (personal mood, social\nstatus, past experiences) – we show that there is significant\ncommon ground to capture emotional responses with a large\nsupport in the subject population. In light of this observa-\ntion, we ask the following questions: i) Can we develop\nneural networks that provide plausible affective responses\nto real-world visual data explained with language? ii) Can\nwe steer such methods towards producing explanations with\nvarying degrees of pragmatic language, justifying different\nemotional reactions by grounding them in the visual stimu-\nlus? Finally, iii) How to evaluate the performance of such\nmethods for this novel task? In this work, we take the first\nsteps in addressing all of these questions, paving the way\nfor more human-centric and emotionally-aware image anal-\nysis systems. Our code and data are publicly available at\nhttps://affective-explanations.org.\n",
        "question": {
            "statement": "What is a key aspect of emotional reactions to visual data that makes developing neural networks to predict them challenging?",
            "options": [
                "They are subjective and influenced by personal factors",
                "They are based on objective measurements",
                "They are universally agreed upon",
                "They are only influenced by the visual stimulus"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Neural Rate Estimator and Unsupervised Learning for Efficient Distributed\nImage Analytics in Split-DNN models\nNilesh Ahuja1, Parual Datta1, Bhavya Kanzariya1,2, V. Srinivasa Somayazulu1, Omesh Tickoo1\n1 Intel Labs, 2 IIT Hyderabad\n{nilesh.ahuja, parual.datta, bhavya.kanzariya, v.srinivasa.somayazulu, omesh.tickoo}@intel.com\nAbstract\nThanks to advances in computer vision and AI, there\nhas been a large growth in the demand for cloud-based vi-\nsual analytics in which images captured by a low-powered\nedge device are transmitted to the cloud for analytics. Use\nof conventional codecs (JPEG, MPEG, HEVC, etc.)\nfor\ncompressing such data introduces artifacts that can seri-\nously degrade the performance of the downstream analytic\ntasks. Split-DNN computing has emerged as a paradigm\nto address such usages, in which a DNN is partitioned\ninto a client-side portion and a server side portion. Low-\ncomplexity neural networks called ‘bottleneck units’ are in-\ntroduced at the split point to transform the intermediate\nlayer features into a lower-dimensional representation bet-\nter suited for compression and transmission. Optimizing\nthe pipeline for both compression and task-performance re-\nquires high-quality estimates of the information-theoretic\nrate of the intermediate features. Most works on compres-\nsion for image analytics use heuristic approaches to esti-\nmate the rate, leading to suboptimal performance. We pro-\npose a high-quality ‘neural rate-estimator’ to address this\ngap.\nWe interpret the lower-dimensional bottleneck out-\nput as a latent representation of the intermediate feature\nand cast the rate-distortion optimization problem as one\nof training an equivalent variational auto-encoder with an\nappropriate loss function. We show that this leads to im-\nproved rate-distortion outcomes. We further show that re-\nplacing supervised loss terms (such as cross-entropy loss)\nby distillation-based losses in a teacher-student framework\nallows for unsupervised training of bottleneck units without\nthe need for explicit training labels. This makes our method\nvery attractive for real world deployments where access to\nlabeled training data is difficult or expensive. We demon-\nstrate that our method outperforms several state-of-the-art\nmethods by obtaining improved task accuracy at lower bi-\ntrates on image classification and semantic segmentation\ntasks.\n",
        "question": {
            "statement": "What is the main goal of using bottleneck units in Split-DNN models?",
            "options": [
                "To enable supervised learning on unlabeled data",
                "To improve the performance of edge devices",
                "To increase the complexity of neural networks",
                "To reduce the dimensionality of intermediate layer features for efficient compression and transmission"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation with\nNatural Prompts\nHan Liu1, Yuhao Wu1, Shixuan Zhai1, Bo Yuan2, Ning Zhang1\n1Washington University in St. Louis, 2Rutgers University\n{h.liu1,yuhao.wu,zhais,zhang.ning}@wustl.edu,bo.yuan@soe.rutgers.edu\nAbstract\nThe field of text-to-image generation has made remark-\nable strides in creating high-fidelity and photorealistic im-\nages. As this technology gains popularity, there is a grow-\ning concern about its potential security risks.\nHowever,\nthere has been limited exploration into the robustness of\nthese models from an adversarial perspective. Existing re-\nsearch has primarily focused on untargeted settings, and\nlacks holistic consideration for reliability (attack success\nrate) and stealthiness (imperceptibility).\nIn this paper, we propose RIATIG, a reliable and im-\nperceptible adversarial attack against text-to-image mod-\nels via inconspicuous examples. By formulating the exam-\nple crafting as an optimization process and solving it using\na genetic-based method, our proposed attack can generate\nimperceptible prompts for text-to-image generation models\nin a reliable way. Evaluation of six popular text-to-image\ngeneration models demonstrates the efficiency and stealthi-\nness of our attack in both white-box and black-box settings.\nTo allow the community to build on top of our findings,\nwe’ve made the artifacts available1.\n",
        "question": {
            "statement": "What is a major concern regarding the increasing popularity of text-to-image generation technology?",
            "options": [
                "Limited creative possibilities",
                "Security risks",
                "Higher cost of implementation",
                "Increased computational power requirements"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Class Prototypes based Contrastive Learning for Classifying Multi-Label and\nFine-Grained Educational Videos\nRohit Gupta 1 *\nAnirban Roy 2\nClaire Christensen 2\nSujeong Kim 2\nSarah Gerard 2\nMadeline Cincebeaux 2\nAjay Divakaran 2\nTodd Grindal 2\nMubarak Shah 1\n1 Center for Research in Computer Vision, University of Central Florida\nrohitg@knights.ucf.edu, shah@crcv.ucf.edu\n2 SRI International\nanirban.roy@sri.com\nAbstract\nThe recent growth in the consumption of online media\nby children during early childhood necessitates data-driven\ntools enabling educators to filter out appropriate educa-\ntional content for young learners. This paper presents an\napproach for detecting educational content in online videos.\nWe focus on two widely used educational content classes:\nliteracy and math. For each class, we choose prominent\ncodes (sub-classes) based on the Common Core Standards.\nFor example, literacy codes include ‘letter names’, ‘letter\nsounds’, and math codes include ‘counting’, ‘sorting’. We\npose this as a fine-grained multilabel classification problem\nas videos can contain multiple types of educational content\nand the content classes can get visually similar (e.g., ‘letter\nnames’ vs ‘letter sounds’). We propose a novel class proto-\ntypes based supervised contrastive learning approach that\ncan handle fine-grained samples associated with multiple\nlabels. We learn a class prototype for each class and a loss\nfunction is employed to minimize the distances between a\nclass prototype and the samples from the class. Similarly,\ndistances between a class prototype and the samples from\nother classes are maximized. As the alignment between vi-\nsual and audio cues are crucial for effective comprehen-\nsion, we consider a multimodal transformer network to cap-\nture the interaction between visual and audio cues in videos\nwhile learning the embedding for videos. For evaluation,\nwe present a dataset, APPROVE, employing educational\nvideos from YouTube labeled with fine-grained education\nclasses by education researchers.\nAPPROVE consists of\n193 hours of expert-annotated videos with 19 classes. The\nproposed approach outperforms strong baselines on AP-\nPROVE and other benchmarks such as Youtube-8M, and\nCOIN. The dataset is available at https://nusci.\ncsl.sri.com/project/APPROVE.\n*Work partly done during an internship at SRI International.\n",
        "question": {
            "statement": "What type of classification problem does the proposed approach address, where videos can contain multiple types of educational content?",
            "options": [
                "coarse-grained classification",
                "single-label classification",
                "binary classification",
                "fine-grained multilabel"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Swept-Angle Synthetic Wavelength Interferometry\nAlankar Kotwal1, Anat Levin2, and Ioannis Gkioulekas1\n1Carnegie Mellon University, 2Technion\nAbstract\nWe present a new imaging technique, swept-angle syn-\nthetic wavelength interferometry, for full-ﬁeld micron-scale\n3D sensing. As in conventional synthetic wavelength interfer-\nometry, our technique uses light consisting of two narrowly-\nseparated optical wavelengths, resulting in per-pixel inter-\nferometric measurements whose phase encodes scene depth.\nOur technique additionally uses a new type of light source\nthat, by emulating spatially-incoherent illumination, makes\ninterferometric measurements insensitive to aberrations and\n(sub)surface scattering, effects that corrupt phase measure-\nments. The resulting technique combines the robustness to\nsuch corruptions of scanning interferometric setups, with\nthe speed of full-ﬁeld interferometric setups. Overall, our\ntechnique can recover full-frame depth at a lateral and ax-\nial resolution of 5 µm, at frame rates of 5 Hz, even under\nstrong ambient light. We build an experimental prototype,\nand use it to demonstrate these capabilities by scanning a\nvariety of objects, including objects representative of ap-\nplications in inspection and fabrication, and objects that\ncontain challenging light scattering effects.\n",
        "question": {
            "statement": "What is the main advantage of using a light source that emulates spatially-incoherent illumination in interferometric measurements?",
            "options": [
                "It increases the frame rate of the measurements",
                "It requires a narrower separation between the optical wavelengths",
                "It reduces the axial resolution of the measurements",
                "It makes the measurements insensitive to aberrations and subsurface scattering"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Seeing What You Miss: Vision-Language Pre-training with\nSemantic Completion Learning\nYatai Ji1*\nRongcheng Tu2*\nJie Jiang2*\nWeijie Kong2\nChengfei Cai2\nWenzhe Zhao2\nHongfa Wang2\nYujiu Yang1†\nWei Liu2†\n1Tsinghua University\n2Tencent\njyt21@mails.tsinghua.edu.cn\nturongcheng@gmail.com\n{zeus, jacobkong, fletchercai, carsonzhao, hongfawang}@tencent.com\nyang.yujiu@sz.tsinghua.edu.cn\nwl2223@columbia.edu\nAbstract\nCross-modal alignment is essential for vision-language\npre-training (VLP) models to learn the correct correspond-\ning information across different modalities. For this purpose,\ninspired by the success of masked language modeling (MLM)\ntasks in the NLP pre-training area, numerous masked mod-\neling tasks have been proposed for VLP to further promote\ncross-modal interactions. The core idea of previous masked\nmodeling tasks is to focus on reconstructing the masked\ntokens based on visible context for learning local-to-local\nalignment. However, most of them pay little attention to\nthe global semantic features generated for the masked data,\nresulting in a limited cross-modal alignment ability of global\nrepresentations. Therefore, in this paper, we propose a novel\nSemantic Completion Learning (SCL) task, complementary\nto existing masked modeling tasks, to facilitate global-to-\nlocal alignment. Specifically, the SCL task complements the\nmissing semantics of masked data by capturing the corre-\nsponding information from the other modality, promoting\nlearning more representative global features which have a\ngreat impact on the performance of downstream tasks. More-\nover, we present a flexible vision encoder, which enables\nour model to perform image-text and video-text multimodal\ntasks simultaneously. Experimental results show that our\nproposed method obtains state-of-the-art performance on\nvarious vision-language benchmarks, such as visual ques-\ntion answering, image-text retrieval, and video-text retrieval.\n",
        "question": {
            "statement": "What is a limitation of traditional masked modeling tasks in vision-language pre-training?",
            "options": [
                "They require additional annotated data",
                "They are too computationally expensive",
                "They fail to capture global semantic features",
                "They are only applicable to image-text tasks"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory\nSanthosh Kumar Ramakrishnan1\nZiad Al-Halah2\nKristen Grauman1,3\n1UT Austin\n2University of Utah\n3FAIR, Meta AI\nAbstract\nSearching long egocentric videos with natural language\nqueries (NLQ) has compelling applications in augmented\nreality and robotics, where a fluid index into everything\nthat a person (agent) has seen before could augment human\nmemory and surface relevant information on demand. How-\never, the structured nature of the learning problem (free-\nform text query inputs, localized video temporal window\noutputs) and its needle-in-a-haystack nature makes it both\ntechnically challenging and expensive to supervise. We in-\ntroduce Narrations-as-Queries (NaQ), a data augmentation\nstrategy that transforms standard video-text narrations into\ntraining data for a video query localization model. Vali-\ndating our idea on the Ego4D benchmark, we find it has\ntremendous impact in practice. NaQ improves multiple top\nmodels by substantial margins (even doubling their accu-\nracy), and yields the very best results to date on the Ego4D\nNLQ challenge, soundly outperforming all challenge win-\nners in the CVPR and ECCV 2022 competitions and top-\nping the current public leaderboard.\nBeyond achieving\nthe state-of-the-art for NLQ, we also demonstrate unique\nproperties of our approach such as the ability to perform\nzero-shot and few-shot NLQ, and improved performance on\nqueries about long-tail object categories. Code and mod-\nels: http://vision.cs.utexas.edu/projects/naq.\n",
        "question": {
            "statement": "What is the main goal of using narrations as queries in episodic memory?",
            "options": [
                "to enable real-time video translation",
                "to reduce the cost of video storage",
                "to improve the accuracy of video query localization models",
                "to enhance the quality of video recordings"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial\nRobustness and Generalization\nZiquan Liu1, Yi Xu2*\n, Xiangyang Ji3 and Antoni B. Chan1∗\n1Department of Computer Science, City University of Hong Kong\n2School of Artificial Intelligence, Dalian University of Technology\n3Department of Automation, Tsinghua University\nziquanliu.cs@gmail.com, yxu@dlut.edu.cn, xyji@tsinghua.edu.cn, abchan@cityu.edu.hk\nAbstract\nRecent years have seen the ever-increasing importance\nof pre-trained models and their downstream training in\ndeep learning research and applications. At the same time,\nthe defense for adversarial examples has been mainly inves-\ntigated in the context of training from random initialization\non simple classification tasks. To better exploit the potential\nof pre-trained models in adversarial robustness, this paper\nfocuses on the fine-tuning of an adversarially pre-trained\nmodel in various classification tasks. Existing research has\nshown that since the robust pre-trained model has already\nlearned a robust feature extractor, the crucial question is\nhow to maintain the robustness in the pre-trained model\nwhen learning the downstream task. We study the model-\nbased and data-based approaches for this goal and find that\nthe two common approaches cannot achieve the objective of\nimproving both generalization and adversarial robustness.\nThus, we propose a novel statistics-based approach, Two-\nWIng NormliSation (TWINS) fine-tuning framework, which\nconsists of two neural networks where one of them keeps\nthe population means and variances of pre-training data in\nthe batch normalization layers. Besides the robust informa-\ntion transfer, TWINS increases the effective learning rate\nwithout hurting the training stability since the relationship\nbetween a weight norm and its gradient norm in standard\nbatch normalization layer is broken, resulting in a faster es-\ncape from the sub-optimal initialization and alleviating the\nrobust overfitting. Finally, TWINS is shown to be effective\non a wide range of image classification datasets in terms of\nboth generalization and robustness.\n",
        "question": {
            "statement": "What is a key challenge when fine-tuning a pre-trained model for adversarial robustness?",
            "options": [
                "Designing a complex neural network architecture",
                "Maintaining the robustness of the pre-trained model while learning the downstream task",
                "Collecting a large dataset for pre-training",
                "Achieving high accuracy on the pre-training task"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for\nGeneralized Novel Category Discovery\nSheng Zhang†",
        "question": {
            "statement": "What is the primary goal of Contrastive Affinity Learning?",
            "options": [
                "To discover novel categories in a generalized manner",
                "To reduce computational resources required for training",
                "To increase interpretability of machine learning models",
                "To improve model performance on existing categories"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Unsupervised Deep Asymmetric Stereo Matching with\nSpatially-Adaptive Self-Similarity\nTaeyong Song1\nSunok Kim2\nKwanghoon Sohn3,4\n1Hyundai Motor Company R&D Division, 2Korea Aerospace University,\n3Yonsei University, 4Korea Institute of Science and Technology (KIST)\ntaeyongsong@hyundai.com, sunok.kim@kau.ac.kr, khsohn@yonsei.ac.kr\nAbstract\nUnsupervised stereo matching has received a lot of atten-\ntion since it enables the learning of disparity estimation\nwithout ground-truth data.\nHowever, most of the un-\nsupervised stereo matching algorithms assume that the\nleft and right images have consistent visual properties,\ni.e., symmetric, and easily fail when the stereo images\nare asymmetric.\nIn this paper, we present a novel\nspatially-adaptive self-similarity (SASS) for unsupervised\nasymmetric stereo matching.\nIt extends the concept of\nself-similarity and generates deep features that are robust\nto the asymmetries.\nThe sampling patterns to calculate\nself-similarities are adaptively generated throughout the\nimage regions to effectively encode diverse patterns.\nIn\norder to learn the effective sampling patterns, we design\na contrastive similarity loss with positive and negative\nweights.\nConsequently, SASS is further encouraged to\nencode asymmetry-agnostic features, while maintaining\nthe distinctiveness for stereo correspondence. We present\nextensive experimental results including ablation studies\nand comparisons with different methods, demonstrating\neffectiveness of the proposed method under resolution and\nnoise asymmetries.\n",
        "question": {
            "statement": "What is a common assumption made by many unsupervised stereo matching algorithms?",
            "options": [
                "That the images are always noisy",
                "That left and right images have inconsistent visual properties",
                "That left and right images have consistent visual properties",
                "That the images are always high-resolution"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Learning Transferable Spatiotemporal Representations\nfrom Natural Script Knowledge\nZiyun Zeng1,2 *\nYuying Ge3 *\nXihui Liu3\nBin Chen4 ",
        "question": {
            "statement": "What type of knowledge is used to learn transferable spatiotemporal representations?",
            "options": [
                "Machine learning algorithms",
                "Computer vision",
                "Natural script",
                "Artificial intelligence"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations\nJoy Hsu\nStanford University\njoycj@stanford.edu\nJiayuan Mao\nMassachusetts Institute of Technology\njiayuanm@mit.edu\nJiajun Wu\nStanford University\njiajunwu@cs.stanford.edu\nAbstract\nGrounding object properties and relations in 3D scenes is\na prerequisite for a wide range of artificial intelligence tasks,\nsuch as visually grounded dialogues and embodied manipu-\nlation. However, the variability of the 3D domain induces\ntwo fundamental challenges: 1) the expense of labeling and\n2) the complexity of 3D grounded language. Hence, essential\ndesiderata for models are to be data-efficient, generalize to\ndifferent data distributions and tasks with unseen semantic\nforms, as well as ground complex language semantics (e.g.,\nview-point anchoring and multi-object reference). To ad-\ndress these challenges, we propose NS3D, a neuro-symbolic\nframework for 3D grounding. NS3D translates language into\nprograms with hierarchical structures by leveraging large\nlanguage-to-code models. Different functional modules in\nthe programs are implemented as neural networks. Notably,\nNS3D extends prior neuro-symbolic visual reasoning meth-\nods by introducing functional modules that effectively reason\nabout high-arity relations (i.e., relations among more than\ntwo objects), key in disambiguating objects in complex 3D\nscenes. Modular and compositional architecture enables\nNS3D to achieve state-of-the-art results on the ReferIt3D\nview-dependence task, a 3D referring expression compre-\nhension benchmark. Importantly, NS3D shows significantly\nimproved performance on settings of data-efficiency and gen-\neralization, and demonstrate zero-shot transfer to an unseen\n3D question-answering task.\n",
        "question": {
            "statement": "What is a major challenge in grounding object properties and relations in 3D scenes?",
            "options": [
                "complexity of 2D scenes",
                "lack of data availability",
                "expense of labeling",
                "variability of the 3D domain"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "5",
                "5",
                "10"
            ]
        },
        "difference": 5,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Neuron Structure Modeling for Generalizable Remote Physiological\nMeasurement\nHao LU1,2, Zitong YU3, Xuesong NIU4, Yingcong CHEN1,2,*\n1The Hong Kong University of Science & Technology (Guangzhou),\n2The Hong Kong University of Science & Technology, 3Great Bay University, 4Kuaishou Technology\nhlu585@connect.hkust-gz.edu.cn, zitong.yu@ieee.org,\nnxsedson@gmail.com, yingcongchen@ust.hk\nAbstract\nRemote photoplethysmography (rPPG) technology has\ndrawn increasing attention in recent years. It can extract\nBlood Volume Pulse (BVP) from facial videos, making many\napplications like health monitoring and emotional analysis\nmore accessible. However, as the BVP signal is easily af-\nfected by environmental changes, existing methods struggle\nto generalize well for unseen domains. In this paper, we sys-\ntematically address the domain shift problem in the rPPG\nmeasurement task. We show that most domain generaliza-\ntion methods do not work well in this problem, as domain la-\nbels are ambiguous in complicated environmental changes.\nIn light of this, we propose a domain-label-free approach\ncalled NEuron STructure modeling (NEST). NEST improves\nthe generalization capacity by maximizing the coverage of\nfeature space during training, which reduces the chance for\nunder-optimized feature activation during inference. Be-\nsides, NEST can also enrich and enhance domain invari-\nant features across multi-domain. We create and bench-\nmark a large-scale domain generalization protocol for the\nrPPG measurement task. Extensive experiments show that\nour approach outperforms the state-of-the-art methods on\nboth cross-dataset and intra-dataset settings. The codes are\navailable at https://github.com/LuPaoPao/NEST.\n",
        "question": {
            "statement": "What is a major challenge in remote photoplethysmography (rPPG) technology?",
            "options": [
                " Limited accessibility of facial videos",
                "High cost of equipment required",
                "Environmental changes affecting Blood Volume Pulse (BVP) signals",
                "Insufficient data for training models"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Improving Robustness of Vision Transformers by Reducing\nSensitivity to Patch Corruptions\nYong Guo, David Stutz, Bernt Schiele\nMax Planck Institute for Informatics, Saarland Informatics Campus\n{yongguo,david.stutz,schiele}@mpi-inf.mpg.de\nAbstract\nDespite their success, vision transformers still remain\nvulnerable to image corruptions, such as noise or blur. In-\ndeed, we find that the vulnerability mainly stems from the\nunstable self-attention mechanism, which is inherently built\nupon patch-based inputs and often becomes overly sensi-\ntive to the corruptions across patches. For example, when\nwe only occlude a small number of patches with random\nnoise (e.g., 10%), these patch corruptions would lead to se-\nvere accuracy drops and greatly distract intermediate at-\ntention layers. To address this, we propose a new training\nmethod that improves the robustness of transformers from a\nnew perspective – reducing sensitivity to patch corruptions\n(RSPC). Specifically, we first identify and occlude/corrupt\nthe most vulnerable patches and then explicitly reduce sen-\nsitivity to them by aligning the intermediate features be-\ntween clean and corrupted examples. We highlight that the\nconstruction of patch corruptions is learned adversarially\nto the following feature alignment process, which is partic-\nularly effective and essentially different from existing meth-\nods. In experiments, our RSPC greatly improves the sta-\nbility of attention layers and consistently yields better ro-\nbustness on various benchmarks, including CIFAR-10/100-\nC, ImageNet-A, ImageNet-C, and ImageNet-P.\n",
        "question": {
            "statement": "What is the main reason why vision transformers are vulnerable to image corruptions?",
            "options": [
                "Overfitting to clean images",
                "The unstable self-attention mechanism",
                "Inadequate model architecture",
                "Insufficient training data"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Deep Graph-based Spatial Consistency for Robust Non-rigid\nPoint Cloud Registration\nZheng Qin1 Hao Yu2 Changjian Wang1 Yuxing Peng1 Kai Xu1*\n1National University of Defense Technology\n2Technical University of Munich\nAbstract\nWe study the problem of outlier correspondence pruning\nfor non-rigid point cloud registration. In rigid registration,\nspatial consistency has been a commonly used criterion to\ndiscriminate outliers from inliers. It measures the compat-\nibility of two correspondences by the discrepancy between\nthe respective distances in two point clouds. However, spa-\ntial consistency no longer holds in non-rigid cases and out-\nlier rejection for non-rigid registration has not been well\nstudied. In this work, we propose Graph-based Spatial Con-\nsistency Network (GraphSCNet) to filter outliers for non-\nrigid registration. Our method is based on the fact that non-\nrigid deformations are usually locally rigid, or local shape\npreserving. We first design a local spatial consistency mea-\nsure over the deformation graph of the point cloud, which\nevaluates the spatial compatibility only between the corre-\nspondences in the vicinity of a graph node. An attention-\nbased non-rigid correspondence embedding module is then\ndevised to learn a robust representation of non-rigid cor-\nrespondences from local spatial consistency.\nDespite its\nsimplicity, GraphSCNet effectively improves the quality of\nthe putative correspondences and attains state-of-the-art\nperformance on three challenging benchmarks. Our code\nand models are available at https://github.com/\nqinzheng93/GraphSCNet.\n",
        "question": {
            "statement": "What assumption about non-rigid deformations is made in the proposed Graph-based Spatial Consistency Network?",
            "options": [
                "Non-rigid deformations are completely random",
                "Non-rigid deformations always preserve global shape",
                "Non-rigid deformations are usually locally rigid",
                "Non-rigid deformations never exhibit local rigidity"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "ALSO: Automotive Lidar Self-supervision by Occupancy estimation\nAlexandre Boulch1\nCorentin Sautier1,2\nBjörn Michele1,3\nGilles Puy1\nRenaud Marlet1,2\n1Valeo.ai, Paris, France\n2LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vallée, France\n3CNRS, IRISA, Univ. Bretagne Sud, Vannes, France\nAbstract\nWe propose a new self-supervised method for pre-training\nthe backbone of deep perception models operating on point\nclouds. The core idea is to train the model on a pretext\ntask which is the reconstruction of the surface on which\nthe 3D points are sampled, and to use the underlying latent\nvectors as input to the perception head. The intuition is\nthat if the network is able to reconstruct the scene surface,\ngiven only sparse input points, then it probably also captures\nsome fragments of semantic information, that can be used to\nboost an actual perception task. This principle has a very\nsimple formulation, which makes it both easy to implement\nand widely applicable to a large range of 3D sensors and\ndeep networks performing semantic segmentation or object\ndetection. In fact, it supports a single-stream pipeline, as\nopposed to most contrastive learning approaches, allowing\ntraining on limited resources. We conducted extensive exper-\niments on various autonomous driving datasets, involving\nvery different kinds of lidars, for both semantic segmenta-\ntion and object detection. The results show the effectiveness\nof our method to learn useful representations without any\nannotation, compared to existing approaches.\nThe code is available at github.com/valeoai/ALSO\n",
        "question": {
            "statement": "What is the main advantage of the ALSO method for training deep perception models on point clouds?",
            "options": [
                "It only works with specific types of lidars",
                "It requires annotated data",
                "It allows training on limited resources",
                "It needs a multi-stream pipeline"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "8",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "GFIE: A Dataset and Baseline for Gaze-Following from 2D to 3D in Indoor\nEnvironments\nZhengxi Hu1,2,3, Yuxue Yang1, Xiaolin Zhai1,2,3, Dingye Yang1,2,3, Bohan Zhou1, Jingtai Liu1,2,3\u0002\n1IRAIS, College of Artiﬁcial Intelligence, Nankai University\n2tjKLIR, Nankai University 3TBI center, Nankai University\n{hzx,yyx,zhaixiaolin,abandon,zhoubohan}@mail.nankai.edu.cn liujt@nankai.edu.cn\nAbstract\nGaze-following is a kind of research that requires locat-\ning where the person in the scene is looking automatically\nunder the topic of gaze estimation. It is an important clue\nfor understanding human intention, such as identifying ob-\njects or regions of interest to humans. However, a survey\nof datasets used for gaze-following tasks reveals defects in\nthe way they collect gaze point labels. Manual labeling may\nintroduce subjective bias and is labor-intensive, while auto-\nmatic labeling with an eye-tracking device would alter the\nperson’s appearance. In this work, we introduce GFIE, a\nnovel dataset recorded by a gaze data collection system we\ndeveloped. The system is constructed with two devices, an\nAzure Kinect and a laser rangeﬁnder, which generate the\nlaser spot to steer the subject’s attention as they perfor-\nm in front of the camera. And an algorithm is developed\nto locate laser spots in images for annotating 2D/3D gaze\ntargets and removing ground truth introduced by the spot-\ns. The whole procedure of collecting gaze behavior allows\nus to obtain unbiased labels in unconstrained environments\nsemi-automatically.\nWe also propose a baseline method\nwith stereo ﬁeld-of-view (FoV) perception for establishing\na 2D/3D gaze-following benchmark on the GFIE dataset.\nProject page: https://sites.google.com/view/\ngfie.\n",
        "question": {
            "statement": "What is a major limitation of current gaze-following datasets?",
            "options": [
                "The datasets are only annotated with 3D targets",
                "Manual labeling introduces subjective bias",
                "The datasets are limited to outdoor environments",
                "The datasets are too large to be managed"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Masked Video Distillation: Rethinking Masked Feature Modeling for\nSelf-supervised Video Representation Learning\nRui Wang1,2\nDongdong Chen3\nZuxuan Wu1,2†\nYinpeng Chen3\nXiyang Dai3\nMengchen Liu3\nLu Yuan3\nYu-Gang Jiang1,2†\n1Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University\n2Shanghai Collaborative Innovation Center of Intelligent Visual Computing\n3Microsoft Cloud + AI\nAbstract\nBenefiting from masked visual modeling, self-supervised\nvideo representation learning has achieved remarkable\nprogress. However, existing methods focus on learning rep-\nresentations from scratch through reconstructing low-level\nfeatures like raw pixel values. In this paper, we propose\nmasked video distillation (MVD), a simple yet effective two-\nstage masked feature modeling framework for video repre-\nsentation learning: firstly we pretrain an image (or video)\nmodel by recovering low-level features of masked patches,\nthen we use the resulting features as targets for masked fea-\nture modeling. For the choice of teacher models, we ob-\nserve that students taught by video teachers perform bet-\nter on temporally-heavy video tasks, while image teachers\ntransfer stronger spatial representations for spatially-heavy\nvideo tasks. Visualization analysis also indicates different\nteachers produce different learned patterns for students. To\nleverage the advantage of different teachers, we design a\nspatial-temporal co-teaching method for MVD. Specifically,\nwe distill student models from both video teachers and im-\nage teachers by masked feature modeling. Extensive ex-\nperimental results demonstrate that video transformers pre-\ntrained with spatial-temporal co-teaching outperform mod-\nels distilled with a single teacher on a multitude of video\ndatasets. Our MVD with vanilla ViT achieves state-of-the-\nart performance compared with previous methods on sev-\neral challenging video downstream tasks. For example, with\nthe ViT-Large model, our MVD achieves 86.4% and 76.7%\nTop-1 accuracy on Kinetics-400 and Something-Something-\nv2, outperforming VideoMAE by 1.2% and 2.4% respec-\ntively. When a larger ViT-Huge model is adopted, MVD\nachieves the state-of-the-art performance with 77.3% Top-1\naccuracy on Something-Something-v2. Code will be avail-\nable at https://github.com/ruiwang2021/mvd.\n†Corresponding authors\n0\n2000\n4000\n6000\n8000\nGFLOPs/Video\n67\n69\n71\n73\n75\n77\nSSv2 T\nop-1 Acc %\nMVD(ours)\nVideoMAE\nST-MAE\nOmniMAE\nBEVT\nMaskFeat\nMViTv1\nMViTv2\nVideoSwin\nMformer\nFigure 1. Comparisons of MVD with previous supervised or\nself-supervised methods on Something-Something v2.\nEach\nline represents the corresponding model of different sizes.\n",
        "question": {
            "statement": "What is the main idea behind the proposed masked video distillation (MVD) approach?",
            "options": [
                "Using a two-stage framework where an image or video model is first pretrained to recover low-level features, then these features are used as targets for masked feature modeling",
                "Using a single teacher model for both spatial and temporal feature learning",
                "Applying masked visual modeling directly to video data",
                "Reconstructing high-level semantic features from scratch"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "2",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Learning Transformations To Reduce the Geometric Shift in Object Detection\nVidit Vidit1 Martin Engilberge1 Mathieu Salzmann1,2\nCVLab, EPFL1, ClearSpace SA2\nfirstname.lastname@epfl.ch\nAbstract\nThe performance of modern object detectors drops when\nthe test distribution differs from the training one. Most of\nthe methods that address this focus on object appearance\nchanges caused by, e.g., different illumination conditions,\nor gaps between synthetic and real images. Here, by con-\ntrast, we tackle geometric shifts emerging from variations in\nthe image capture process, or due to the constraints of the\nenvironment causing differences in the apparent geometry\nof the content itself. We introduce a self-training approach\nthat learns a set of geometric transformations to minimize\nthese shifts without leveraging any labeled data in the new\ndomain, nor any information about the cameras. We evalu-\nate our method on two different shifts, i.e., a camera’s field\nof view (FoV) change and a viewpoint change. Our results\nevidence that learning geometric transformations helps de-\ntectors to perform better in the target domains.\n",
        "question": {
            "statement": "What type of changes in object detection are often overlooked despite being critical, according to recent research?",
            "options": [
                "Image quality degradation",
                "Geometric shifts",
                "Object appearance changes",
                "Illumination condition changes"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Shape-aware Text-driven Layered Video Editing\nYao-Chih Lee\nJi-Ze Genevieve Jang\nYi-Ting Chen\nElizabeth Qiu\nJia-Bin Huang\nUniversity of Maryland, College Park\nhttps://text-video-edit.github.io\nFigure 1. Shape-aware consistent video editing. Our method enables consistent text-guided video editing with both appearance and\nshape changes. The top row shows the input frames. The second and third rows present editing results from two text prompts: “running\nsports car” and “running minivan”, respectively. Note that text-driven editing involves both texture and structure editing on the\nforeground object. Our method performs consistent edits on sequential frames while preserving the object motion in the input video.\nAbstract\nTemporal consistency is essential for video editing appli-\ncations. Existing work on layered representation of videos\nallows propagating edits consistently to each frame. These\nmethods, however, can only edit object appearance rather\nthan object shape changes due to the limitation of using\na fixed UV mapping field for texture atlas.\nWe present\na shape-aware, text-driven video editing method to tackle\nthis challenge. To handle shape changes in video editing,\nwe first propagate the deformation field between the in-\nput and edited keyframe to all frames. We then leverage\na pre-trained text-conditioned diffusion model as guidance\nfor refining shape distortion and completing unseen regions.\nThe experimental results demonstrate that our method can\nachieve shape-aware consistent video editing and compare\nfavorably with the state-of-the-art.\n",
        "question": {
            "statement": "What is a major limitation of existing methods for layered representation of videos?",
            "options": [
                "They can only edit object appearance and not object shape",
                "They require manual editing of every frame",
                "They cannot handle complex textures",
                "They are limited to static objects"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "2",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Human-Art: A Versatile Human-Centric Dataset\nBridging Natural and Artificial Scenes\nXuan Ju1,2* † , Ailing Zeng1* ‡ , Jianan Wang1, Qiang Xu2, Lei Zhang1\n1International Digital Economy Academy, 2The Chinese University of Hong Kong\n{xju22, qxu}@cse.cuhk.edu.hk, {zengailing, wangjianan, leizhang}@idea.edu.cn\nhttps://github.com/IDEA-Research/HumanArt\nAbstract\nHumans have long been recorded in a variety of forms\nsince antiquity. For example, sculptures and paintings were\nthe primary media for depicting human beings before the in-\nvention of cameras. However, most current human-centric\ncomputer vision tasks like human pose estimation and hu-\nman image generation focus exclusively on natural images\nin the real world. Artificial humans, such as those in sculp-\ntures, paintings, and cartoons, are commonly neglected,\nmaking existing models fail in these scenarios.\nAs an abstraction of life, art incorporates humans in both\nnatural and artificial scenes. We take advantage of it and\nintroduce the Human-Art dataset to bridge related tasks in\nnatural and artificial scenarios. Specifically, Human-Art\ncontains 50k high-quality images with over 123k person\ninstances from 5 natural and 15 artificial scenarios, which\nare annotated with bounding boxes, keypoints, self-contact\npoints, and text information for humans represented in both\n2D and 3D. It is, therefore, comprehensive and versatile\nfor various downstream tasks. We also provide a rich set\nof baseline results and detailed analyses for related tasks,\nincluding human detection, 2D and 3D human pose estima-\ntion, image generation, and motion transfer. As a challeng-\ning dataset, we hope Human-Art can provide insights for\nrelevant research and open up new research questions.\n",
        "question": {
            "statement": "What type of data does the Human-Art dataset contain?",
            "options": [
                "videos",
                "audio files",
                "images",
                "text documents"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "3",
                "0"
            ]
        },
        "difference": 3,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Towards Realistic Long-Tailed Semi-Supervised Learning:\nConsistency Is All You Need\nTong Wei, Kai Gan\nSchool of Computer Science and Engineering, Southeast University, Nanjing 210096, China\n{weit, gank}@seu.edu.cn\nAbstract\nWhile long-tailed semi-supervised learning (LTSSL) has\nreceived tremendous attention in many real-world classi-\nfication problems, existing LTSSL algorithms typically as-\nsume that the class distributions of labeled and unlabeled\ndata are almost identical. Those LTSSL algorithms built\nupon the assumption can severely suffer when the class dis-\ntributions of labeled and unlabeled data are mismatched\nsince they utilize biased pseudo-labels from the model. To\nalleviate this issue, we propose a new simple method that\ncan effectively utilize unlabeled data of unknown class dis-\ntributions by introducing the adaptive consistency regular-\nizer (ACR). ACR realizes the dynamic refinery of pseudo-\nlabels for various distributions in a unified formula by esti-\nmating the true class distribution of unlabeled data. Despite\nits simplicity, we show that ACR achieves state-of-the-art\nperformance on a variety of standard LTSSL benchmarks,\ne.g., an averaged 10% absolute increase of test accuracy\nagainst existing algorithms when the class distributions of\nlabeled and unlabeled data are mismatched. Even when the\nclass distributions are identical, ACR consistently outper-\nforms many sophisticated LTSSL algorithms. We carry out\nextensive ablation studies to tease apart the factors that are\nmost important to ACR’s success. Source code is available\nat https://github.com/Gank0078/ACR.\n",
        "question": {
            "statement": "What is a common assumption made by existing long-tailed semi-supervised learning algorithms?",
            "options": [
                "The class distributions of labeled and unlabeled data are almost identical",
                "The class distributions of labeled and unlabeled data are randomly generated",
                "There is no correlation between the class distributions of labeled and unlabeled data",
                "The class distributions of labeled and unlabeled data are completely different"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Learning Articulated Shape with Keypoint Pseudo-labels from Web Images\nAnastasis Stathopoulos\nRutgers University\nGeorgios Pavlakos\nUC Berkeley\nLigong Han\nRutgers University\nDimitris Metaxas\nRutgers University\nAbstract\nThis paper shows that it is possible to learn models for\nmonocular 3D reconstruction of articulated objects (e.g.\nhorses, cows, sheep), using as few as 50-150 images la-\nbeled with 2D keypoints. Our proposed approach involves\ntraining category-specific keypoint estimators, generating\n2D keypoint pseudo-labels on unlabeled web images, and\nusing both the labeled and self-labeled sets to train 3D re-\nconstruction models. It is based on two key insights: (1)\n2D keypoint estimation networks trained on as few as 50-\n150 images of a given object category generalize well and\ngenerate reliable pseudo-labels; (2) a data selection mech-\nanism can automatically create a “curated” subset of the\nunlabeled web images that can be used for training – we\nevaluate four data selection methods. Coupling these two\ninsights enables us to train models that effectively utilize\nweb images, resulting in improved 3D reconstruction per-\nformance for several articulated object categories beyond\nthe fully-supervised baseline. Our approach can quickly\nbootstrap a model and requires only a few images labeled\nwith 2D keypoints. This requirement can be easily satisfied\nfor any new object category. To showcase the practicality\nof our approach for predicting the 3D shape of arbitrary\nobject categories, we annotate 2D keypoints on 250 giraffe\nand bear images from COCO in just 2.5 hours per category.\n",
        "question": {
            "statement": "What is the main advantage of the approach described in this paper for monocular 3D reconstruction of articulated objects?",
            "options": [
                "It does not require any labeled images at all",
                "It can accurately reconstruct 3D shapes from a single image",
                "It can only be used for specific object categories like horses and cows",
                "It can quickly bootstrap a model with only a few labeled images"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "3",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Event-Based Frame Interpolation with Ad-hoc Deblurring\nLei Sun1,2\nChristos Sakaridis2\nJingyun Liang2\nPeng Sun1\nJiezhang Cao2\nKai Zhang2\nQi Jiang1\nKaiwei Wang1\nLuc Van Gool2,3\n1Zhejiang University\n2ETH Z¨\nurich\n3KU Leuven\nAbstract\nThe performance of video frame interpolation is inher-\nently correlated with the ability to handle motion in the in-\nput scene. Even though previous works recognize the utility\nof asynchronous event information for this task, they ignore\nthe fact that motion may or may not result in blur in the\ninput video to be interpolated, depending on the length of\nthe exposure time of the frames and the speed of the motion,\nand assume either that the input video is sharp, restrict-\ning themselves to frame interpolation, or that it is blurry,\nincluding an explicit, separate deblurring stage before in-\nterpolation in their pipeline. We instead propose a general\nmethod for event-based frame interpolation that performs\ndeblurring ad-hoc and thus works both on sharp and blurry\ninput videos. Our model consists in a bidirectional recur-\nrent network that naturally incorporates the temporal di-\nmension of interpolation and fuses information from the in-\nput frames and the events adaptively based on their tempo-\nral proximity. In addition, we introduce a novel real-world\nhigh-resolution dataset with events and color videos named\nHighREV, which provides a challenging evaluation setting\nfor the examined task. Extensive experiments on the stan-\ndard GoPro benchmark and on our dataset show that our\nnetwork consistently outperforms previous state-of-the-art\nmethods on frame interpolation, single image deblurring\nand the joint task of interpolation and deblurring. Our code\nand dataset are available at https://github.com/\nAHupuJR/REFID.\n",
        "question": {
            "statement": "What is a key limitation of previous methods for video frame interpolation?",
            "options": [
                "They are unable to handle motion in the input scene",
                "They assume the input video is either always sharp or always blurry",
                "They require manual annotation of the input frames",
                "They are limited to low-resolution input videos"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Thermal Spread Functions (TSF): Physics-guided Material Classification\nAniket Dashpute1,3, Vishwanath Saragadam3, Emma Alexander2,\nFlorian Willomitzer4, Aggelos Katsaggelos1, Ashok Veeraraghavan3, Oliver Cossairt1,2\n1Electrical and Computer Engineering, 2Computer Science, Northwestern University,\n3Electrical and Computer Engineering, Rice University,\n4Wyant College of Optical Sciences, University of Arizona\nAbstract\nRobust and non-destructive material classification is a\nchallenging but crucial first-step in numerous vision appli-\ncations. We propose a physics-guided material classifica-\ntion framework that relies on thermal properties of the ob-\nject. Our key observation is that the rate of heating and\ncooling of an object depends on the unique intrinsic proper-\nties of the material, namely the emissivity and diffusivity. We\nleverage this observation by gently heating the objects in the\nscene with a low-power laser for a fixed duration and then\nturning it off, while a thermal camera captures measure-\nments during the heating and cooling process. We then take\nthis spatial and temporal “thermal spread function” (TSF)\nto solve an inverse heat equation using the finite-differences\napproach, resulting in a spatially varying estimate of dif-\nfusivity and emissivity. These tuples are then used to train\na classifier that produces a fine-grained material label at\neach spatial pixel. Our approach is extremely simple re-\nquiring only a small light source (low power laser) and a\nthermal camera, and produces robust classification results\nwith 86% accuracy over 16 classes1.\n",
        "question": {
            "statement": "What property of an object determines how quickly it heats up and cools down?",
            "options": [
                "reflectivity",
                "conductivity",
                "absorptivity",
                "diffusivity"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": false,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "5"
            ]
        },
        "difference": 5,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "PartDistillation: Learning Parts from Instance Segmentation\nJang Hyun Cho*\nUT Austin\njanghyuncho7@utexas.edu\nPhilipp Kr¨\nahenb¨\nuhl\nUT Austin\nphilkr@cs.utexas.edu\nVignesh Ramanathan\nMeta AI\nvigneshr@meta.com\n[t]\nFigure 1. PartDistillation distills a grouping of embedding distances of the penultimate later of a instance segmentation model into a part\nsegmentation model in a completely unsupervised manner. Above are some representative results obtained without any part supervision.\nAbstract\nWe present a scalable framework to learn part segmen-\ntation from object instance labels. State-of-the-art instance\nsegmentation models contain a surprising amount of part\ninformation. However, much of this information is hidden\nfrom plain view.\nFor each object instance, the part in-\nformation is noisy, inconsistent, and incomplete. PartDis-\ntillation transfers the part information of an instance seg-\nmentation model into a part segmentation model through\nself-supervised self-training on a large dataset. The result-\ning segmentation model is robust, accurate, and generalizes\nwell. We evaluate the model on various part segmentation\ndatasets. Our model outperforms supervised part segmen-\ntation in zero-shot generalization performance by a large\nmargin.\nOur model outperforms when finetuned on tar-\nget datasets compared to supervised counterpart and other\nbaselines especially in few-shot regime. Finally, our model\nprovides a wider coverage of rare parts when evaluated\nover 10K object classes. Code is at https://github.\ncom/facebookresearch/PartDistillation.\n*This work was done during Jang Hyun Cho’s internship at Meta AI.\n",
        "question": {
            "statement": "What is the primary goal of the PartDistillation framework?",
            "options": [
                "To improve the accuracy of instance segmentation models",
                "To reduce the computational cost of part segmentation models",
                "To transfer part information from an instance segmentation model to a part segmentation model",
                "To increase the number of object classes that can be segmented"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "High-Fidelity Clothed Avatar Reconstruction from a Single Image\nTingting Liao1,2*\nXiaomei Zhang1,2∗\nYuliang Xiu3\nHongwei Yi3\nXudong Liu4\nGuo-Jun Qi4,5\nYong Zhang6\nXuan Wang6\nXiangyu Zhu1,2\nZhen Lei1,2,7†\n1University of Chinese Academy of Sciences, Beijing, China\n2MAIS, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n3Max Planck Institute for Intelligent Systems, T¨\nubingen, Germany\n4OPPO Research\n5Westlake University\n6Tencent AI Lab\n7CAIR, HKISI, CAS\n{tingting.liao, xiaomei.zhang, xiangyu.zhu, zlei}@nlpr.ia.ac.cn\n{yuliang.xiu, hongwei.yi}@tuebingen.mpg.de\n{yongzhang201303, xwang.cv, guojunq}@gmail.com\n{xudong.liu}@oppo.com\nAbstract\nThis paper presents a framework for efficient 3D clothed\navatar reconstruction. By combining the advantages of the\nhigh accuracy of optimization-based methods and the ef-\nficiency of learning-based methods, we propose a coarse-\nto-fine way to realize a high-fidelity clothed avatar recon-\nstruction (CAR) from a single image. At the first stage, we\nuse an implicit model to learn the general shape in the\ncanonical space of a person in a learning-based way, and\nat the second stage, we refine the surface detail by esti-\nmating the non-rigid deformation in the posed space in an\noptimization way. A hyper-network is utilized to generate\na good initialization so that the convergence o f the opti-\nmization process is greatly accelerated. Extensive exper-\niments on various datasets show that the proposed CAR\nsuccessfully produces high-fidelity avatars for arbitrarily\nclothed humans in real scenes. The codes will be released in\nhttps://github.com/TingtingLiao/CAR.\n",
        "question": {
            "statement": "What approach does the proposed method use to achieve high-fidelity clothed avatar reconstruction?",
            "options": [
                "Employing solely learning-based methods",
                "Using only optimization-based methods",
                "Relying on manual input from users",
                "Combining optimization-based and learning-based methods"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Zero-shot Pose Transfer for Unrigged Stylized 3D Characters\nJiashun Wang1*\nXueting Li2\nSifei Liu2\nShalini De Mello2\nOrazio Gallo2\nXiaolong Wang3\nJan Kautz2\n1Carnegie Mellon University\n2NVIDIA\n3UC San Diego\nSource\nZero-shot pose transfer onto stylized characters\nFigure 1. Our algorithm transfers the pose of a reference avatar (source) to stylized characters. Unlike existing methods, at training time\nour approach needs only the mesh of the source avatar in rest and desired pose, and the mesh of the stylized characater only in rest pose.\nAbstract\nTransferring the pose of a reference avatar to stylized 3D\ncharacters of various shapes is a fundamental task in com-\nputer graphics. Existing methods either require the stylized\ncharacters to be rigged, or they use the stylized character in\nthe desired pose as ground truth at training. We present a\nzero-shot approach that requires only the widely available\ndeformed non-stylized avatars in training, and deforms styl-\nized characters of significantly different shapes at inference.\nClassical methods achieve strong generalization by deform-\ning the mesh at the triangle level, but this requires labelled\ncorrespondences. We leverage the power of local deforma-\ntion, but without requiring explicit correspondence labels.\nWe introduce a semi-supervised shape-understanding mod-\nule to bypass the need for explicit correspondences at test\ntime, and an implicit pose deformation module that deforms\nindividual surface points to match the target pose. Further-\nmore, to encourage realistic and accurate deformation of\n*Work done during Jiashun Wang’s internship at NVIDIA.\nstylized characters, we introduce an efficient volume-based\ntest-time training procedure. Because it does not need rig-\nging, nor the deformed stylized character at training time,\nour model generalizes to categories with scarce annotation,\nsuch as stylized quadrupeds. Extensive experiments demon-\nstrate the effectiveness of the proposed method compared\nto the state-of-the-art approaches trained with compara-\nble or more supervision. Our project page is available at\nhttps://jiashunwang.github.io/ZPT/\n",
        "question": {
            "statement": "What is a key advantage of the zero-shot pose transfer approach described in the paper?",
            "options": [
                "It needs explicit correspondence labels between the source and stylized characters",
                "It does not require the stylized character to be rigged",
                "It only works with human-shaped characters",
                "It requires the stylized character to be in the desired pose at training"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Unpaired Image-to-Image Translation with Shortest Path Regularization\nShaoan Xie 1 , Yanwu Xu2, Mingming Gong4,3, Kun Zhang1,3\n1Carnegie Mellon University\n2Boston University\n3Mohamed bin Zayed University of Artificial Intelligence\n4University of Melbourne\nshaoan@cmu.edu, yanwu@bu.edu, mingming.gong@unimelb.edu.au, kunz1@cmu.edu\nAbstract\nUnpaired image-to-image translation aims to learn\nproper mappings that can map images from one domain to\nanother domain while preserving the content of the input\nimage. However, with large enough capacities, the network\ncan learn to map the inputs to any random permutation of\nimages in another domain. Existing methods treat two do-\nmains as discrete and propose different assumptions to ad-\ndress this problem. In this paper, we start from a different\nperspective and consider the paths connecting the two do-\nmains. We assume that the optimal path length between\nthe input and output image should be the shortest among\nall possible paths. Based on this assumption, we propose a\nnew method to allow generating images along the path and\npresent a simple way to encourage the network to find the\nshortest path without pair information. Extensive experi-\nments on various tasks demonstrate the superiority of our\napproach. The code is available at https://github.com/Mid-\nPush/santa.\n",
        "question": {
            "statement": "What is a common issue in unpaired image-to-image translation tasks?",
            "options": [
                "The network fails to preserve the content of the input image",
                "The network requires paired data for training",
                "The network learns to map inputs to random permutations of images in another domain",
                "The network is unable to generate diverse outputs"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Learning to Name Classes for Vision and Language Models\nSarah Parisot\nYongxin Yang\nSteven McDonagh\nHuawei Noah’s Ark Lab\n{sarah.parisot, yongxin.yang, steven.mcdonagh}@huawei.com\nAbstract\nLarge scale vision and language models can achieve im-\npressive zero-shot recognition performance by mapping class\nspecific text queries to image content. Two distinct challenges\nthat remain however, are high sensitivity to the choice of hand-\ncrafted class names that define queries, and the difficulty of\nadaptation to new, smaller datasets. Towards addressing these\nproblems, we propose to leverage available data to learn, for\neach class, an optimal word embedding as a function of the\nvisual content. By learning new word embeddings on an other-\nwise frozen model, we are able to retain zero-shot capabilities\nfor new classes, easily adapt models to new datasets, and ad-\njust potentially erroneous, non-descriptive or ambiguous class\nnames. We show that our solution can easily be integrated in\nimage classification and object detection pipelines, yields sig-\nnificant performance gains in multiple scenarios and provides\ninsights into model biases and labelling errors.\n",
        "question": {
            "statement": "What is a major challenge in using large-scale vision and language models for zero-shot recognition?",
            "options": [
                "Inability to generalize to new datasets",
                "Limited availability of visual content",
                "High sensitivity to the choice of hand-crafted class names",
                "Insufficient training data for language models"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "5",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 5,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Change-Aware Sampling and Contrastive Learning for Satellite Images\nUtkarsh Mall\nBharath Hariharan\nKavita Bala\nCornell University\n{utkarshm, bharathh, kb}@cs.cornell.edu\nAbstract\nAutomatic remote sensing tools can help inform many\nlarge-scale challenges such as disaster management, cli-\nmate change, etc. While a vast amount of spatio-temporal\nsatellite image data is readily available, most of it remains\nunlabelled. Without labels, this data is not very useful for\nsupervised learning algorithms. Self-supervised learning\ninstead provides a way to learn effective representations for\nvarious downstream tasks without labels. In this work, we\nleverage characteristics unique to satellite images to learn\nbetter self-supervised features. Specifically, we use the tem-\nporal signal to contrast images with long-term and short-\nterm differences, and we leverage the fact that satellite im-\nages do not change frequently. Using these characteristics,\nwe formulate a new loss contrastive loss called Change-\nAware Contrastive (CACo) Loss. Further, we also present a\nnovel method of sampling different geographical regions.\nWe show that leveraging these properties leads to better\nperformance on diverse downstream tasks. For example,\nwe see a 6.5% relative improvement for semantic segmenta-\ntion and an 8.5% relative improvement for change detection\nover the best-performing baseline with our method.\n",
        "question": {
            "statement": "What is a major challenge in using satellite image data for supervised learning algorithms?",
            "options": [
                "The lack of labeled data",
                "The low resolution of satellite images",
                "The limited availability of satellite image data",
                "The high cost of satellite imaging"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Generative Semantic Segmentation\nJiaqi Chen1\nJiachen Lu1\nXiatian Zhu2\nLi Zhang1*\n1Fudan University\n2University of Surrey\nhttps://github.com/fudan-zvg/GSS\nAbstract\nWe present Generative Semantic Segmentation (GSS), a\ngenerative learning approach for semantic segmentation.\nUniquely, we cast semantic segmentation as an image-\nconditioned mask generation problem. This is achieved\nby replacing the conventional per-pixel discriminative learn-\ning with a latent prior learning process. Specifically, we\nmodel the variational posterior distribution of latent vari-\nables given the segmentation mask. To that end, the seg-\nmentation mask is expressed with a special type of image\n(dubbed as maskige). This posterior distribution allows to\ngenerate segmentation masks unconditionally. To achieve\nsemantic segmentation on a given image, we further intro-\nduce a conditioning network. It is optimized by minimizing\nthe divergence between the posterior distribution of maskige\n(i.e. segmentation masks) and the latent prior distribution\nof input training images. Extensive experiments on standard\nbenchmarks show that our GSS can perform competitively to\nprior art alternatives in the standard semantic segmentation\nsetting, whilst achieving a new state of the art in the more\nchallenging cross-domain setting.\n",
        "question": {
            "statement": "What is the key difference between traditional semantic segmentation approaches and the proposed Generative Semantic Segmentation method?",
            "options": [
                "It focuses on instance segmentation instead of semantic segmentation.",
                "It uses a different type of neural network architecture.",
                "It replaces per-pixel discriminative learning with a latent prior learning process.",
                "It requires additional labeled data for training."
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Unsupervised Sampling Promoting for Stochastic Human Trajectory Prediction\nGuangyi Chen*,♦,♣, Zhenhao Chen*,♦, Shunxing Fan♦, Kun Zhang ♦,♣\n♦Mohamed bin Zayed University of Artificial Intelligence\n♣Carnegie Mellon University\nAbstract\nThe indeterminate nature of human motion requires tra-\njectory prediction systems to use a probabilistic model to\nformulate the multi-modality phenomenon and infer a finite\nset of future trajectories. However, the inference processes\nof most existing methods rely on Monte Carlo random sam-\npling, which is insufficient to cover the realistic paths with\nfinite samples, due to the long tail effect of the predicted\ndistribution. To promote the sampling process of stochastic\nprediction, we propose a novel method, called BOsampler\n, to adaptively mine potential paths with Bayesian optimiza-\ntion in an unsupervised manner, as a sequential design strat-\negy in which new prediction is dependent on the previously\ndrawn samples. Specifically, we model the trajectory sam-\npling as a Gaussian process and construct an acquisition\nfunction to measure the potential sampling value. This ac-\nquisition function applies the original distribution as prior\nand encourages exploring paths in the long-tail region. This\nsampling method can be integrated with existing stochastic\npredictive models without retraining. Experimental results\non various baseline methods demonstrate the effectiveness\nof our method. The source code is released in this link.\n",
        "question": {
            "statement": "What is a limitation of using Monte Carlo random sampling in trajectory prediction systems?",
            "options": [
                "It fails to cover realistic paths with finite samples due to the long tail effect",
                "It is only applicable to linear trajectory predictions",
                "It requires a large amount of labeled data",
                "It is unable to formulate the multi-modality phenomenon"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "2",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Boundary Unlearning: Rapid Forgetting of Deep Networks via Shifting the\nDecision Boundary\nMin Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, Chen Wang\nHubei Key Laboratoryof Smart Internet Technology, School of EIC,\nHuazhong University of Science and Technology, Wuhan 430074, China\n{chenmin7,wzgao,liugaoyang,pkhust,chenwang}@hust.edu.cn\nAbstract\nThe practical needs of the “right to be forgotten” and\npoisoned data removal call for efﬁcient machine unlearn-\ning techniques, which enable machine learning models to\nunlearn, or to forget a fraction of training data and its lin-\neage. Recent studies on machine unlearning for deep neural\nnetworks (DNNs) attempt to destroy the inﬂuence of the for-\ngetting data by scrubbing the model parameters. However,\nit is prohibitively expensive due to the large dimension of\nthe parameter space. In this paper, we refocus our attention\nfrom the parameter space to the decision space of the DNN\nmodel, and propose Boundary Unlearning, a rapid yet ef-\nfective way to unlearn an entire class from a trained DNN\nmodel. The key idea is to shift the decision boundary of the\noriginal DNN model to imitate the decision behavior of the\nmodel retrained from scratch. We develop two novel bound-\nary shift methods, namely Boundary Shrink and Boundary\nExpanding, both of which can rapidly achieve the utility and\nprivacy guarantees. We extensively evaluate Boundary Un-\nlearning on CIFAR-10 and Vggface2 datasets, and the re-\nsults show that Boundary Unlearning can effectively forget\nthe forgetting class on image classiﬁcation and face recog-\nnition tasks, with an expected speed-up of 17× and 19×,\nrespectively, compared with retraining from the scratch.\n",
        "question": {
            "statement": "What is the main goal of'machine unlearning' in the context of deep neural networks?",
            "options": [
                "To remove the influence of specific training data",
                "To increase the size of the model's parameter space",
                "To add new classes to the classification task",
                "To improve the accuracy of the model"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Zero-Shot Everything Sketch-Based Image Retrieval, and in Explainable Style\nFengyin Lin1∗\nMingkang Li1∗\nDa Li2†\nTimothy Hospedales2,3\nYi-Zhe Song4\nYonggang Qi1\n1Beijing University of Posts and Telecommunications\n2Samsung AI Centre, Cambridge\n3University of Edinburgh\n4SketchX, CVSSP, University of Surrey\n{fylin,lmk,qiyg}@bupt.edu.cn\ndali.academic@gmail.com\nt.hospedales@ed.ac.uk\ny.song@surrey.ac.uk\nAbstract\nThis paper studies the problem of zero-short sketch-\nbased image retrieval (ZS-SBIR), however with two sig-\nnificant differentiators to prior art (i) we tackle all vari-\nants (inter-category, intra-category, and cross datasets) of\nZS-SBIR with just one network (“everything”), and (ii)\nwe would really like to understand how this sketch-photo\nmatching operates (“explainable”). Our key innovation lies\nwith the realization that such a cross-modal matching prob-\nlem could be reduced to comparisons of groups of key local\npatches – akin to the seasoned “bag-of-words” paradigm.\nJust with this change, we are able to achieve both of the\naforementioned goals, with the added benefit of no longer\nrequiring external semantic knowledge. Technically, ours is\na transformer-based cross-modal network, with three novel\ncomponents (i) a self-attention module with a learnable tok-\nenizer to produce visual tokens that correspond to the most\ninformative local regions, (ii) a cross-attention module to\ncompute local correspondences between the visual tokens\nacross two modalities, and finally (iii) a kernel-based rela-\ntion network to assemble local putative matches and pro-\nduce an overall similarity metric for a sketch-photo pair.\nExperiments show ours indeed delivers superior perfor-\nmances across all ZS-SBIR settings. The all important ex-\nplainable goal is elegantly achieved by visualizing cross-\nmodal token correspondences, and for the first time, via\nsketch to photo synthesis by universal replacement of all\nmatched photo patches. Code and model are available at\nhttps://github.com/buptLinfy/ZSE-SBIR.\n",
        "question": {
            "statement": "What is the main advantage of reducing the cross-modal matching problem to comparisons of groups of key local patches in the context of zero-shot sketch-based image retrieval?",
            "options": [
                "No longer requiring external semantic knowledge",
                "Reduced computational complexity",
                "Ability to handle only inter-category tasks",
                "Improved performance on intra-category tasks"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Learning Locally Editable Virtual Humans\nHsuan-I Ho\nLixin Xue\nJie Song\nOtmar Hilliges\nDepartment of Computer Science, ETH Z¨\nurich\nAbstract\nIn this paper, we propose a novel hybrid representa-\ntion and end-to-end trainable network architecture to model\nfully editable and customizable neural avatars. At the core\nof our work lies a representation that combines the mod-\neling power of neural fields with the ease of use and in-\nherent 3D consistency of skinned meshes. To this end, we\nconstruct a trainable feature codebook to store local geom-\netry and texture features on the vertices of a deformable\nbody model, thus exploiting its consistent topology under\narticulation.\nThis representation is then employed in a\ngenerative auto-decoder architecture that admits fitting to\nunseen scans and sampling of realistic avatars with var-\nied appearances and geometries. Furthermore, our repre-\nsentation allows local editing by swapping local features\nbetween 3D assets. To verify our method for avatar cre-\nation and editing, we contribute a new high-quality dataset,\ndubbed CustomHumans, for training and evaluation. Our\nexperiments quantitatively and qualitatively show that our\nmethod generates diverse detailed avatars and achieves bet-\nter model fitting performance compared to state-of-the-art\nmethods. Our code and dataset are available at https:\n//ait.ethz.ch/custom-humans.\n",
        "question": {
            "statement": "What is the main advantage of combining neural fields with skinned meshes in modeling virtual humans?",
            "options": [
                "It allows for both detailed geometry and texture features while maintaining 3D consistency",
                "It reduces the need for large datasets",
                "It enables faster rendering of virtual humans",
                "It makes the models more suitable for real-time applications"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "SeqTrack: Sequence to Sequence Learning for Visual Object Tracking\nXin Chen1, Houwen Peng2,†, Dong Wang1,†, Huchuan Lu1,3, Han Hu2\n1Dalian University of Technology\n2Microsoft Research\n3Peng Cheng Laboratory\nAbstract\nIn this paper, we present a new sequence-to-sequence\nlearning framework for visual tracking, dubbed SeqTrack.\nIt casts visual tracking as a sequence generation problem,\nwhich predicts object bounding boxes in an autoregres-\nsive fashion.\nThis is different from prior Siamese track-\ners and transformer trackers, which rely on designing com-\nplicated head networks, such as classification and regres-\nsion heads. SeqTrack only adopts a simple encoder-decoder\ntransformer architecture. The encoder extracts visual fea-\ntures with a bidirectional transformer, while the decoder\ngenerates a sequence of bounding box values autoregres-\nsively with a causal transformer.\nThe loss function is a\nplain cross-entropy. Such a sequence learning paradigm\nnot only simplifies tracking framework, but also achieves\ncompetitive performance on benchmarks. For instance, Se-\nqTrack gets 72.5% AUC on LaSOT, establishing a new state-\nof-the-art performance. Code and models are available at\nhttps://github.com/microsoft/VideoX.\n",
        "question": {
            "statement": "What type of architecture does the SeqTrack framework use?",
            "options": [
                "long short-term memory network",
                "encoder-decoder transformer",
                "recurrent neural network",
                "convolutional neural network"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "CloSET: Modeling Clothed Humans on Continuous Surface with\nExplicit Template Decomposition\nHongwen Zhang1 Siyou Lin1 Ruizhi Shao1 Yuxiang Zhang1 Zerong Zheng1\nHan Huang2 Yandong Guo2 Yebin Liu1\n1Tsinghua University\n2OPPO Research Institute\nAbstract\nCreating animatable avatars from static scans re-\nquires the modeling of clothing deformations in different\nposes. Existing learning-based methods typically add pose-\ndependent deformations upon a minimally-clothed mesh\ntemplate or a learned implicit template, which have limita-\ntions in capturing details or hinder end-to-end learning. In\nthis paper, we revisit point-based solutions and propose to\ndecompose explicit garment-related templates and then add\npose-dependent wrinkles to them. In this way, the clothing\ndeformations are disentangled such that the pose-dependent\nwrinkles can be better learned and applied to unseen poses.\nAdditionally, to tackle the seam artifact issues in recent\nstate-of-the-art point-based methods, we propose to learn\npoint features on a body surface, which establishes a contin-\nuous and compact feature space to capture the fine-grained\nand pose-dependent clothing geometry. To facilitate the re-\nsearch in this field, we also introduce a high-quality scan\ndataset of humans in real-world clothing. Our approach is\nvalidated on two existing datasets and our newly introduced\ndataset, showing better clothing deformation results in un-\nseen poses. The project page with code and dataset can be\nfound at https://www.liuyebin.com/closet.\n",
        "question": {
            "statement": "What is a common limitation of existing learning-based methods for creating animatable avatars from static scans?",
            "options": [
                "They rely heavily on physics-based simulations.",
                "They are unable to handle complex background scenes.",
                "They struggle to capture detailed clothing deformations.",
                "They require a large amount of manual annotation."
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Accelerating Vision-Language Pretraining with Free Language Modeling\nTeng Wang1,2†, Yixiao Ge3, Feng Zheng1,5∗, Ran Cheng1, Ying Shan3, Xiaohu Qie4, Ping Luo2,6\n1Southern University of Science and Technology 2The University of Hong Kong\n3ARC Lab, 4Tencent PCG\n5Peng Cheng Laboratory\n6Shanghai AI Laboratory\ntengwang@connect.hku.hk\n{yixiaoge, yingsshan, tigerqie}@tencent.com\nf.zheng@ieee.org\nranchengcn@gmail.com\npluo@cs.hku.hk\nAbstract\nThe state of the arts in vision-language pretraining\n(VLP) achieves exemplary performance but suffers from\nhigh training costs resulting from slow convergence and\nlong training time, especially on large-scale web datasets.\nAn essential obstacle to training efficiency lies in the entan-\ngled prediction rate (percentage of tokens for reconstruc-\ntion) and corruption rate (percentage of corrupted tokens)\nin masked language modeling (MLM), that is, a proper cor-\nruption rate is achieved at the cost of a large portion of\noutput tokens being excluded from prediction loss. To ac-\ncelerate the convergence of VLP, we propose a new pre-\ntraining task, namely, free language modeling (FLM), that\nenables a 100% prediction rate with arbitrary corruption\nrates. FLM successfully frees the prediction rate from the\ntie-up with the corruption rate while allowing the corrup-\ntion spans to be customized for each token to be predicted.\nFLM-trained models are encouraged to learn better and\nfaster given the same GPU time by exploiting bidirectional\ncontexts more flexibly. Extensive experiments show FLM\ncould achieve an impressive 2.5× pretraining time reduc-\ntion in comparison to the MLM-based methods, while keep-\ning competitive performance on both vision-language un-\nderstanding and generation tasks.\nCode will be public\nat https://github.com/TencentARC/FLM.\n",
        "question": {
            "statement": "What is the main limitation of masked language modeling (MLM) in vision-language pretraining?",
            "options": [
                "The model requires a large amount of labeled data",
                "The prediction rate is tied up with the corruption rate",
                "The model is too complex to implement",
                "The model is not suitable for small-scale datasets"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Active Exploration of Multimodal Complementarity for Few-Shot Action\nRecognition\nYuyang Wanyan1,2, Xiaoshan Yang1,2,3, Chaofan Chen4, Changsheng Xu1,2,3*\n1State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA)\n2School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS)\n3Peng Cheng Laboratory, China\n4School of Information Science and Technology, University of Science and Technology of China (USTC)\nwanyanyuyang2021@ia.ac.cn, xiaoshan.yang@nlpr.ia.ac.cn, chencfbupt@gmail.com, csxu@nlpr.ia.ac.cn\nAbstract\nRecently, few-shot action recognition receives increasing\nattention and achieves remarkable progress. However, pre-\nvious methods mainly rely on limited unimodal data (e.g.,\nRGB frames) while the multimodal information remains rel-\natively underexplored. In this paper, we propose a novel\nActive Multimodal Few-shot Action Recognition (AMFAR)\nframework, which can actively find the reliable modality for\neach sample based on task-dependent context information\nto improve few-shot reasoning procedure. In meta-training,\nwe design an Active Sample Selection (ASS) module to or-\nganize query samples with large differences in the reliabil-\nity of modalities into different groups based on modality-\nspecific posterior distributions. In addition, we design an\nActive Mutual Distillation (AMD) to capture discrimina-\ntive task-specific knowledge from the reliable modality to\nimprove the representation learning of unreliable modal-\nity by bidirectional knowledge distillation.\nIn meta-test,\nwe adopt Adaptive Multimodal Inference (AMI) to adap-\ntively fuse the modality-specific posterior distributions with\na larger weight on the reliable modality. Extensive experi-\nmental results on four public benchmarks demonstrate that\nour model achieves significant improvements over existing\nunimodal and multimodal methods.\n",
        "question": {
            "statement": "What is the main goal of the Active Multimodal Few-shot Action Recognition framework?",
            "options": [
                "To ignore multimodal information and rely on unimodal data",
                "To adaptively select and combine reliable modalities for each sample",
                "To focus solely on improving the performance of a single modality",
                "To use only RGB frames for action recognition"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "NeMo: 3D Neural Motion Fields from Multiple Video Instances of the Same Action\nKuan-Chieh Wang†, Zhenzhen Weng, Maria Xenochristou, João Pedro Araújo, Jeffrey Gu,\nC. Karen Liu‡, Serena Yeung⊺\nStanford University\n†wangkua1@stanford.edu,\n‡karenliu@cs.stanford.edu,\n⊺syyeung@stanford.edu\nVIBE\nNeMo\nVIBE\nNeMo\nBaseball Pitch\nTennis Serve\nFigure 1. We propose the Neural Motion (NeMo) field that learns the global 3D motion from multiple video instances of the same action.\nNeMo can accurately recover the dynamic ranges of athletic motion. To illustrate, VIBE, a baseline video-based HMR method, fails to capture\nthe large step taken by the subject in the “Baseball Pitch” example, and swaps the arms in “Tennis Serve”. Visit our project page for video\nvisualization, our code, and the NeMo-MoCap dataset: https://sites.google.com/view/nemo-neural-motion-field.\nAbstract\nThe task of reconstructing 3D human motion has wide-\nranging applications. The gold standard Motion capture\n(MoCap) systems are accurate but inaccessible to the general\npublic due to their cost, hardware, and space constraints. In\ncontrast, monocular human mesh recovery (HMR) methods\nare much more accessible than MoCap as they take single-\nview videos as inputs. Replacing the multi-view MoCap\nsystems with a monocular HMR method would break the cur-\nrent barriers to collecting accurate 3D motion thus making\nexciting applications like motion analysis and motion-driven\nanimation accessible to the general public. However, the\nperformance of existing HMR methods degrades when the\nvideo contains challenging and dynamic motion that is not\nin existing MoCap datasets used for training. This reduces\nits appeal as dynamic motion is frequently the target in 3D\nmotion recovery in the aforementioned applications. Our\nstudy aims to bridge the gap between monocular HMR and\nmulti-view MoCap systems by leveraging information shared\nacross multiple video instances of the same action. We in-\ntroduce the Neural Motion (NeMo) field. It is optimized to\nrepresent the underlying 3D motions across a set of videos\nof the same action. Empirically, we show that NeMo can re-\ncover 3D motion in sports using videos from the Penn Action\ndataset, where NeMo outperforms existing HMR methods in\nterms of 2D keypoint detection. To further validate NeMo\nusing 3D metrics, we collected a small MoCap dataset mim-\nicking actions in Penn Action, and show that NeMo achieves\nbetter 3D reconstruction compared to various baselines.\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n22129\n",
        "question": {
            "statement": "What is a major limitation of current monocular Human Mesh Recovery (HMR) methods?",
            "options": [
                "They struggle to capture dynamic and challenging motions",
                "They can only be applied to simple actions",
                "They are highly accurate in all scenarios",
                "They require expensive hardware"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "The Dark Side of Dynamic Routing Neural Networks: Towards Efﬁciency\nBackdoor Injection\nSimin Chen1\nHanlin Chen2\nMirazul Haque1\nCong Liu3\nWei Yang1\n1University of Texas at Dallas\n2Purdue University\n3University of California, Riverside\n{simin.chen, mirazul.haque, wei.yang}@utdallas.edu, chen1368@purdue.edu, congl@ucr.edu\nAbstract\nRecent advancements in deploying deep neural networks\n(DNNs) on resource-constrained devices have generated in-\nterest in input-adaptive dynamic neural networks (DyNNs).\nDyNNs offer more efﬁcient inferences and enable the de-\nployment of DNNs on devices with limited resources, such\nas mobile devices. However, we have discovered a new vul-\nnerability in DyNNs that could potentially compromise their\nefﬁciency. Speciﬁcally, we investigate whether adversaries\ncan manipulate DyNNs’ computational costs to create a\nfalse sense of efﬁciency. To address this question, we pro-\npose EfficFrog, an adversarial attack that injects uni-\nversal efﬁciency backdoors in DyNNs. To inject a backdoor\ntrigger into DyNNs, EfficFrog poisons only a minimal\npercentage of the DyNNs’ training data. During the infer-\nence phase, EfficFrog can slow down the backdoored\nDyNNs and abuse the computational resources of systems\nrunning DyNNs by adding the trigger to any input. To eval-\nuate EfficFrog, we tested it on three DNN backbone ar-\nchitectures (based on VGG16, MobileNet, and ResNet56)\nusing two popular datasets (CIFAR-10 and Tiny ImageNet).\nOur results demonstrate that EfficFrog reduces the efﬁ-\nciency of DyNNs on triggered input samples while keeping\nthe efﬁciency of clean samples almost the same.\n",
        "question": {
            "statement": "What type of vulnerability was discovered in dynamic neural networks (DyNNs)?",
            "options": [
                "Data poisoning attack",
                "Manipulation of computational costs",
                "Insufficient training data",
                "Overfitting issue"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "RealFusion\n360◦Reconstruction of Any Object from a Single Image\nLuke Melas-Kyriazi\nIro Laina\nChristian Rupprecht\nAndrea Vedaldi\nVisual Geometry Group, Department of Engineering Science, University of Oxford\n{lukemk,iro,chrisr,vedaldi}@robots.ox.ac.uk\nhttps://lukemelas.github.io/realfusion\nFigure 1. RealFusion generates a full 360◦reconstruction of any object given a single image of it (left column). It does so by leveraging\nan existing diffusion-based 2D image generator. From the given image, it synthesizes a prompt that causes the diffusion model to “dream\nup” other views of the object. It then extracts a neural radiance field from the original image and the diffusion model-based prior, thereby\nreconstructing the object in full. Both appearance and geometry are reconstructed faithfully and extrapolated in a plausible manner.\nAbstract\nWe consider the problem of reconstructing a full 360◦\nphotographic model of an object from a single image of it.\nWe do so by fitting a neural radiance field to the image,\nbut find this problem to be severely ill-posed. We thus take\nan off-the-self conditional image generator based on diffu-\nsion and engineer a prompt that encourages it to “dream\nup” novel views of the object. Using the recent DreamFu-\nsion method, we fuse the given input view, the conditional\nprior, and other regularizers into a final, consistent recon-\nstruction. We demonstrate state-of-the-art reconstruction\nresults on benchmark images when compared to prior meth-\nods for monocular 3D reconstruction of objects. Qualita-\ntively, our reconstructions provide a faithful match of the\ninput view and a plausible extrapolation of its appearance\nand 3D shape, including to the side of the object not visible\nin the image.\n",
        "question": {
            "statement": "What technique is used to generate novel views of an object from a single image?",
            "options": [
                "neural radiance fields",
                "diffusion-based image generation",
                "traditional computer vision techniques",
                "conditional prior modeling"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Power Bundle Adjustment for Large-Scale 3D Reconstruction\nSimon Weber1,2\nsim.weber@tum.de\nNikolaus Demmel1,2\nnikolaus.demmel@tum.de\nTin Chon Chan1,2\ntin-1254@hotmail.com\nDaniel Cremers1,2,3\ncremers@tum.de\nAbstract\nWe introduce Power Bundle Adjustment as an expansion\ntype algorithm for solving large-scale bundle adjustment\nproblems. It is based on the power series expansion of the\ninverse Schur complement and constitutes a new family of\nsolvers that we call inverse expansion methods. We theo-\nretically justify the use of power series and we prove the\nconvergence of our approach. Using the real-world BAL\ndataset we show that the proposed solver challenges the\nstate-of-the-art iterative methods and significantly acceler-\nates the solution of the normal equation, even for reaching a\nvery high accuracy. This easy-to-implement solver can also\ncomplement a recently presented distributed bundle adjust-\nment framework. We demonstrate that employing the pro-\nposed Power Bundle Adjustment as a sub-problem solver\nsignificantly improves speed and accuracy of the distributed\noptimization.\n",
        "question": {
            "statement": "What type of algorithm is Power Bundle Adjustment, which is introduced as an expansion for solving large-scale bundle adjustment problems?",
            "options": [
                "Inverse expansion method",
                "Iterative method",
                "Distributed optimization method",
                "Normal equation method"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Dynamic Inference with Grounding Based Vision and Language Models\nBurak Uzkent, Amanmeet Garg, Wentao Zhu, Keval Doshi, Jingru Yi, Xiaolong Wang, Mohamed Omar\nAmazon Prime Video\n{burauzke,amanmega,zhuwent,kcdos,jyijingr,xiaowanf,omarmk}@amazon.com\nAbstract\nTransformers have been recently utilized for vision and\nlanguage tasks successfully. For example, recent image and\nlanguage models with more than 200M parameters have\nbeen proposed to learn visual grounding in the pre-training\nstep and show impressive results on downstream vision and\nlanguage tasks.\nOn the other hand, there exists a large\namount of computational redundancy in these large models\nwhich skips their run-time efficiency. To address this prob-\nlem, we propose dynamic inference for grounding based vi-\nsion and language models conditioned on the input image-\ntext pair. We first design an approach to dynamically skip\nmultihead self-attention and feed forward network layers\nacross two backbones and multimodal network. Addition-\nally, we propose dynamic token pruning and fusion for two\nbackbones. In particular, we remove redundant tokens at\ndifferent levels of the backbones and fuse the image tokens\nwith the language tokens in an adaptive manner. To learn\npolicies for dynamic inference, we train agents using rein-\nforcement learning. In this direction, we replace the CNN\nbackbone in a recent grounding-based vision and language\nmodel, MDETR, with a vision transformer and call it ViT-\nMDETR. Then, we apply our dynamic inference method\nto ViTMDETR, called D-ViTDMETR, and perform experi-\nments on image-language tasks. Our results show that we\ncan improve the run-time efficiency of the state-of-the-art\nmodels MDETR and GLIP by up to ∼50% on Referring Ex-\npression Comprehension and Segmentation, and VQA with\nonly maximum ∼0.3% accuracy drop.\n",
        "question": {
            "statement": "What technique is used to learn policies for dynamic inference in grounding-based vision and language models?",
            "options": [
                "unsupervised learning",
                "supervised learning",
                "transfer learning",
                "reinforcement learning"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "SelfME: Self-Supervised Motion Learning for Micro-Expression Recognition\nXinqi Fan1, Xueli Chen1, Mingjie Jiang1, Ali Raza Shahid1,2, Hong Yan1\n1City University of Hong Kong\n2COMSATS University Islamabad\n{xinqi.fan, xuelichen3-c, minjiang5-c, a.raza}@my.cityu.edu.hk, h.yan@cityu.edu.hk\nAbstract\nFacial micro-expressions (MEs) refer to brief sponta-\nneous facial movements that can reveal a person’s gen-\nuine emotion. They are valuable in lie detection, crimi-\nnal analysis, and other areas. While deep learning-based\nME recognition (MER) methods achieved impressive suc-\ncess, these methods typically require pre-processing using\nconventional optical flow-based methods to extract facial\nmotions as inputs.\nTo overcome this limitation, we pro-\nposed a novel MER framework using self-supervised learn-\ning to extract facial motion for ME (SelfME). To the best of\nour knowledge, this is the first work using an automatically\nself-learned motion technique for MER. However, the self-\nsupervised motion learning method might suffer from ignor-\ning symmetrical facial actions on the left and right sides of\nfaces when extracting fine features. To address this issue,\nwe developed a symmetric contrastive vision transformer\n(SCViT) to constrain the learning of similar facial action\nfeatures for the left and right parts of faces. Experiments\nwere conducted on two benchmark datasets showing that\nour method achieved state-of-the-art performance, and ab-\nlation studies demonstrated the effectiveness of our method.\n",
        "question": {
            "statement": "What is the main advantage of using self-supervised motion learning in facial micro-expression recognition?",
            "options": [
                "It reduces the computational cost of facial micro-expression recognition.",
                "It eliminates the need for pre-processing using conventional optical flow-based methods.",
                "It allows for real-time processing of facial micro-expressions.",
                "It improves the accuracy of recognizing emotions from facial expressions."
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "PartManip: Learning Cross-Category Generalizable Part Manipulation Policy\nfrom Point Cloud Observations\nHaoran Geng 1,2*\nZiming Li 1,2*\nYiran Geng 1,2\nJiayi Chen 1,3\nHao Dong 1,2\nHe Wang1,2†\n1CFCS, Peking University\n2School of EECS, Peking University\n3Beijing Academy of Artificial Intelligence\nFigure 1. Overview. We introduce a large-scale cross-category part manipulation benchmark PartManip with diverse object datasets,\nrealistic settings, and rich annotations. We propose a generalizable vision-based policy learning strategy and boost the performance of\npart-based object manipulation by a large margin, which can generalize to unseen object categories and novel objects in the real world.\nAbstract\nLearning a generalizable object manipulation policy is\nvital for an embodied agent to work in complex real-world\nscenes. Parts, as the shared components in different object\ncategories, have the potential to increase the generaliza-\ntion ability of the manipulation policy and achieve cross-\ncategory object manipulation. In this work, we build the\nfirst large-scale, part-based cross-category object manip-\nulation benchmark, PartManip, which is composed of 11\nobject categories, 494 objects, and 1432 tasks in 6 task\nclasses.\nCompared to previous work, our benchmark is\nalso more diverse and realistic, i.e., having more objects\nand using sparse-view point cloud as input without oracle\ninformation like part segmentation. To tackle the difficul-\nties of vision-based policy learning, we first train a state-\n*Equal contribution.\n†Corresponding author: hewang@pku.edu.cn.\nbased expert with our proposed part-based canonicaliza-\ntion and part-aware rewards, and then distill the knowledge\nto a vision-based student. We also find an expressive back-\nbone is essential to overcome the large diversity of different\nobjects. For cross-category generalization, we introduce\ndomain adversarial learning for domain-invariant feature\nextraction. Extensive experiments in simulation show that\nour learned policy can outperform other methods by a large\nmargin, especially on unseen object categories. We also\ndemonstrate our method can successfully manipulate novel\nobjects in the real world. Our benchmark has been released\nin https://pku-epic.github.io/PartManip.\n",
        "question": {
            "statement": "What is the main advantage of using parts as the shared components in different object categories for object manipulation?",
            "options": [
                "faster computation",
                "enhanced visualization",
                "increased generalizability",
                "improved precision"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Weakly supervised segmentation with point annotations for histopathology\nimages via contrast-based variational model\nHongrun Zhang1,4†, Liam Burrows2†, Yanda Meng1, Declan Sculthorpe3, Abhik Mukherjee3,\nSarah E Coupland4, Ke Chen2, Yalin Zheng1∗\n1Department of Eye and Vision Science, University of Liverpool, Liverpool, UK 2 Department of Mathematical\nSciences and Centre for Mathematical Imaging Techniques, University of Liverpool, Liverpool, UK\n3 Biodiscovery Institute, School of Medicine, University of Nottingham, Nottingham, UK\n4 Institute of Systems, Molecular and Integrative Biology, University of Liverpool, Liverpool, UK\n{hongrun.zhang,liam.burrows,yanda.meng,s.e.coupland,k.chen,yalin.zheng}@liverpool.ac.uk\n{declan.sculthorpe,abhik.mukherjee1}@nottingham.ac.uk, zhang.hr.jlu@gmail.com\nAbstract\nImage segmentation is a fundamental task in the field of\nimaging and vision. Supervised deep learning for segmen-\ntation has achieved unparalleled success when sufficient\ntraining data with annotated labels are available. However,\nannotation is known to be expensive to obtain, especially\nfor histopathology images where the target regions are usu-\nally with high morphology variations and irregular shapes.\nThus, weakly supervised learning with sparse annotations\nof points is promising to reduce the annotation workload. In\nthis work, we propose a contrast-based variational model\nto generate segmentation results, which serve as reliable\ncomplementary supervision to train a deep segmentation\nmodel for histopathology images.\nThe proposed method\nconsiders the common characteristics of target regions in\nhistopathology images and can be trained in an end-to-end\nmanner.\nIt can generate more regionally consistent and\nsmoother boundary segmentation, and is more robust to un-\nlabeled ‘novel’ regions. Experiments on two different his-\ntology datasets demonstrate its effectiveness and efficiency\nin comparison to previous models. Code is available at:\nhttps://github.com/hrzhang1123/CVM_WS_\nSegmentation.\n",
        "question": {
            "statement": "What is a major challenge in achieving success with supervised deep learning for image segmentation?",
            "options": [
                "Deep learning models are too complex",
                "Image resolution is too high",
                "Annotation is expensive to obtain",
                "Computational resources are limited"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "VILA: Learning Image Aesthetics from User Comments\nwith Vision-Language Pretraining\nJunjie Ke, Keren Ye, Jiahui Yu, Yonghui Wu, Peyman Milanfar, Feng Yang\nGoogle Research\n{junjiek, yek, jiahuiyu, yonghui, milanfar, fengyang}@google.com\nAbstract\nAssessing the aesthetics of an image is challenging, as\nit is inﬂuenced by multiple factors including composition,\ncolor, style, and high-level semantics. Existing image aes-\nthetic assessment (IAA) methods primarily rely on human-\nlabeled rating scores, which oversimplify the visual aes-\nthetic information that humans perceive. Conversely, user\ncomments offer more comprehensive information and are a\nmore natural way to express human opinions and prefer-\nences regarding image aesthetics. In light of this, we pro-\npose learning image aesthetics from user comments, and ex-\nploring vision-language pretraining methods to learn mul-\ntimodal aesthetic representations. Speciﬁcally, we pretrain\nan image-text encoder-decoder model with image-comment\npairs, using contrastive and generative objectives to learn\nrich and generic aesthetic semantics without human labels.\nTo efﬁciently adapt the pretrained model for downstream\nIAA tasks, we further propose a lightweight rank-based\nadapter that employs text as an anchor to learn the aesthetic\nranking concept. Our results show that our pretrained aes-\nthetic vision-language model outperforms prior works on\nimage aesthetic captioning over the AVA-Captions dataset,\nand it has powerful zero-shot capability for aesthetic tasks\nsuch as zero-shot style classiﬁcation and zero-shot IAA, sur-\npassing many supervised baselines. With only minimal ﬁne-\ntuning parameters using the proposed adapter module, our\nmodel achieves state-of-the-art IAA performance over the\nAVA dataset. 1\n",
        "question": {
            "statement": "What is a limitation of traditional image aesthetic assessment methods?",
            "options": [
                "They require large amounts of computational resources",
                "They rely solely on human-labeled rating scores",
                "They are only applicable to specific styles of images",
                "They are highly subjective and prone to bias"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Primitive Generation and Semantic-related Alignment\nfor Universal Zero-Shot Segmentation\nShuting He1†\nHenghui Ding2† ",
        "question": {
            "statement": "What is the primary goal of universal zero-shot segmentation?",
            "options": [
                "To improve the performance of pre-trained language models",
                "To enable models to perform image segmentation tasks without requiring training data",
                "To reduce the computational cost of semantic alignment",
                "To increase the robustness of primitive generation"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "9",
            "options": [
                "0",
                "8",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "TrojDiff: Trojan Attacks on Diffusion Models with Diverse Targets\nWeixin Chen\nUIUC\nweixinc2@illinois.edu\nDawn Song\nUC Berkeley\ndawnsong@cs.berkeley.edu\nBo Li\nUIUC\nlbo@illinois.edu\nAbstract\nDiffusion models have achieved great success in a range\nof tasks, such as image synthesis and molecule design. As\nsuch successes hinge on large-scale training data collected\nfrom diverse sources, the trustworthiness of these collected\ndata is hard to control or audit. In this work, we aim to\nexplore the vulnerabilities of diffusion models under poten-\ntial training data manipulations and try to answer: How\nhard is it to perform Trojan attacks on well-trained diffu-\nsion models? What are the adversarial targets that such\nTrojan attacks can achieve? To answer these questions, we\npropose an effective Trojan attack against diffusion mod-\nels, TrojDiff, which optimizes the Trojan diffusion and gen-\nerative processes during training.\nIn particular, we de-\nsign novel transitions during the Trojan diffusion process\nto diffuse adversarial targets into a biased Gaussian dis-\ntribution and propose a new parameterization of the Tro-\njan generative process that leads to an effective training\nobjective for the attack.\nIn addition, we consider three\ntypes of adversarial targets: the Trojaned diffusion models\nwill always output instances belonging to a certain class\nfrom the in-domain distribution (In-D2D attack), out-of-\ndomain distribution (Out-D2D-attack), and one specific in-\nstance (D2I attack).\nWe evaluate TrojDiff on CIFAR-10\nand CelebA datasets against both DDPM and DDIM dif-\nfusion models. We show that TrojDiff always achieves high\nattack performance under different adversarial targets us-\ning different types of triggers, while the performance in be-\nnign environments is preserved. The code is available at\nhttps://github.com/chenweixin107/TrojDiff.\n",
        "question": {
            "statement": "What type of vulnerability do diffusion models face due to their reliance on large-scale training data?",
            "options": [
                "Model architecture flaws",
                "Lack of regularization techniques",
                "Overfitting issues",
                "Data manipulation attacks"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": false,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "FedDM: Iterative Distribution Matching\nfor Communication-Efficient Federated Learning\nYuanhao Xiong1*\nRuochen Wang1*\nMinhao Cheng2\nFelix Yu3\nCho-Jui Hsieh1\n1UCLA\n2HKUST\n3Google Research\nAbstract\nFederated learning (FL) has recently attracted increas-\ning attention from academia and industry, with the ultimate\ngoal of achieving collaborative training under privacy and\ncommunication constraints.\nExisting iterative model av-\neraging based FL algorithms require a large number of\ncommunication rounds to obtain a well-performed model\ndue to extremely unbalanced and non-i.i.d data partitioning\namong different clients. Thus, we propose FedDM to build\nthe global training objective from multiple local surrogate\nfunctions, which enables the server to gain a more global\nview of the loss landscape.\nIn detail, we construct syn-\nthetic sets of data on each client to locally match the loss\nlandscape from original data through distribution match-\ning. FedDM reduces communication rounds and improves\nmodel quality by transmitting more informative and smaller\nsynthesized data compared with unwieldy model weights.\nWe conduct extensive experiments on three image classifica-\ntion datasets, and show that our method outperforms other\nFL counterparts in terms of efficiency and model perfor-\nmance given a limited number of communication rounds.\nMoreover, we demonstrate that FedDM can be adapted to\npreserve differential privacy with Gaussian mechanism and\ntrain a better model under the same privacy budget.\n",
        "question": {
            "statement": "What is the main goal of federated learning?",
            "options": [
                "improving model performance regardless of data distribution",
                "increasing the amount of data shared among clients",
                "achieving collaborative training under privacy and communication constraints",
                "enabling faster communication between clients and servers"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "A Unified Knowledge Distillation Framework for Deep Directed Graphical Models\nYizhuo Chen *\nWilliam and Mary & UIUC\nyizhuoc@illinois.edu\nKaizhao Liang\nUIUC\nkl2@illinois.edu\nZhe Zeng\nUCLA\nzhezeng@cs.ucla.edu\nShuochao Yao\nGeorge Mason University\nshuochao@gmu.edu\nHuajie Shao †\nWilliam and Mary\nhshao@wm.edu\nAbstract\nKnowledge distillation (KD) is a technique that transfers\nthe knowledge from a large teacher network to a small stu-\ndent network. It has been widely applied to many different\ntasks, such as model compression and federated learning.\nHowever, existing KD methods fail to generalize to gen-\neral deep directed graphical models (DGMs) with arbitrary\nlayers of random variables. We refer by deep DGMs to\nDGMs whose conditional distributions are parameterized\nby deep neural networks. In this work, we propose a novel\nunified knowledge distillation framework for deep DGMs\non various applications. Specifically, we leverage the repa-\nrameterization trick to hide the intermediate latent variables,\nresulting in a compact DGM. Then we develop a surrogate\ndistillation loss to reduce error accumulation through mul-\ntiple layers of random variables. Moreover, we present the\nconnections between our method and some existing knowl-\nedge distillation approaches. The proposed framework is\nevaluated on four applications: data-free hierarchical varia-\ntional autoencoder (VAE) compression, data-free variational\nrecurrent neural networks (VRNN) compression, data-free\nHelmholtz Machine (HM) compression, and VAE continual\nlearning. The results show that our distillation method out-\nperforms the baselines in data-free model compression tasks.\nWe further demonstrate that our method significantly im-\nproves the performance of KD-based continual learning for\ndata generation. Our source code is available at https:\n//github.com/YizhuoChen99/KD4DGM-CVPR.\n",
        "question": {
            "statement": "What is the primary goal of knowledge distillation in machine learning?",
            "options": [
                "Reduce overfitting in a neural network",
                "Improve the accuracy of a single model",
                "Transfer knowledge from a large complex model to a smaller simpler model",
                "Increase the size of a model to handle more data"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "2",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving\nBen Agro*, Quinlan Sykora∗, Sergio Casas∗, Raquel Urtasun\nWaabi, University of Toronto\n{bagro, qsykora, sergio, urtasun}@waabi.ai\nAbstract\nA self-driving vehicle (SDV) must be able to perceive its\nsurroundings and predict the future behavior of other traf-\nfic participants. Existing works either perform object de-\ntection followed by trajectory forecasting of the detected\nobjects, or predict dense occupancy and flow grids for the\nwhole scene. The former poses a safety concern as the num-\nber of detections needs to be kept low for efficiency rea-\nsons, sacrificing object recall. The latter is computation-\nally expensive due to the high-dimensionality of the out-\nput grid, and suffers from the limited receptive field inher-\nent to fully convolutional networks. Furthermore, both ap-\nproaches employ many computational resources predicting\nareas or objects that might never be queried by the motion\nplanner. This motivates our unified approach to percep-\ntion and future prediction that implicitly represents occu-\npancy and flow over time with a single neural network. Our\nmethod avoids unnecessary computation, as it can be di-\nrectly queried by the motion planner at continuous spatio-\ntemporal locations. Moreover, we design an architecture\nthat overcomes the limited receptive field of previous ex-\nplicit occupancy prediction methods by adding an efficient\nyet effective global attention mechanism. Through exten-\nsive experiments in both urban and highway settings, we\ndemonstrate that our implicit model outperforms the cur-\nrent state-of-the-art. For more information, visit the project\nwebsite: https://waabi.ai/research/implicito.\n",
        "question": {
            "statement": "What is a limitation of existing approaches to perception and prediction in self-driving vehicles?",
            "options": [
                "They can only operate in urban settings",
                "They are unable to detect objects at all",
                "They require human input to function",
                "They waste computational resources on areas that may not be relevant to the motion planner"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "MAESTER: Masked Autoencoder Guided Segmentation at Pixel Resolution for\nAccurate, Self-Supervised Subcellular Structure Recognition\nRonald Xie1,2,3,4,*,†\nKuan Pang1,4,*\nGary D. Bader1,3,4,‡\nBo Wang1,2,3,‡\n1University of Toronto, 2Vector Institute, 3University Health Network, 4The Donnelly Centre\n{ronald.xie, kuan.pang, gary.bader}@mail.utoronto.ca , bowang@vectorinstitute.ai\nAbstract\nAccurate segmentation of cellular images remains an\nelusive task due to the intrinsic variability in morphology\nof biological structures. Complete manual segmentation is\nunfeasible for large datasets, and while supervised meth-\nods have been proposed to automate segmentation, they\noften rely on manually generated ground truths which are\nespecially challenging and time consuming to generate in\nbiology due to the requirement of domain expertise. Fur-\nthermore, these methods have limited generalization capac-\nity, requiring additional manual labels to be generated for\neach dataset and use case. We introduce MAESTER (Masked\nAutoEncoder guided SegmenTation at pixEl Resolution), a\nself-supervised method for accurate, subcellular structure\nsegmentation at pixel resolution. MAESTER treats segmen-\ntation as a representation learning and clustering problem.\nSpecifically, MAESTER learns semantically meaningful to-\nken representations of multi-pixel image patches while si-\nmultaneously maintaining a sufficiently large field of view\nfor contextual learning. We also develop a cover-and-stride\ninference strategy to achieve pixel-level subcellular struc-\nture segmentation. We evaluated MAESTER on a publicly\navailable volumetric electron microscopy (VEM) dataset of\nprimary mouse pancreatic islets β cells and achieved up-\nwards of 29.1% improvement over state-of-the-art under\nthe same evaluation criteria. Furthermore, our results are\ncompetitive against supervised methods trained on the same\ntasks, closing the gap between self-supervised and super-\nvised approaches. MAESTER shows promise for alleviating\nthe critical bottleneck of ground truth generation for imaging\nrelated data analysis and thereby greatly increasing the rate\nof biological discovery.\nCode\navailable\nat\nhttps : / / github . com /\nbowang-lab/MAESTER\n*Equal contribution\n†Project lead\n‡Co-senior author\n",
        "question": {
            "statement": "What is a major limitation of supervised methods for automated segmentation of cellular images?",
            "options": [
                "They are unable to handle large datasets.",
                "They require additional manual labels to be generated for each dataset and use case.",
                "They require domain expertise to interpret the results.",
                "They are not effective for segmenting subcellular structures."
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "10",
                "2",
                "2"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "GlassesGAN: Eyewear Personalization using Synthetic Appearance Discovery\nand Targeted Subspace Modeling*\nRichard Plesh1\nPeter Peer2\nVitomir Struc2\n1 Clarkson University, USA\n2 University of Ljubljana, Slovenia\nhttps://github.com/pleshro/GlassesGAN_release\nEnlarge\nRaise\nRound\nEnlarge/Square\nCateye\nThin\nThicken\nReverse Cateye\nShrink/Round\nSquare\nLower\nShrink\nFigure 1. This paper introduces GlassesGAN, an innovative approach to image editing capable of generating continuously tunable, multi-\nattribute, and photo-realistic editing of eyeglasses by leveraging a novel method for modeling sub-spaces in the StyleGAN2 latent space.\nThe presented (1024 × 1024) examples show editing results for twelve different tuning attributes. Best viewed zoomed-in.\nAbstract\nWe present GlassesGAN, a novel image editing frame-\nwork for custom design of glasses, that sets a new stan-\ndard in terms of output-image quality, edit realism, and\ncontinuous multi-style edit capability.\nTo facilitate the\nediting process with GlassesGAN, we propose a Targeted\nSubspace Modelling (TSM) procedure that, based on a\nnovel mechanism for (synthetic) appearance discovery in\nthe latent space of a pre-trained GAN generator, constructs\nan eyeglasses-specific (latent) subspace that the editing\nframework can utilize. Additionally, we also introduce an\nappearance-constrained subspace initialization (SI) tech-\nnique that centers the latent representation of the given in-\nput image in the well-defined part of the constructed sub-\nspace to improve the reliability of the learned edits. We\ntest GlassesGAN on two (diverse) high-resolution datasets\n(CelebA-HQ and SiblingsDB-HQf) and compare it to three\nstate-of-the-art baselines, i.e., InterfaceGAN, GANSpace,\nand MaskGAN. The reported results show that GlassesGAN\nconvincingly outperforms all competing techniques, while\n*Supported by ARRS J2-2501(A), the Fulbright Scholarship Fund, the\nCenter for Identification Technology Research, and the National Science\nFoundation under Grant No. 1650503.\noffering functionality (e.g., fine-grained multi-style editing)\nnot available with any of the competitors. The source code\nfor GlassesGAN is made publicly available.\n",
        "question": {
            "statement": "What is the main advantage of the GlassesGAN framework over other state-of-the-art image editing techniques?",
            "options": [
                "It uses a pre-trained GAN generator for faster processing",
                "It offers fine-grained multi-style editing capabilities",
                "It requires manual input from users to define editing attributes",
                "It is limited to editing eyeglasses only"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via\nTransformers\nCheng Zhang1\nHai Liu1,*\nYongjian Deng2,3\nBochen Xie4\nYoufu Li4,∗\n1National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, China\n2College of Computer Science, Beijing University of Technology, Beijing, China\n3Engineering Research Center of Intelligence Perception and Autonomous Control, Ministry of Education, Beijing, China\n4Department of Mechanical Engineering, City University of Hong Kong, Kowloon, Hong Kong, China\nzc2021@mails.ccnu.edu.cn, hailiu0204@ccnu.edu.cn, yjdeng@bjut.edu.cn,\nboxie4-c@my.cityu.edu.hk, meyfli@cityu.edu.hk\nAbstract\nHead pose estimation (HPE) has been widely used in\nthe fields of human machine interaction, self-driving, and\nattention estimation.\nHowever, existing methods cannot\ndeal with extreme head pose randomness and serious oc-\nclusions.\nTo address these challenges, we identify three\ncues from head images, namely, neighborhood similari-\nties, significant facial changes, and critical minority rela-\ntionships. To leverage the observed findings, we propose\na novel critical minority relationship-aware method based\non the Transformer architecture in which the facial part\nrelationships can be learned. Specifically, we design sev-\neral orientation tokens to explicitly encode the basic ori-\nentation regions. Meanwhile, a novel token guide multi-\nloss function is designed to guide the orientation tokens as\nthey learn the desired regional similarities and relation-\nships.\nWe evaluate the proposed method on three chal-\nlenging benchmark HPE datasets. Experiments show that\nour method achieves better performance compared with\nstate-of-the-art methods. Our code is publicly available at\nhttps://github.com/zc2023/TokenHPE.\n",
        "question": {
            "statement": "What is a common application area of Head Pose Estimation?",
            "options": [
                "Medical Diagnosis",
                "Agriculture",
                "Robotics",
                "Human Machine Interaction"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "5",
                "10"
            ]
        },
        "difference": 5,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Mask3D: Pre-training 2D Vision Transformers by Learning Masked 3D Priors\nJi Hou1\nXiaoliang Dai1\nZijian He1\nAngela Dai2\nMatthias Nießner2\n1Meta Reality Labs\n2Technical University of Munich\nFigure 1.\nWe present Mask3D, which learns to embed 3D priors to 2D representations for image understanding tasks, based on a self-\nsupervised pre-training formulation from single RGB-D views, without requiring any camera pose or multi-view correspondence informa-\ntion. Our pre-training takes masked RGB and depth patches as input to reconstruct the dense depth map, and the pre-trained color backbone\nis used to fine-tune various downstream image understanding tasks. This results in effective ViT pre-training for a variety of downstream\ntasks and datasets.\nAbstract\nCurrent popular backbones in computer vision, such as\nVision Transformers (ViT) and ResNets are trained to per-\nceive the world from 2D images. However, to more effec-\ntively understand 3D structural priors in 2D backbones, we\npropose Mask3D to leverage existing large-scale RGB-D\ndata in a self-supervised pre-training to embed these 3D\npriors into 2D learned feature representations.\nIn con-\ntrast to traditional 3D contrastive learning paradigms re-\nquiring 3D reconstructions or multi-view correspondences,\nour approach is simple: we formulate a pre-text reconstruc-\ntion task by masking RGB and depth patches in individual\nRGB-D frames. We demonstrate the Mask3D is particu-\nlarly effective in embedding 3D priors into the powerful\n2D ViT backbone, enabling improved representation learn-\ning for various scene understanding tasks, such as semantic\nsegmentation, instance segmentation and object detection.\nExperiments show that Mask3D notably outperforms exist-\ning self-supervised 3D pre-training approaches on ScanNet,\nNYUv2, and Cityscapes image understanding tasks, with\nan improvement of +6.5% mIoU against the state-of-the-art\nPri3D on ScanNet image semantic segmentation.\n",
        "question": {
            "statement": "What is the main goal of the proposed method Mask3D?",
            "options": [
                "To develop a new type of CNN architecture",
                "To incorporate 3D structural priors into 2D vision transformers",
                "To improve the performance of object detection models",
                "To create a dataset for 3D reconstruction tasks"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "ResFormer: Scaling ViTs with Multi-Resolution Training\nRui Tian1,2\nZuxuan Wu1,2†\nQi Dai3\nHan Hu3\nYu Qiao4\nYu-Gang Jiang1,2\n1Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University\n2Shanghai Collaborative Innovation Center of Intelligent Visual Computing\n3Microsoft Research Asia\n4Shanghai AI Laboratory\nAbstract\nVision Transformers (ViTs) have achieved overwhelming\nsuccess, yet they suffer from vulnerable resolution scalabil-\nity, i.e., the performance drops drastically when presented\nwith input resolutions that are unseen during training. We\nintroduce, ResFormer, a framework that is built upon the\nseminal idea of multi-resolution training for improved per-\nformance on a wide spectrum of, mostly unseen, testing res-\nolutions. In particular, ResFormer operates on replicated\nimages of different resolutions and enforces a scale con-\nsistency loss to engage interactive information across dif-\nferent scales. More importantly, to alternate among vary-\ning resolutions effectively, especially novel ones in testing,\nwe propose a global-local positional embedding strategy\nthat changes smoothly conditioned on input sizes. We con-\nduct extensive experiments for image classification on Im-\nageNet. The results provide strong quantitative evidence\nthat ResFormer has promising scaling abilities towards a\nwide range of resolutions. For instance, ResFormer-B-MR\nachieves a Top-1 accuracy of 75.86% and 81.72% when\nevaluated on relatively low and high resolutions respec-\ntively (i.e., 96 and 640), which are 48% and 7.49% better\nthan DeiT-B. We also demonstrate, moreover, ResFormer is\nflexible and can be easily extended to semantic segmenta-\ntion, object detection and video action recognition.\n",
        "question": {
            "statement": "What is a major limitation of Vision Transformers (ViTs) that can be addressed through multi-resolution training?",
            "options": [
                "They are limited to image classification tasks",
                "Performance drops drastically when presented with input resolutions that are unseen during training",
                "They are unable to handle large input images",
                "They require significant computational resources"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Understanding Imbalanced Semantic Segmentation Through Neural Collapse\nZhisheng Zhong1,∗\nJiequan Cui1,∗\nYibo Yang2,∗\nXiaoyang Wu3\nXiaojuan Qi3\nXiangyu Zhang4\nJiaya Jia1,5\nCUHK1\nJD Explore Academy2\nHKU3\nMEGVII4\nSmartMore5\nCode: https://github.com/dvlab-research/Imbalanced-Learning\nAbstract\nA recent study has shown a phenomenon called neural\ncollapse in that the within-class means of features and the\nclassifier weight vectors converge to the vertices of a sim-\nplex equiangular tight frame at the terminal phase of train-\ning for classification. In this paper, we explore the cor-\nresponding structures of the last-layer feature centers and\nclassifiers in semantic segmentation. Based on our empir-\nical and theoretical analysis, we point out that semantic\nsegmentation naturally brings contextual correlation and\nimbalanced distribution among classes, which breaks the\nequiangular and maximally separated structure of neural\ncollapse for both feature centers and classifiers. However,\nsuch a symmetric structure is beneficial to discrimination\nfor the minor classes. To preserve these advantages, we in-\ntroduce a regularizer on feature centers to encourage the\nnetwork to learn features closer to the appealing struc-\nture in imbalanced semantic segmentation. Experimental\nresults show that our method can bring significant improve-\nments on both 2D and 3D semantic segmentation bench-\nmarks. Moreover, our method ranks 1st and sets a new\nrecord (+6.8% mIoU) on the ScanNet200 test leaderboard.\n",
        "question": {
            "statement": "What happens to the within-class means of features and classifier weight vectors in the terminal phase of training for classification?",
            "options": [
                "They converge to zero",
                "They remain constant",
                "They diverge randomly",
                "They converge to the vertices of a simplex equiangular tight frame"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "GradICON: Approximate Diffeomorphisms via Gradient Inverse Consistency\nLin Tian∗1 Hastings Greer*1 François-Xavier Vialard2,3 Roland Kwitt4 Raúl San José Estépar5\nRichard Jarrett Rushmore6 Nikolaos Makris5 Sylvain Bouix7 Marc Niethammer1\n1UNC Chapel Hill\n2LIGM, Université Gustave Eiffel 3MOKAPLAN, INRIA Paris\n4University of Salzburg\n5Harvard Medical School\n6Boston University\n7ÉTS Montréal\nAbstract\nWe present an approach to learning regular spatial trans-\nformations between image pairs in the context of medical\nimage registration. Contrary to optimization-based registra-\ntion techniques and many modern learning-based methods,\nwe do not directly penalize transformation irregularities but\ninstead promote transformation regularity via an inverse\nconsistency penalty. We use a neural network to predict a\nmap between a source and a target image as well as the map\nwhen swapping the source and target images. Different from\nexisting approaches, we compose these two resulting maps\nand regularize deviations of the Jacobian of this composi-\ntion from the identity matrix. This regularizer – GradICON –\nresults in much better convergence when training registra-\ntion models compared to promoting inverse consistency of\nthe composition of maps directly while retaining the desir-\nable implicit regularization effects of the latter. We achieve\nstate-of-the-art registration performance on a variety of real-\nworld medical image datasets using a single set of hyperpa-\nrameters and a single non-dataset-specific training protocol.\nCode is available at https://github.com/uncbiag/ICON.\n",
        "question": {
            "statement": "What is the main advantage of the GradICON approach in medical image registration?",
            "options": [
                "It requires dataset-specific training protocols",
                "It directly penalizes transformation irregularities",
                "It uses a single neural network to predict the transformation map",
                "It promotes transformation regularity via an inverse consistency penalty"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "3",
                "8"
            ]
        },
        "difference": 5,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Learning Procedure-aware Video Representation\nfrom Instructional Videos and Their Narrations\nYiwu Zhong1*\n, Licheng Yu2, Yang Bai2, Shangwen Li2, Xueting Yan2†\n, Yin Li1†\n1University of Wisconsin-Madison, 2Meta AI\nyzhong52@wisc.edu, {lichengyu, yangbai, dylanwen, xyan18}@meta.com, yin.li@wisc.edu\nAbstract\nThe abundance of instructional videos and their narra-\ntions over the Internet offers an exciting avenue for un-\nderstanding procedural activities.\nIn this work, we pro-\npose to learn video representation that encodes both ac-\ntion steps and their temporal ordering, based on a large-\nscale dataset of web instructional videos and their narra-\ntions, without using human annotations. Our method jointly\nlearns a video representation to encode individual step con-\ncepts, and a deep probabilistic model to capture both tem-\nporal dependencies and immense individual variations in\nthe step ordering. We empirically demonstrate that learn-\ning temporal ordering not only enables new capabilities\nfor procedure reasoning, but also reinforces the recognition\nof individual steps. Our model significantly advances the\nstate-of-the-art results on step classification (+2.8%/+3.3%\non COIN / EPIC-Kitchens) and step forecasting (+7.4%\non COIN). Moreover, our model attains promising re-\nsults in zero-shot inference for step classification and fore-\ncasting, as well as in predicting diverse and plausible\nsteps for incomplete procedures. Our code is available at\nhttps://github.com/facebookresearch/ProcedureVRL.\n",
        "question": {
            "statement": "What is the primary goal of learning procedure-aware video representations from instructional videos and their narrations?",
            "options": [
                "To understand procedural activities",
                "To improve video compression algorithms",
                "To generate new instructional videos",
                "To recognize objects in videos"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "8",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Wavelet Diffusion Models are fast and scalable Image Generators\nHao Phung†, Quan Dao†, Anh Tran\nVinAI Research\n{v.haopt12, v.quandm7, v.anhtt152}@vinai.io\nAbstract\nDiffusion models are rising as a powerful solution for\nhigh-fidelity image generation, which exceeds GANs in\nquality in many circumstances. However, their slow train-\ning and inference speed is a huge bottleneck, blocking them\nfrom being used in real-time applications. A recent Diffu-\nsionGAN method significantly decreases the models’ run-\nning time by reducing the number of sampling steps from\nthousands to several, but their speeds still largely lag be-\nhind the GAN counterparts.\nThis paper aims to reduce\nthe speed gap by proposing a novel wavelet-based diffu-\nsion scheme. We extract low-and-high frequency compo-\nnents from both image and feature levels via wavelet decom-\nposition and adaptively handle these components for faster\nprocessing while maintaining good generation quality. Fur-\nthermore, we propose to use a reconstruction term, which\neffectively boosts the model training convergence. Exper-\nimental results on CelebA-HQ, CIFAR-10, LSUN-Church,\nand STL-10 datasets prove our solution is a stepping-stone\nto offering real-time and high-fidelity diffusion models. Our\ncode and pre-trained checkpoints are available at https:\n//github.com/VinAIResearch/WaveDiff.git.\n",
        "question": {
            "statement": "What is a limitation of diffusion models that prevents them from being used in real-time applications?",
            "options": [
                "Their inability to handle complex data sets",
                "Their requirement for large amounts of computational resources",
                "Their lack of ability to generate high-quality images",
                "Their slow training and inference speed"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive\nLearning\nJishnu Mukhoti*1,2, Tsung-Yu Lin2, Omid Poursaeed2, Rui Wang2, Ashish Shah2,\nPhilip H.S. Torr1, Ser-Nam Lim2\n1University of Oxford, 2Meta AI\njishnu.mukhoti@eng.ox.ac.uk, {tsungyulin,opoursaeed,raywang,ashishbshah}@meta.com\nphilip.torr@eng.ox.ac.uk, sernamlim@meta.com\nAbstract\nWe introduce Patch Aligned Contrastive Learning\n(PACL), a modified compatibility function for CLIP’s con-\ntrastive loss, intending to train an alignment between the\npatch tokens of the vision encoder and the CLS token of the\ntext encoder. With such an alignment, a model can identify\nregions of an image corresponding to a given text input, and\ntherefore transfer seamlessly to the task of open vocabulary\nsemantic segmentation without requiring any segmentation\nannotations during training. Using pre-trained CLIP en-\ncoders with PACL, we are able to set the state-of-the-art\non the task of open vocabulary zero-shot segmentation on\n4 different segmentation benchmarks: Pascal VOC, Pascal\nContext, COCO Stuff and ADE20K. Furthermore, we show\nthat PACL is also applicable to image-level predictions and\nwhen used with a CLIP backbone, provides a general im-\nprovement in zero-shot classification accuracy compared to\nCLIP, across a suite of 12 image classification datasets.\n",
        "question": {
            "statement": "What is the primary goal of the Patch Aligned Contrastive Learning (PACL) method?",
            "options": [
                "To improve the accuracy of image-level predictions",
                "To align patch tokens of the vision encoder with the CLS token of the text encoder",
                "To enable the model to generate text descriptions of images",
                "To reduce the number of segmentation annotations required during training"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "1",
                "7"
            ]
        },
        "difference": 3,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Lookahead Diffusion Probabilistic Models for Reﬁning Mean Estimation\nGuoqiang Zhang\nUniversity of Technology Sydney\nguoqiang.zhang@uts.edu.au\nKenta Niwa\nNTT Communication Science Laboratories\nkenta.niwa.bk@hco.ntt.co.jp\nW. Bastiaan Kleijn\nVictoria University of Wellington\nbastiaan.kleijn@vuw.ac.nz\nAbstract\nWe propose lookahead diffusion probabilistic models\n(LA-DPMs) to exploit the correlation in the outputs of the\ndeep neural networks (DNNs) over subsequent timesteps in\ndiffusion probabilistic models (DPMs) to reﬁne the mean\nestimation of the conditional Gaussian distributions in the\nbackward process. A typical DPM ﬁrst obtains an estimate\nof the original data sample x by feeding the most recent\nstate zi and index i into the DNN model and then computes\nthe mean vector of the conditional Gaussian distribution for\nzi−1. We propose to calculate a more accurate estimate for\nx by performing extrapolation on the two estimates of x\nthat are obtained by feeding (zi+1, i+1) and (zi, i) into the\nDNN model. The extrapolation can be easily integrated into\nthe backward process of existing DPMs by introducing an\nadditional connection over two consecutive timesteps, and\nﬁne-tuning is not required. Extensive experiments showed\nthat plugging in the additional connection into DDPM,\nDDIM, DEIS, S-PNDM, and high-order DPM-Solvers leads\nto a signiﬁcant performance gain in terms of Fr´\nechet in-\nception distance (FID) score. Our implementation is avail-\nable at https://github.com/guoqiang-zhang-\nx/LA-DPM.\n",
        "question": {
            "statement": "What is the primary goal of lookahead diffusion probabilistic models?",
            "options": [
                "To refine the mean estimation of conditional Gaussian distributions",
                "To reduce the complexity of deep neural networks",
                "To increase the number of timesteps in diffusion probabilistic models",
                "To replace traditional diffusion probabilistic models with new architectures"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "MetaCLUE: Towards Comprehensive Visual Metaphors Research\nArjun R. Akula∗, Brendan Driscoll∗, Pradyumna Narayana, Soravit Changpinyo,\nZhiwei Jia, Suyash Damle, Garima Pruthi, Sugato Basu,\nLeonidas Guibas, William T. Freeman, Yuanzhen Li, Varun Jampani∗\nGoogle\nMetaphor: Killing trees is as harmful\nas killing wildlife.\nClassiﬁcation\nIs this a visual metaphor?\n- YES\n- NO\nLocalization\nDetect image regions that invoke the\nconcepts:\n- Killing trees\n- killing wildlife\ngEneration\nPrompt: “An advertisement where\nkilling trees is as harmful as killing\nwildlife.”\nStable Diﬀusion\nImagen\nUnderstanding\nRetrieval\nPick the right one:\n(a) Killing the forest is as deadly as\nkilling the animals too.\n(b) Birds is as much a part of our\nworld as used cans.\nCaptioning\nSample predictions:\n1. Deforestation is as damaging as\nkilling wildlife.\n2. Deforestation is as bad as ending the\ndeath penalty.\nVisual Question Answering\nSample Questions:\nQ. What is as harmful as killing\nwildlife?\nQ. What is compared to killing wildlife?\nFigure 1. With MetaCLUE, we introduce several interesting tasks related to visual metaphors. We collect metaphor annotations (objects,\nabstract concepts, relationships and object boxes) for evaluating existing models on these tasks. Speciﬁcally we perform a comprehensive\nevaluation of vision and language models on four different tasks (Classiﬁcation, Localization, Understanding, and gEneration). Com-\nprehensive experiments in this work show that state-of-the-art techniques mostly focus on literal interpretation and perform poorly in\nunderstanding and generation of metaphor images.\nAbstract\nCreativity is an indispensable part of human cognition\nand also an inherent part of how we make sense of the\nworld. Metaphorical abstraction is fundamental in commu-\nnicating creative ideas through nuanced relationships be-\ntween abstract concepts such as feelings. While computer\nvision benchmarks and approaches predominantly focus on\nunderstanding and generating literal interpretations of im-\nages, metaphorical comprehension of images remains rel-\natively unexplored. Towards this goal, we introduce Meta-\nCLUE, a set of vision tasks on visual metaphor. We also\ncollect high-quality and rich metaphor annotations (ab-\n*Equal Contribution\nstract objects, concepts, relationships along with their cor-\nresponding object boxes) as there do not exist any datasets\nthat facilitate the evaluation of these tasks. We perform\na comprehensive analysis of state-of-the-art models in vi-\nsion and language based on our annotations, highlight-\ning strengths and weaknesses of current approaches in vi-\nsual metaphor classiﬁcation, localization, understanding\n(retrieval, question answering, captioning) and generation\n(text-to-image synthesis) tasks.\nWe hope this work pro-\nvides a concrete step towards developing AI systems with\nhuman-like creative capabilities. Project page: https:\n//metaclue.github.io\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n23201\n",
        "question": {
            "statement": "What is a key aspect of human cognition that is being targeted in the development of AI systems?",
            "options": [
                "creativity",
                "memory",
                "logic",
                "reasoning"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Topology-Guided Multi-Class Cell Context Generation for Digital Pathology\nShahira Abousamra1, Rajarsi Gupta2, Tahsin Kurc2, Dimitris Samaras1, Joel Saltz2 and Chao Chen2\n1Stony Brook University, Department of Computer Science, USA\n2Stony Brook University, Department of Biomedical Informatics, USA\nAbstract\nIn digital pathology, the spatial context of cells is impor-\ntant for cell classification, cancer diagnosis and prognosis.\nTo model such complex cell context, however, is challeng-\ning. Cells form different mixtures, lineages, clusters and\nholes.\nTo model such structural patterns in a learnable\nfashion, we introduce several mathematical tools from spa-\ntial statistics and topological data analysis. We incorporate\nsuch structural descriptors into a deep generative model as\nboth conditional inputs and a differentiable loss. This way,\nwe are able to generate high quality multi-class cell layouts\nfor the first time. We show that the topology-rich cell layouts\ncan be used for data augmentation and improve the perfor-\nmance of downstream tasks such as cell classification.\n",
        "question": {
            "statement": "What type of data analysis is used to model structural patterns in cell context generation?",
            "options": [
                "topological",
                "morphological",
                "statistical",
                "spatial"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "5",
                "5"
            ]
        },
        "difference": 5,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Distilling Cross-Temporal Contexts for Continuous Sign Language Recognition\nLeming Guo1\nWanli Xue1*\nQing Guo2*\nBo Liu3\nKaihua Zhang4\nTiantian Yuan5\nShengyong Chen1\n1 School of Computer Science and Engineering, Tianjin University of Technology,\n2 Centre for Frontier AI Research (CFAR), A*STAR, Singapore, 3 Walmart Global Tech, Sunnyvale, CA, USA,\n4 School of Computer and Software, Nanjing University of Information Science and Technology,\n5 Technical College for the Deaf, Tianjin University of Technology\nAbstract\nContinuous sign language recognition (CSLR) aims to\nrecognize glosses in a sign language video. State-of-the-\nart methods typically have two modules, a spatial percep-\ntion module and a temporal aggregation module, which are\njointly learned end-to-end. Existing results in [9,20,25,36]\nhave indicated that, as the frontal component of the over-\nall model, the spatial perception module used for spatial\nfeature extraction tends to be insufficiently trained. In this\npaper, we first conduct empirical studies and show that a\nshallow temporal aggregation module allows more thor-\nough training of the spatial perception module. However, a\nshallow temporal aggregation module cannot well capture\nboth local and global temporal context information in sign\nlanguage. To address this dilemma, we propose a cross-\ntemporal context aggregation (CTCA) model. Specifically,\nwe build a dual-path network that contains two branches for\nperceptions of local temporal context and global temporal\ncontext. We further design a cross-context knowledge distil-\nlation learning objective to aggregate the two types of con-\ntext and the linguistic prior. The knowledge distillation en-\nables the resultant one-branch temporal aggregation mod-\nule to perceive local-global temporal and semantic context.\nThis shallow temporal perception module structure facili-\ntates spatial perception module learning. Extensive exper-\niments on challenging CSLR benchmarks demonstrate that\nour method outperforms all state-of-the-art methods.\n",
        "question": {
            "statement": "What is a limitation of current continuous sign language recognition models?",
            "options": [
                "Inability to capture emotional cues",
                "Difficulty in recognizing handshapes",
                "Insufficient training of the spatial perception module",
                "High computational requirements"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Run, Don’t Walk: Chasing Higher FLOPS for Faster Neural Networks\nJierun Chen1, Shiu-hong Kao1, Hao He1\nWeipeng Zhuo1, Song Wen2, Chul-Ho Lee3, S.-H. Gary Chan1\n1HKUST, 2Rutgers University, 3Texas State University\nAbstract\nTo design fast neural networks, many works have been\nfocusing on reducing the number of floating-point opera-\ntions (FLOPs). We observe that such reduction in FLOPs,\nhowever, does not necessarily lead to a similar level of re-\nduction in latency. This mainly stems from inefficiently low\nfloating-point operations per second (FLOPS). To achieve\nfaster networks, we revisit popular operators and demon-\nstrate that such low FLOPS is mainly due to frequent mem-\nory access of the operators, especially the depthwise con-\nvolution.\nWe hence propose a novel partial convolution\n(PConv) that extracts spatial features more efficiently, by\ncutting down redundant computation and memory access\nsimultaneously. Building upon our PConv, we further pro-\npose FasterNet, a new family of neural networks, which\nattains substantially higher running speed than others on\na wide range of devices, without compromising on accu-\nracy for various vision tasks. For example, on ImageNet-\n1k, our tiny FasterNet-T0 is 2.8×, 3.3×, and 2.4× faster\nthan MobileViT-XXS on GPU, CPU, and ARM processors,\nrespectively, while being 2.9% more accurate. Our large\nFasterNet-L achieves impressive 83.5% top-1 accuracy, on\npar with the emerging Swin-B, while having 36% higher in-\nference throughput on GPU, as well as saving 37% compute\ntime on CPU. Code is available at https://github.\ncom/JierunChen/FasterNet.\n",
        "question": {
            "statement": "What is a major factor that contributes to inefficient floating-point operations per second (FLOPS) in neural networks?",
            "options": [
                "Inadequate model architecture",
                "Low-quality input images",
                "Insufficient training data",
                "Frequent memory access"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Image as a Foreign Language: BEIT Pretraining for\nVision and Vision-Language Tasks\nWenhui Wang*\n, Hangbo Bao*\n, Li Dong*\n, Johan Bjorck, Zhiliang Peng, Qiang Liu\nKriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, Furu Wei†\nMicrosoft Corporation\nhttps://aka.ms/beit-3\nAbstract\nA big convergence of language, vision, and multimodal\npretraining is emerging.\nIn this work, we introduce a\ngeneral-purpose multimodal foundation model BEIT-3,\nwhich achieves excellent transfer performance on both vi-\nsion and vision-language tasks. Specifically, we advance\nthe big convergence from three aspects: backbone architec-\nture, pretraining task, and model scaling up. We use Mul-\ntiway Transformers for general-purpose modeling, where\nthe modular architecture enables both deep fusion and\nmodality-specific encoding.\nBased on the shared back-\nbone, we perform masked “language” modeling on images\n(Imglish), texts (English), and image-text pairs (“parallel\nsentences”) in a unified manner. Experimental results show\nthat BEIT-3 obtains remarkable performance on object de-\ntection (COCO), semantic segmentation (ADE20K), image\nclassification (ImageNet), visual reasoning (NLVR2), visual\nquestion answering (VQAv2), image captioning (COCO),\nand cross-modal retrieval (Flickr30K, COCO).\n",
        "question": {
            "statement": "What is the main advantage of using Multiway Transformers in the BEIT-3 model?",
            "options": [
                "It enables both deep fusion and modality-specific encoding",
                "It increases the model's computational efficiency",
                "It improves object detection accuracy",
                "It enhances image classification performance"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "2",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Open-set Semantic Segmentation for Point Clouds\nvia Adversarial Prototype Framework\nJianan Li1,2, Qiulei Dong∗,1,2,3\n1School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n2State Key Laboratory of Multimodal Artificial Intelligence Systems,\nInstitute of Automation, Chinese Academy of Sciences, Beijing, China\n3Center for Excellence in Brain Science and Intelligence Technology,\nChinese Academy of Sciences, Beijing, China\nlijianan211@mails.ucas.ac.cn, qldong@nlpr.ac.cn\nAbstract\nRecently, point cloud semantic segmentation has attracted\nmuch attention in computer vision.\nMost of the existing\nworks in literature assume that the training and testing\npoint clouds have the same object classes, but they are gen-\nerally invalid in many real-world scenarios for identifying\nthe 3D objects whose classes are not seen in the training\nset. To address this problem, we propose an Adversarial\nPrototype Framework (APF) for handling the open-set\n3D semantic segmentation task, which aims to identify 3D\nunseen-class points while maintaining the segmentation\nperformance on seen-class points.\nThe proposed APF\nconsists of a feature extraction module for extracting point\nfeatures, a prototypical constraint module, and a feature\nadversarial module. The prototypical constraint module is\ndesigned to learn prototypes for each seen class from point\nfeatures. The feature adversarial module utilizes generative\nadversarial networks to estimate the distribution of unseen-\nclass features implicitly, and the synthetic unseen-class\nfeatures are utilized to prompt the model to learn more\neffective point features and prototypes for discriminating\nunseen-class samples from the seen-class ones.\nExperi-\nmental results on two public datasets demonstrate that the\nproposed APF outperforms the comparative methods by a\nlarge margin in most cases.\n",
        "question": {
            "statement": "What is the main challenge addressed by the Adversarial Prototype Framework in point cloud semantic segmentation?",
            "options": [
                "Improving computational efficiency in point cloud processing",
                "Segmenting point clouds with varying densities",
                "Identifying 3D objects whose classes are not seen in the training set",
                "Handling noisy point cloud data"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "ACR: Attention Collaboration-based Regressor\nfor Arbitrary Two-Hand Reconstruction\nZhengdi Yu1,2, Shaoli Huang1*\n, Chen Fang1, Toby P. Breckon 2, Jue Wang\n1Tencent AI Lab\n2Durham University\n{zhengdiyu,shaolihuang,fcfang}@tencent.com\ntoby.breckon@durham.com\narphid@gmail.com\nAbstract\nReconstructing two hands from monocular RGB images\nis challenging due to frequent occlusion and mutual con-\nfusion. Existing methods mainly learn an entangled rep-\nresentation to encode two interacting hands, which are in-\ncredibly fragile to impaired interaction, such as truncated\nhands, separate hands, or external occlusion.\nThis pa-\nper presents ACR (Attention Collaboration-based Regres-\nsor), which makes the ﬁrst attempt to reconstruct hands in\narbitrary scenarios. To achieve this, ACR explicitly miti-\ngates interdependencies between hands and between parts\nby leveraging center and part-based attention for feature\nextraction. However, reducing interdependence helps re-\nlease the input constraint while weakening the mutual rea-\nsoning about reconstructing the interacting hands. Thus,\nbased on center attention, ACR also learns cross-hand\nprior that handle the interacting hands better.\nWe eval-\nuate our method on various types of hand reconstruction\ndatasets.\nOur method signiﬁcantly outperforms the best\ninteracting-hand approaches on the InterHand2.6M dataset\nwhile yielding comparable performance with the state-of-\nthe-art single-hand methods on the FreiHand dataset. More\nqualitative results on in-the-wild and hand-object interac-\ntion datasets and web images/videos further demonstrate\nthe effectiveness of our approach for arbitrary hand recon-\nstruction. Our code is available at this link 1.\n",
        "question": {
            "statement": "What is a major challenge in reconstructing two hands from monocular RGB images?",
            "options": [
                "Low-quality image resolution",
                "Insufficient training data",
                "Frequent occlusion and mutual confusion",
                "Limited computational resources"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Robust Unsupervised StyleGAN Image Restoration\nYohan Poirier-Ginter•⋄, Jean-Franc\n¸ois Lalonde•\n⋄Inria, Universit´\ne Cˆ\note d’Azur, •Universit´\ne Laval\nhttps://lvsn.github.io/RobustUnsupervised/\nFigure 1. Our unsupervised StyleGAN image restoration method is robust and effective across a variety of tasks (left, top to bottom:\nupsampling, denoising, deartifacting, and inpainting) and a wide range of degradation levels. Since it avoids the need for task-speciﬁc\nhyperparameter tuning, it can directly handle combinations of such degradations, e.g., pairs (center) or even all four (right).\nAbstract\nGAN-based image restoration inverts the generative\nprocess to repair images corrupted by known degrada-\ntions. Existing unsupervised methods must be carefully\ntuned for each task and degradation level. In this work,\nwe make StyleGAN image restoration robust: a single\nset of hyperparameters works across a wide range of\ndegradation levels. This makes it possible to handle\ncombinations of several degradations, without the need\nto retune. Our proposed approach relies on a 3-phase\nprogressive latent space extension and a conservative\noptimizer, which avoids the need for any additional reg-\nularization terms. Extensive experiments demonstrate\nrobustness on inpainting, upsampling, denoising, and\ndeartifacting at varying degradations levels, outperform-\ning other StyleGAN-based inversion techniques. Our\napproach also favorably compares to diffusion-based\nrestoration by yielding much more realistic inversion\nresults. Code is available at the above URL.\n",
        "question": {
            "statement": "What is a key advantage of the proposed StyleGAN image restoration method?",
            "options": [
                "It is limited to a specific type of image degradation.",
                "It requires task-specific hyperparameter tuning.",
                "It can handle combinations of several degradations without the need to retune.",
                "It produces less realistic inversion results."
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Cross-Guided Optimization of Radiance Fields with Multi-View Image\nSuper-Resolution for High-Resolution Novel View Synthesis\nYoungho Yoon and Kuk-Jin Yoon\nVisual Intelligence Lab., KAIST, Korea\n{dudgh1732,kjyoon}@kaist.ac.kr\nAbstract\nNovel View Synthesis (NVS) aims at synthesizing an im-\nage from an arbitrary viewpoint using multi-view images\nand camera poses. Among the methods for NVS, Neural\nRadiance Fields (NeRF) is capable of NVS for an arbitrary\nresolution as it learns a continuous volumetric representa-\ntion. However, radiance fields rely heavily on the spectral\ncharacteristics of coordinate-based networks. Thus, there\nis a limit to improving the performance of high-resolution\nnovel view synthesis (HRNVS). To solve this problem, we\npropose a novel framework using cross-guided optimiza-\ntion of the single-image super-resolution (SISR) and radi-\nance fields. We perform multi-view image super-resolution\n(MVSR) on train-view images during the radiance fields op-\ntimization process. It derives the updated SR result by fus-\ning the feature map obtained from SISR and voxel-based un-\ncertainty fields generated by integrated errors of train-view\nimages. By repeating the updates during radiance fields op-\ntimization, train-view images for radiance fields optimiza-\ntion have multi-view consistency and high-frequency de-\ntails simultaneously, ultimately improving the performance\nof HRNVS. Experiments of HRNVS and MVSR on various\nbenchmark datasets show that the proposed method signifi-\ncantly surpasses existing methods.\n",
        "question": {
            "statement": "What is the main limitation of Neural Radiance Fields (NeRF) when it comes to high-resolution novel view synthesis?",
            "options": [
                "lack of multi-view consistency",
                "insufficient training data",
                "computational complexity",
                "reliance on spectral characteristics of coordinate-based networks"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "3D-Aware Face Swapping\nYixuan Li\nChao Ma* Yichao Yan* Wenhan Zhu\nXiaokang Yang\nMoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China\n{lyx0208, chaoma, yanyichao, zhuwenhan823, xkyang}@sjtu.edu.cn\nFigure 1. Demonstration of the proposed 3dSwap. Given single-view source and target images, our method synthesizes high-fidelity and\nmulti-view-consistent images of the swapped faces and the corresponding geometries. More results can be found on our project page.\nAbstract\nFace swapping is an important research topic in com-\nputer vision with wide applications in entertainment and\nprivacy protection. Existing methods directly learn to swap\n2D facial images, taking no account of the geometric in-\nformation of human faces. In the presence of large pose\nvariance between the source and the target faces, there\nalways exist undesirable artifacts on the swapped face.\nIn this paper, we present a novel 3D-aware face swap-\nping method that generates high-fidelity and multi-view-\nconsistent swapped faces from single-view source and tar-\nget images. To achieve this, we take advantage of the strong\ngeometry and texture prior of 3D human faces, where the\n2D faces are projected into the latent space of a 3D genera-\ntive model. By disentangling the identity and attribute fea-\ntures in the latent space, we succeed in swapping faces in\na 3D-aware manner, being robust to pose variations while\ntransferring fine-grained facial details. Extensive experi-\nments demonstrate the superiority of our 3D-aware face\nswapping framework in terms of visual quality, identity sim-\nilarity, and multi-view consistency. Code is available at\nhttps://lyx0208.github.io/3dSwap.\n∗Corresponding authors.\n",
        "question": {
            "statement": "What is the main limitation of existing face swapping methods in computer vision?",
            "options": [
                "They are limited to swapping faces within the same ethnicity",
                "They cannot handle varying lighting conditions",
                "They do not consider the geometric information of human faces",
                "They require multiple views of the source and target faces"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game\nPerspective\nJinjing Zhu1*\nHaotian Bai1*\nLin Wang1,2†\n1 AI Thrust, HKUST(GZ)\n2 Dept. of CSE, HKUST\nzhujinjing.hkust@gmail.com, haotianwhite@outlook.com, linwang@ust.hk\nAbstract\nEndeavors have been recently made to leverage the vi-\nsion transformer (ViT) for the challenging unsupervised\ndomain adaptation (UDA) task. They typically adopt the\ncross-attention in ViT for direct domain alignment. However,\nas the performance of cross-attention highly relies on the\nquality of pseudo labels for targeted samples, it becomes\nless effective when the domain gap becomes large. We solve\nthis problem from a game theory’s perspective with the pro-\nposed model dubbed as PMTrans, which bridges source and\ntarget domains with an intermediate domain. Speciﬁcally,\nwe propose a novel ViT-based module called PatchMix that\neffectively builds up the intermediate domain, i.e., proba-\nbility distribution, by learning to sample patches from both\ndomains based on the game-theoretical models. This way,\nit learns to mix the patches from the source and target do-\nmains to maximize the cross entropy (CE), while exploiting\ntwo semi-supervised mixup losses in the feature and label\nspaces to minimize it. As such, we interpret the process of\nUDA as a min-max CE game with three players, including\nthe feature extractor, classiﬁer, and PatchMix, to ﬁnd the\nNash Equilibria. Moreover, we leverage attention maps from\nViT to re-weight the label of each patch by its importance,\nmaking it possible to obtain more domain-discriminative\nfeature representations. We conduct extensive experiments\non four benchmark datasets, and the results show that\nPMTrans signiﬁcantly surpasses the ViT-based and CNN-\nbased SoTA methods by +3.6% on Ofﬁce-Home, +1.4% on\nOfﬁce-31, and +17.7% on DomainNet, respectively. https:\n//vlis2022.github.io/cvpr23/PMTrans\n",
        "question": {
            "statement": "What is the primary challenge of using vision transformers for unsupervised domain adaptation tasks?",
            "options": [
                "The quality of pseudo labels for targeted samples",
                "Insufficient training data",
                "Limited computational resources",
                "Inability to handle high-dimensional data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Self-Supervised Video Forensics by Audio-Visual Anomaly Detection\nChao Feng\nZiyang Chen\nAndrew Owens\nUniversity of Michigan\nAbstract\nManipulated videos often contain subtle inconsistencies\nbetween their visual and audio signals. We propose a video\nforensics method, based on anomaly detection, that can\nidentify these inconsistencies, and that can be trained solely\nusing real, unlabeled data. We train an autoregressive model\nto generate sequences of audio-visual features, using feature\nsets that capture the temporal synchronization between video\nframes and sound. At test time, we then flag videos that the\nmodel assigns low probability. Despite being trained entirely\non real videos, our model obtains strong performance on the\ntask of detecting manipulated speech videos. Project site:\nhttps://cfeng16.github.io/audio-visual-forensics.\n",
        "question": {
            "statement": "What is the primary goal of the proposed video forensics method?",
            "options": [
                "To identify inconsistencies between visual and audio signals in manipulated videos",
                "To recognize objects in images",
                "To detect fake news in online articles",
                "To analyze sentiment from customer reviews"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "BUOL: A Bottom-Up Framework with Occupancy-aware Lifting for\nPanoptic 3D Scene Reconstruction From A Single Image\nTao Chu1,2*, Pan Zhang2, Qiong Liu1†, Jiaqi Wang2\n1 South China University of Technology\n2 Shanghai AI Laboratory\n{chutao, zhangpan, wangjiaqi}@pjlab.org.cn\nliuqiong@scut.edu.cn\nAbstract\nUnderstanding and modeling the 3D scene from a single\nimage is a practical problem. A recent advance proposes a\npanoptic 3D scene reconstruction task that performs both\n3D reconstruction and 3D panoptic segmentation from a\nsingle image. Although having made substantial progress,\nrecent works only focus on top-down approaches that fill\n2D instances into 3D voxels according to estimated depth,\nwhich hinders their performance by two ambiguities. (1)\ninstance-channel ambiguity: The variable ids of instances\nin each scene lead to ambiguity during filling voxel chan-\nnels with 2D information, confusing the following 3D re-\nfinement. (2) voxel-reconstruction ambiguity: 2D-to-3D\nlifting with estimated single view depth only propagates 2D\ninformation onto the surface of 3D regions, leading to ambi-\nguity during the reconstruction of regions behind the frontal\nview surface. In this paper, we propose BUOL, a Bottom-\nUp framework with Occupancy-aware Lifting to address the\ntwo issues for panoptic 3D scene reconstruction from a sin-\ngle image. For instance-channel ambiguity, a bottom-up\nframework lifts 2D information to 3D voxels based on de-\nterministic semantic assignments rather than arbitrary in-\nstance id assignments. The 3D voxels are then refined and\ngrouped into 3D instances according to the predicted 2D\ninstance centers. For voxel-reconstruction ambiguity, the\nestimated multi-plane occupancy is leveraged together with\ndepth to fill the whole regions of things and stuff.\nOur\nmethod shows a tremendous performance advantage over\nstate-of-the-art methods on synthetic dataset 3D-Front and\nreal-world dataset Matterport3D. Code and models will be\nreleased.\n",
        "question": {
            "statement": "What is the main limitation of current top-down approaches to 3D scene reconstruction from a single image?",
            "options": [
                "They rely heavily on manual annotation of the input image",
                "They require multiple images as input",
                "They suffer from instance-channel ambiguity and voxel-reconstruction ambiguity",
                "They are limited to reconstructing scenes with simple geometry"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "AeDet: Azimuth-invariant Multi-view 3D Object Detection\nChengjian Feng\nZequn Jie\nYujie Zhong\nXiangxiang Chu\nLin Ma\nMeituan Inc.\nAbstract\nRecent LSS-based multi-view 3D object detection has\nmade tremendous progress, by processing the features in\nBrid-Eye-View (BEV) via the convolutional detector. How-\never, the typical convolution ignores the radial symmetry\nof the BEV features and increases the difficulty of the de-\ntector optimization. To preserve the inherent property of\nthe BEV features and ease the optimization, we propose an\nazimuth-equivariant convolution (AeConv) and an azimuth-\nequivariant anchor. The sampling grid of AeConv is always\nin the radial direction, thus it can learn azimuth-invariant\nBEV features. The proposed anchor enables the detection\nhead to learn predicting azimuth-irrelevant targets. In ad-\ndition, we introduce a camera-decoupled virtual depth to\nunify the depth prediction for the images with different cam-\nera intrinsic parameters. The resultant detector is dubbed\nAzimuth-equivariant Detector (AeDet). Extensive experi-\nments are conducted on nuScenes, and AeDet achieves a\n62.0% NDS, surpassing the recent multi-view 3D object de-\ntectors such as PETRv2 and BEVDepth by a large mar-\ngin. Project page: https://fcjian.github.io/\naedet.\n",
        "question": {
            "statement": "What is the main advantage of using an azimuth-equivariant convolution in 3D object detection?",
            "options": [
                "It preserves the radial symmetry of Brid-Eye-View features",
                "It increases the computational efficiency of the detector",
                "It improves the accuracy of depth prediction",
                "It requires less training data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "2",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Focused and Collaborative Feedback Integration\nfor Interactive Image Segmentation\nQiaoqiao Wei\nHui Zhang\nJun-Hai Yong\nSchool of Software, BNRist, Tsinghua University, Beijing, China\nwqq18@mails.tsinghua.edu.cn\n{huizhang,yongjh}@tsinghua.edu.cn\nAbstract\nInteractive image segmentation aims at obtaining a seg-\nmentation mask for an image using simple user annotations.\nDuring each round of interaction, the segmentation result\nfrom the previous round serves as feedback to guide the\nuser’s annotation and provides dense prior information for\nthe segmentation model, effectively acting as a bridge be-\ntween interactions. Existing methods overlook the impor-\ntance of feedback or simply concatenate it with the orig-\ninal input, leading to underutilization of feedback and an\nincrease in the number of required annotations. To address\nthis, we propose an approach called Focused and Collabo-\nrative Feedback Integration (FCFI) to fully exploit the feed-\nback for click-based interactive image segmentation. FCFI\nfirst focuses on a local area around the new click and cor-\nrects the feedback based on the similarities of high-level\nfeatures. It then alternately and collaboratively updates the\nfeedback and deep features to integrate the feedback into the\nfeatures. The efficacy and efficiency of FCFI were validated\non four benchmarks, namely GrabCut, Berkeley, SBD, and\nDAVIS. Experimental results show that FCFI achieved new\nstate-of-the-art performance with less computational over-\nhead than previous methods. The source code is available at\nhttps://github.com/veizgyauzgyauz/FCFI.\n",
        "question": {
            "statement": "What is the main goal of interactive image segmentation?",
            "options": [
                "to develop a new feature extraction technique",
                "to automatically segment images without human intervention",
                "to obtain a segmentation mask for an image using simple user annotations",
                "to improve the quality of image compression"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Twin Contrastive Learning with Noisy Labels\nZhizhong Huang1\nJunping Zhang1\nHongming Shan2,3∗\n1 Shanghai Key Lab of Intelligent Information Processing, School of Computer Science,\nFudan University, Shanghai 200433, China\n2 Institute of Science and Technology for Brain-inspired Intelligence and MOE Frontiers Center\nfor Brain Science, Fudan University, Shanghai 200433, China\n3 Shanghai Center for Brain Science and Brain-inspired Technology, Shanghai 200031, China\n{zzhuang19, jpzhang, hmshan}@fudan.edu.cn\nAbstract\nLearning from noisy data is a challenging task that sig-\nniﬁcantly degenerates the model performance. In this paper,\nwe present TCL, a novel twin contrastive learning model\nto learn robust representations and handle noisy labels for\nclassiﬁcation. Speciﬁcally, we construct a Gaussian mix-\nture model (GMM) over the representations by injecting\nthe supervised model predictions into GMM to link label-\nfree latent variables in GMM with label-noisy annotations.\nThen, TCL detects the examples with wrong labels as the out-\nof-distribution examples by another two-component GMM,\ntaking into account the data distribution. We further propose\na cross-supervision with an entropy regularization loss that\nbootstraps the true targets from model predictions to handle\nthe noisy labels. As a result, TCL can learn discriminative\nrepresentations aligned with estimated labels through mixup\nand contrastive learning. Extensive experimental results\non several standard benchmarks and real-world datasets\ndemonstrate the superior performance of TCL. In particular,\nTCL achieves 7.5% improvements on CIFAR-10 with 90%\nnoisy label—an extremely noisy scenario. The source code\nis available at https://github.com/Hzzone/TCL.\n",
        "question": {
            "statement": "What technique does the Twin Contrastive Learning model use to handle noisy labels in classification?",
            "options": [
                "cross-supervision with entropy regularization",
                "early stopping",
                "transfer learning",
                "data augmentation"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "8",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 6,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Scaling up GANs for Text-to-Image Synthesis\nMinguk Kang1,3\nJun-Yan Zhu2\nRichard Zhang3\nJaesik Park1\nEli Shechtman3\nSylvain Paris3\nTaesung Park3\n1POSTECH\n2Carnegie Mellon University\n3Adobe Research\nAbstract\nThe recent success of text-to-image synthesis has taken\nthe world by storm and captured the general public’s imag-\nination. From a technical standpoint, it also marked a dras-\ntic change in the favored architecture to design generative\nimage models. GANs used to be the de facto choice, with\ntechniques like StyleGAN. With DALL·E 2, autoregressive\nand diffusion models became the new standard for large-\nscale generative models overnight. This rapid shift raises\na fundamental question: can we scale up GANs to benefit\nfrom large datasets like LAION? We find that na¨\nıvely in-\ncreasing the capacity of the StyleGAN architecture quickly\nbecomes unstable. We introduce GigaGAN, a new GAN ar-\nchitecture that far exceeds this limit, demonstrating GANs\nas a viable option for text-to-image synthesis. GigaGAN\noffers three major advantages. First, it is orders of mag-\nnitude faster at inference time, taking only 0.13 seconds\nto synthesize a 512px image.\nSecond, it can synthesize\nhigh-resolution images, for example, 16-megapixel images\nin 3.66 seconds. Finally, GigaGAN supports various latent\nspace editing applications such as latent interpolation, style\nmixing, and vector arithmetic operations.\n",
        "question": {
            "statement": "What is a key challenge when trying to increase the capacity of traditional GAN architectures like StyleGAN?",
            "options": [
                "Instability",
                "Computational cost",
                "Lack of interpretability",
                "Limited dataset size"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "3",
                "2",
                "2"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Universal Instance Perception as Object Discovery and Retrieval\nBin Yan1*\n, Yi Jiang2,†, Jiannan Wu3, Dong Wang1,\nPing Luo3, Zehuan Yuan2, Huchuan Lu1,4,†\n1 School of Information and Communication Engineering, Dalian University of Technology, China\n2 ByteDance 3 The University of Hong Kong 4 Peng Cheng Laboratory\nAbstract\nAll instance perception tasks aim at finding certain ob-\njects specified by some queries such as category names, lan-\nguage expressions, and target annotations, but this com-\nplete field has been split into multiple independent sub-\ntasks. In this work, we present a universal instance per-\nception model of the next generation, termed UNINEXT.\nUNINEXT reformulates diverse instance perception tasks\ninto a unified object discovery and retrieval paradigm and\ncan flexibly perceive different types of objects by simply\nchanging the input prompts. This unified formulation brings\nthe following benefits: (1) enormous data from different\ntasks and label vocabularies can be exploited for jointly\ntraining general instance-level representations, which is es-\npecially beneficial for tasks lacking in training data. (2)\nthe unified model is parameter-efficient and can save re-\ndundant computation when handling multiple tasks simul-\ntaneously. UNINEXT shows superior performance on 20\nchallenging benchmarks from 10 instance-level tasks in-\ncluding classical image-level tasks (object detection and\ninstance segmentation), vision-and-language tasks (refer-\nring expression comprehension and segmentation), and six\nvideo-level object tracking tasks.\nCode is available at\nhttps://github.com/MasterBin-IIAU/UNINEXT.\n",
        "question": {
            "statement": "What is a key advantage of using a unified model for instance perception tasks?",
            "options": [
                "It requires more computational resources",
                "It allows for jointly training general instance-level representations",
                "It only works for image-level tasks",
                "It reduces the number of tasks that need to be performed"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models\nfor 3D Generation\nHaochen Wang∗1\nXiaodan Du∗1\nJiahao Li∗1\nRaymond A. Yeh2\nGreg Shakhnarovich1\n1TTI-Chicago\n2Purdue University\nA zoomed out high-quality photo of Temple of Heaven\nA high quality photo of a delicious burger\nA high quality photo of a Victorian style wooden chair\nwith velvet upholstery\nA high quality photo of a classic silver muscle car\nFigure 1. Results for text-driven 3D generation using Score Jacobian Chaining with Stable Diffusion as the pretrained model.\nAbstract\nA diffusion model learns to predict a vector field of gradi-\nents. We propose to apply chain rule on the learned gradients,\nand back-propagate the score of a diffusion model through\nthe Jacobian of a differentiable renderer, which we instan-\ntiate to be a voxel radiance field. This setup aggregates 2D\nscores at multiple camera viewpoints into a 3D score, and re-\npurposes a pretrained 2D model for 3D data generation. We\nidentify a technical challenge of distribution mismatch that\narises in this application, and propose a novel estimation\nmechanism to resolve it. We run our algorithm on several off-\nthe-shelf diffusion image generative models, including the\nrecently released Stable Diffusion trained on the large-scale\nLAION 5B dataset.\n",
        "question": {
            "statement": "What technique is proposed to repurpose a 2D diffusion model for generating 3D data?",
            "options": [
                "Modifying the architecture of the 2D diffusion model to accommodate 3D inputs",
                "Applying the chain rule on the learned gradients and back-propagating the score through the Jacobian of a differentiable renderer",
                "Using transfer learning to fine-tune the 2D diffusion model on a small set of 3D examples",
                "Training the 2D diffusion model on a new dataset of 3D images"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "HARP: Personalized Hand Reconstruction from a Monocular RGB Video\nKorrawe Karunratanakul\nSergey Prokudin\nOtmar Hilliges\nSiyu Tang\nETH Z¨\nurich, Switzerland\n{korrawe.karunratanakul,sergey.prokudin,otmar.hilliges,siyu.tang}@inf.ethz.ch\nhttps://korrawe.github.io/harp-project/\nFigure 1. Given a short monocular video of a hand and a coarse hand pose and shape estimation for initialization, we reconstruct a photo-\nrealistic hand avatar exhibiting faithful personalized appearance and geometry using standard explicit representations and a differentiable\nrenderer without any neural networks. Compared to the baselines, our hand avatar demonstrates better-reconstructed geometry and appear-\nance. The hand avatar can be used to render high-ﬁdelity hand images in novel views and poses in real-time, which serves as a foundation\nfor many AR/VR applications.\nAbstract\nWe present HARP (HAnd Reconstruction and Personal-\nization), a personalized hand avatar creation approach that\ntakes a short monocular RGB video of a human hand as\ninput and reconstructs a faithful hand avatar exhibiting a\nhigh-ﬁdelity appearance and geometry. In contrast to the\nmajor trend of neural implicit representations, HARP mod-\nels a hand with a mesh-based parametric hand model, a\nvertex displacement map, a normal map, and an albedo\nwithout any neural components. The explicit nature of our\nrepresentation enables a truly scalable, robust, and efﬁcient\napproach to hand avatar creation as validated by our ex-\nperiments. HARP is optimized via gradient descent from\na short sequence captured by a hand-held mobile phone\nand can be directly used in AR/VR applications with real-\ntime rendering capability. To enable this, we carefully de-\nsign and implement a shadow-aware differentiable render-\ning scheme that is robust to high degree articulations and\nself-shadowing regularly present in hand motions, as well\nas challenging lighting conditions. It also generalizes to un-\nseen poses and novel viewpoints, producing photo-realistic\nrenderings of hand animations. Furthermore, the learned\nHARP representation can be used for improving 3D hand\npose estimation quality in challenging viewpoints. The key\nadvantages of HARP are validated by the in-depth analyses\non appearance reconstruction, novel view and novel pose\nsynthesis, and 3D hand pose reﬁnement. It is an AR/VR-\nready personalized hand representation that shows superior\nﬁdelity and scalability.\n",
        "question": {
            "statement": "What is a key advantage of using explicit representations in hand avatar creation?",
            "options": [
                "Flexibility",
                "Portability",
                "Scalability",
                "Realism"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "2",
                "10",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Point2Pix: Photo-Realistic Point Cloud Rendering via Neural Radiance Fields\nTao Hu1\nXiaogang Xu1*\nShu Liu2\nJiaya Jia1,2\n1 The Chinese University of Hong Kong\n2 SmartMore\n{taohu, xgxu, leojia}@cse.cuhk.edu.hk, liushuhust@gmail.com\nAbstract\nSynthesizing photo-realistic images from a point cloud is\nchallenging because of the sparsity of point cloud represen-\ntation. Recent Neural Radiance Fields and extensions are\nproposed to synthesize realistic images from 2D input. In\nthis paper, we present Point2Pix as a novel point renderer\nto link the 3D sparse point clouds with 2D dense image pix-\nels. Taking advantage of the point cloud 3D prior and NeRF\nrendering pipeline, our method can synthesize high-quality\nimages from colored point clouds, generally for novel in-\ndoor scenes.\nTo improve the efficiency of ray sampling,\nwe propose point-guided sampling, which focuses on valid\nsamples. Also, we present Point Encoding to build Multi-\nscale Radiance Fields that provide discriminative 3D point\nfeatures. Finally, we propose Fusion Encoding to efficiently\nsynthesize high-quality images. Extensive experiments on\nthe ScanNet and ArkitScenes datasets demonstrate the ef-\nfectiveness and generalization.\n",
        "question": {
            "statement": "What is the main challenge when synthesizing photo-realistic images from a point cloud?",
            "options": [
                "The complexity of the rendering pipeline",
                "The sparsity of point cloud representation",
                "The need for manual editing of the output image",
                "The lack of color information in the point cloud"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Reconstructing Signing Avatars From Video Using Linguistic Priors\nMaria-Paola Forte\nPeter Kulits\nChun-Hao Huang\nVasileios Choutas\nDimitrios Tzionas\nKatherine J. Kuchenbecker\nMichael J. Black\nMax Planck Institute for Intelligent Systems, Stuttgart and Tübingen, Germany\n{forte,kjk}@is.mpg.de\n{kulits,chuang2,vchoutas,dtzionas,black}@tue.mpg.de\nFigure 1. Given a monocular, in-the-wild video of a sign-language sign, SGNify automatically reconstructs a 3D body with accurate hand\npose, facial motion, and body pose. Note that motion blur obscures the finger articulations in several video frames; this is a common\nproblem. Our novel linguistic priors enable accurate 3D reconstruction despite such image degradation.\nAbstract\nSign language (SL) is the primary method of communica-\ntion for the 70 million Deaf people around the world. Video\ndictionaries of isolated signs are a core SL learning tool.\nReplacing these with 3D avatars can aid learning and en-\nable AR/VR applications, improving access to technology\nand online media. However, little work has attempted to\nestimate expressive 3D avatars from SL video; occlusion,\nnoise, and motion blur make this task difficult. We address\nthis by introducing novel linguistic priors that are univer-\nsally applicable to SL and provide constraints on 3D hand\npose that help resolve ambiguities within isolated signs.\nOur method, SGNify, captures fine-grained hand pose, fa-\ncial expression, and body movement fully automatically\nfrom in-the-wild monocular SL videos. We evaluate SGNify\nquantitatively by using a commercial motion-capture sys-\ntem to compute 3D avatars synchronized with monocular\nvideo. SGNify outperforms state-of-the-art 3D body-pose-\nand shape-estimation methods on SL videos. A perceptual\nstudy shows that SGNify’s 3D reconstructions are signifi-\ncantly more comprehensible and natural than those of pre-\nvious methods and are on par with the source videos. Code\nand data are available at sgnify.is.tue.mpg.de.\n",
        "question": {
            "statement": "What is the main benefit of replacing video dictionaries of isolated signs with 3D avatars in sign language learning?",
            "options": [
                "Increasing the speed of sign language communication",
                "Improving access to technology and online media",
                "Reducing the cost of sign language education",
                "Enhancing the accuracy of sign language interpretation"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Efficient View Synthesis and 3D-based Multi-Frame Denoising\nwith Multiplane Feature Representations\nThomas Tanay\nAleˇ\ns Leonardis\nMatteo Maggioni\nHuawei Noah’s Ark Lab\n{thomas.tanay,ales.leonardis,matteo.maggioni}@huawei.com\nAbstract\nWhile current multi-frame restoration methods combine\ninformation from multiple input images using 2D align-\nment techniques, recent advances in novel view synthesis\nare paving the way for a new paradigm relying on volu-\nmetric scene representations. In this work, we introduce\nthe first 3D-based multi-frame denoising method that sig-\nnificantly outperforms its 2D-based counterparts with lower\ncomputational requirements. Our method extends the mul-\ntiplane image (MPI) framework for novel view synthesis\nby introducing a learnable encoder-renderer pair manip-\nulating multiplane representations in feature space.\nThe\nencoder fuses information across views and operates in a\ndepth-wise manner while the renderer fuses information\nacross depths and operates in a view-wise manner.\nThe\ntwo modules are trained end-to-end and learn to separate\ndepths in an unsupervised way, giving rise to Multiplane\nFeature (MPF) representations. Experiments on the Spaces\nand Real Forward-Facing datasets as well as on raw burst\ndata validate our approach for view synthesis, multi-frame\ndenoising, and view synthesis under noisy conditions.\n",
        "question": {
            "statement": "What is the key advantage of the proposed 3D-based multi-frame denoising method compared to traditional 2D-based approaches?",
            "options": [
                "better handling of noisy data",
                "lower computational requirements",
                "improved depth estimation",
                "increased robustness to varying lighting conditions"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "9",
            "options": [
                "5",
                "8",
                "0",
                "0"
            ]
        },
        "difference": 3,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Decompose, Adjust, Compose: Effective Normalization by Playing with\nFrequency for Domain Generalization\nSangrok Lee*1, Jongseong Bae*2, and Ha Young Kim†1\n1Graduate School of Information, Yonsei University\n2Department of Artificial Intelligence, Yonsei University\n{lsrock1, js.bae, hayoung.kim}@yonsei.ac.kr\nAbstract\nDomain generalization (DG) is a principal task to eval-\nuate the robustness of computer vision models. Many previ-\nous studies have used normalization for DG. In normaliza-\ntion, statistics and normalized features are regarded as style\nand content, respectively. However, it has a content vari-\nation problem when removing style because the boundary\nbetween content and style is unclear. This study addresses\nthis problem from the frequency domain perspective, where\namplitude and phase are considered as style and content,\nrespectively. First, we verify the quantitative phase varia-\ntion of normalization through the mathematical derivation\nof the Fourier transform formula. Then, based on this, we\npropose a novel normalization method, PCNorm, which\neliminates style only as the preserving content through spec-\ntral decomposition.\nFurthermore, we propose advanced\nPCNorm variants, CCNorm and SCNorm, which ad-\njust the degrees of variations in content and style, respec-\ntively. Thus, they can learn domain-agnostic representa-\ntions for DG. With the normalization methods, we propose\nResNet-variant models, DAC-P and DAC-SC, which are ro-\nbust to the domain gap. The proposed models outperform\nother recent DG methods. The DAC-SC achieves an aver-\nage state-of-the-art performance of 65.6% on five datasets:\nPACS, VLCS, Office-Home, DomainNet, and TerraIncog-\nnita.\n",
        "question": {
            "statement": "What is a common challenge in normalization techniques used for domain generalization, particularly when trying to remove style information?",
            "options": [
                "Overfitting to specific domains",
                "Insufficient training data",
                "Inability to handle high-dimensional data",
                "Unclear boundaries between content and style"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Ultra-High Resolution Segmentation with Ultra-Rich Context: A Novel\nBenchmark\nDeyi Ji1,2\nFeng Zhao1* Hongtao Lu3,4* Mingyuan Tao2\nJieping Ye2\n1University of Science and Technology of China\n2Alibaba Group\n3Department of Computer Science and Engineering, Shanghai Jiao Tong University\n4MOE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University\njideyi@mail.ustc.edu.cn\nfzhao956@ustc.edu.cn\nhtlu@sjtu.edu.cn\n{juchen.tmy, yejieping.ye}@alibaba-inc.com\nAbstract\nWith the increasing interest and rapid development of\nmethods for Ultra-High Resolution (UHR) segmentation,\na large-scale benchmark covering a wide range of scenes\nwith full fine-grained dense annotations is urgently needed\nto facilitate the field.\nTo this end, the URUR dataset\nis introduced, in the meaning of Ultra-High Resolution\ndataset with Ultra-Rich Context.\nAs the name suggests,\nURUR contains amounts of images with high enough res-\nolution (3,008 images of size 5,120×5,120), a wide range\nof complex scenes (from 63 cities), rich-enough context\n(1 million instances with 8 categories) and fine-grained\nannotations (about 80 billion manually annotated pixels),\nwhich is far superior to all the existing UHR datasets\nincluding DeepGlobe, Inria Aerial, UDD, etc..\nMore-\nover, we also propose WSDNet, a more efficient and ef-\nfective framework for UHR segmentation especially with\nultra-rich context. Specifically, multi-level Discrete Wavelet\nTransform (DWT) is naturally integrated to release com-\nputation burden while preserve more spatial details, along\nwith a Wavelet Smooth Loss (WSL) to reconstruct orig-\ninal structured context and texture with a smooth con-\nstrain. Experiments on several UHR datasets demonstrate\nits state-of-the-art performance. The dataset is available at\nhttps://github.com/jankyee/URUR.\n",
        "question": {
            "statement": "What characteristic makes the URUR dataset stand out from other Ultra-High Resolution (UHR) datasets?",
            "options": [
                "it has a wide range of complex scenes and fine-grained annotations",
                "it is limited to a single city or scene",
                "it uses a specific type of neural network architecture",
                "it has low-resolution images"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "9",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 9,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis\nRishabh Dabral1\nMuhammad Hamza Mughal1,2\nVladislav Golyanik1\nChristian Theobalt1\n1Max Planck Institute for Informatics, SIC\n2Saarland University\nA person jumps multiple times\nA ﬁgure crawls \non the ﬂoor\nA person kicks \nwith right leg\nWalking counter-clockwise\nFigure 1. Our MoFusion approach synthesises long sequences of human motions in 3D from textual and audio inputs (e.g., by\nproviding music samples). Our model has significantly improved generalisability and realism, and can be conditioned on modalities like\ntext and audio. The resulting dance movements match the rhythm of the conditioning music, even if it is outside the training distribution.\nAbstract\nConventional methods for human motion synthesis have\neither been deterministic or have had to struggle with the\ntrade-off between motion diversity vs motion quality. In re-\nsponse to these limitations, we introduce MoFusion, i.e., a\nnew denoising-diffusion-based framework for high-quality\nconditional human motion synthesis that can synthesise\nlong, temporally plausible, and semantically accurate mo-\ntions based on a range of conditioning contexts (such as mu-\nsic and text). We also present ways to introduce well-known\nkinematic losses for motion plausibility within the motion-\ndiffusion framework through our scheduled weighting strat-\negy. The learned latent space can be used for several inter-\nactive motion-editing applications like in-betweening, seed-\nconditioning, and text-based editing, thus, providing cru-\ncial abilities for virtual-character animation and robotics.\nThrough comprehensive quantitative evaluations and a per-\nceptual user study, we demonstrate the effectiveness of Mo-\nFusion compared to the state of the art on established\nbenchmarks in the literature. We urge the reader to watch\nour supplementary video at https://vcai.mpi-inf.\nmpg.de/projects/MoFusion/.\n",
        "question": {
            "statement": "What is a key advantage of the MoFusion framework for human motion synthesis?",
            "options": [
                "It uses a physics-based approach to simulate human motion",
                "It can generate diverse and realistic motions while avoiding trade-offs between motion quality and diversity",
                "It requires manual annotation of motion data",
                "It is limited to synthesizing short motion sequences"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "DynaMask: Dynamic Mask Selection for Instance Segmentation\nRuihuang Li*\n,\nChenhang He*\n,\nShuai Li,\nYabin Zhang,\nLei Zhang†\nThe Hong Kong Polytechnic University\n{csrhli, csche, cslzhang}@comp.polyu.edu.hk\nAbstract\nThe\nrepresentative\ninstance\nsegmentation\nmethods\nmostly segment different object instances with a mask of\nthe ﬁxed resolution, e.g., 28 × 28 grid. However, a low-\nresolution mask loses rich details, while a high-resolution\nmask incurs quadratic computation overhead. It is a chal-\nlenging task to predict the optimal binary mask for each in-\nstance. In this paper, we propose to dynamically select suit-\nable masks for different object proposals. First, a dual-level\nFeature Pyramid Network (FPN) with adaptive feature ag-\ngregation is developed to gradually increase the mask grid\nresolution, ensuring high-quality segmentation of objects.\nSpeciﬁcally, an efﬁcient region-level top-down path (r-FPN)\nis introduced to incorporate complementary contextual and\ndetailed information from different stages of image-level\nFPN (i-FPN). Then, to alleviate the increase of computa-\ntion and memory costs caused by using large masks, we de-\nvelop a Mask Switch Module (MSM) with negligible compu-\ntational cost to select the most suitable mask resolution for\neach instance, achieving high efﬁciency while maintaining\nhigh segmentation accuracy. Without bells and whistles, the\nproposed method, namely DynaMask, brings consistent and\nnoticeable performance improvements over other state-of-\nthe-arts at a moderate computation overhead. The source\ncode: https://github.com/lslrh/DynaMask.\n",
        "question": {
            "statement": "What is a common limitation of traditional instance segmentation methods when it comes to predicting masks for object instances?",
            "options": [
                "They are limited to segmenting objects in specific orientations.",
                "They require a large amount of annotated training data.",
                "They use a fixed resolution mask that may lose rich details or incur computational overhead.",
                "They are unable to distinguish between different object classes."
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "ACSeg: Adaptive Conceptualization for Unsupervised Semantic Segmentation\nKehan Li1,3\nZhennan Wang2\nZesen Cheng1,3\nRunyi Yu1,3\nYian Zhao5\nGuoli Song2\nChang Liu4\nLi Yuan1,2,3*\nJie Chen1,2,3∗\n1 School of Electronic and Computer Engineering, Peking University, Shenzhen, China\n2 Peng Cheng Laboratory, Shenzhen, China\n3 AI for Science (AI4S)-Preferred Program, Peking University Shenzhen Graduate School, Shenzhen, China\n4 Department of Automation and BNRist, Tsinghua University, Beijing, China\n5 Dalian University of Technology\nAbstract\nRecently, self-supervised large-scale visual pre-training\nmodels have shown great promise in representing pixel-\nlevel semantic relationships, significantly promoting the de-\nvelopment of unsupervised dense prediction tasks, e.g., un-\nsupervised semantic segmentation (USS). The extracted re-\nlationship among pixel-level representations typically con-\ntains rich class-aware information that semantically iden-\ntical pixel embeddings in the representation space gather\ntogether to form sophisticated concepts. However, lever-\naging the learned models to ascertain semantically con-\nsistent pixel groups or regions in the image is non-trivial\nsince over/ under-clustering overwhelms the conceptualiza-\ntion procedure under various semantic distributions of dif-\nferent images. In this work, we investigate the pixel-level\nsemantic aggregation in self-supervised ViT pre-trained\nmodels as image Segmentation and propose the Adaptive\nConceptualization approach for USS, termed ACSeg. Con-\ncretely, we explicitly encode concepts into learnable proto-\ntypes and design the Adaptive Concept Generator (ACG),\nwhich adaptively maps these prototypes to informative con-\ncepts for each image. Meanwhile, considering the scene\ncomplexity of different images, we propose the modularity\nloss to optimize ACG independent of the concept number\nbased on estimating the intensity of pixel pairs belonging to\nthe same concept. Finally, we turn the USS task into clas-\nsifying the discovered concepts in an unsupervised manner.\nExtensive experiments with state-of-the-art results demon-\nstrate the effectiveness of the proposed ACSeg.\n",
        "question": {
            "statement": "What is the main challenge in leveraging self-supervised visual pre-training models for unsupervised semantic segmentation?",
            "options": [
                "poor quality of input images",
                "over/under-clustering overwhelms the conceptualization procedure",
                "insufficient computational resources",
                "limited availability of labeled data"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Iterative Geometry Encoding Volume for Stereo Matching\nGangwei Xu\nXianqi Wang\nXiaohuan Ding\nXin Yang†\nSchool of EIC, Huazhong University of Science and Technology\n{gwxu, xianqiw, dingxiaohuan, xinyang2014}@hust.edu.cn\nAbstract\nRecurrent All-Pairs Field Transforms (RAFT) has shown\ngreat potentials in matching tasks. However, all-pairs cor-\nrelations lack non-local geometry knowledge and have dif-\nficulties tackling local ambiguities in ill-posed regions. In\nthis paper, we propose Iterative Geometry Encoding Volume\n(IGEV-Stereo), a new deep network architecture for stereo\nmatching. The proposed IGEV-Stereo builds a combined\ngeometry encoding volume that encodes geometry and con-\ntext information as well as local matching details, and itera-\ntively indexes it to update the disparity map. To speed up the\nconvergence, we exploit GEV to regress an accurate starting\npoint for ConvGRUs iterations. Our IGEV-Stereo ranks 1st\non KITTI 2015 and 2012 (Reflective) among all published\nmethods and is the fastest among the top 10 methods. In\naddition, IGEV-Stereo has strong cross-dataset generaliza-\ntion as well as high inference efficiency. We also extend our\nIGEV to multi-view stereo (MVS), i.e. IGEV-MVS, which\nachieves competitive accuracy on DTU benchmark. Code\nis available at https://github.com/gangweiX/IGEV.\n",
        "question": {
            "statement": "What is a limitation of Recurrent All-Pairs Field Transforms (RAFT) in stereo matching tasks?",
            "options": [
                "Insufficient training data required",
                "Lack of non-local geometry knowledge",
                "Difficulty in handling reflective surfaces",
                "Inability to process color images"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Uncertainty-Aware Optimal Transport for Semantically Coherent\nOut-of-Distribution Detection\nFan Lu1,∗\nKai Zhu1,∗,‡\nWei Zhai1\nKecheng Zheng2\nYang Cao1,3,†\n1 University of Science and Technology of China\n2 Ant Group\n3 Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\n{lufan@mail., zkzy@mail., wzhai056@}ustc.edu.cn\nzkccloud@gmail.com\nforrest@ustc.edu.cn\nAbstract\nSemantically coherent out-of-distribution (SCOOD) de-\ntection aims to discern outliers from the intended data\ndistribution with access to unlabeled extra set.\nThe co-\nexistence of in-distribution and out-of-distribution samples\nwill exacerbate the model overfitting when no distinction\nis made.\nTo address this problem, we propose a novel\nuncertainty-aware optimal transport scheme. Our scheme\nconsists of an energy-based transport (ET) mechanism that\nestimates the fluctuating cost of uncertainty to promote\nthe assignment of semantic-agnostic representation, and an\ninter-cluster extension strategy that enhances the discrim-\nination of semantic property among different clusters by\nwidening the corresponding margin distance. Furthermore,\na T-energy score is presented to mitigate the magnitude gap\nbetween the parallel transport and classifier branches. Ex-\ntensive experiments on two standard SCOOD benchmarks\ndemonstrate the above-par OOD detection performance,\noutperforming the state-of-the-art methods by a margin of\n27.69% and 34.4% on FPR@95, respectively.\nCode is\navailable at https://github.com/LuFan31/ET-OOD.\n",
        "question": {
            "statement": "What is the main goal of semantically coherent out-of-distribution (SCOOD) detection?",
            "options": [
                "to distinguish between in-distribution and out-of-distribution samples",
                "to improve the accuracy of classification models",
                "to reduce the computational cost of machine learning algorithms",
                "to increase the robustness of models against adversarial attacks"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Annealing-based Label-Transfer Learning for Open World Object Detection\nYuqing Ma1, Hainan Li4, Zhange Zhang1, Jinyang Guo1,\nShanghang Zhang3, Ruihao Gong1, Xianglong Liu1,2,4*\n1 SKLSDE Lab, Beihang University, 2 Zhongguancun Laboratory,\n3 National Key Laboratory for Multimedia Information Processing, Peking University,\n4 Institute of Data Space, Hefei Comprehensive National Science Center\nAbstract\nOpen world object detection (OWOD) has attracted ex-\ntensive attention due to its practicability in the real world.\nPrevious OWOD works manually designed unknown-\ndiscover strategies to select unknown proposals from the\nbackground, suffering from uncertainties without appropri-\nate priors.\nIn this paper, we claim the learning of ob-\nject detection could be seen as an object-level feature-\nentanglement process, where unknown traits are propa-\ngated to the known proposals through convolutional opera-\ntions and could be distilled to benefit unknown recognition\nwithout manual selection. Therefore, we propose a simple\nyet effective Annealing-based Label-Transfer framework,\nwhich sufficiently explores the known proposals to allevi-\nate the uncertainties. Specifically, a Label-Transfer Learn-\ning paradigm is introduced to decouple the known and un-\nknown features, while a Sawtooth Annealing Scheduling\nstrategy is further employed to rebuild the decision bound-\naries of the known and unknown classes, thus promoting\nboth known and unknown recognition.\nMoreover, previ-\nous OWOD works neglected the trade-off of known and un-\nknown performance, and we thus introduce a metric called\nEquilibrium Index to comprehensively evaluate the effec-\ntiveness of the OWOD models. To the best of our knowledge,\nthis is the first OWOD work without manual unknown selec-\ntion. Extensive experiments conducted on the common-used\nbenchmark validate that our model achieves superior detec-\ntion performance (200% unknown mAP improvement with\nthe even higher known detection performance) compared\nto other state-of-the-art methods. Our code is available at\nhttps://github.com/DIG-Beihang/ALLOW.git.\n",
        "question": {
            "statement": "What is the primary goal of introducing the 'Equilibrium Index' metric in Open World Object Detection?",
            "options": [
                "To reduce the uncertainty of known proposals",
                "To promote the use of manual unknown discovery strategies",
                "To comprehensively evaluate the effectiveness of OWOD models",
                "To improve the detection performance of unknown objects"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "3D Human Keypoints Estimation from Point Clouds in the Wild\nwithout Human Labels\nZhenzhen Weng1*\nAlexander S. Gorban2\nJingwei Ji2\nMahyar Najibi2\nYin Zhou2\nDragomir Anguelov2\n1Stanford University\n2Waymo\nAbstract\nTraining a 3D human keypoint detector from point\nclouds in a supervised manner requires large volumes of\nhigh quality labels. While it is relatively easy to capture\nlarge amounts of human point clouds, annotating 3D key-\npoints is expensive, subjective, error prone and especially\ndifficult for long-tail cases (pedestrians with rare poses,\nscooterists, etc.).\nIn this work, we propose GC-KPL -\nGeometry Consistency inspired Key Point Leaning, an ap-\nproach for learning 3D human joint locations from point\nclouds without human labels. We achieve this by our novel\nunsupervised loss formulations that account for the struc-\nture and movement of the human body. We show that by\ntraining on a large training set from Waymo Open Dataset\n[21] without any human annotated keypoints, we are able\nto achieve reasonable performance as compared to the fully\nsupervised approach. Further, the backbone benefits from\nthe unsupervised training and is useful in downstream few-\nshot learning of keypoints, where fine-tuning on only 10 per-\ncent of the labeled training data gives comparable perfor-\nmance to fine-tuning on the entire set. We demonstrated that\nGC-KPL outperforms by a large margin over SoTA when\ntrained on entire dataset and efficiently leverages large vol-\numes of unlabeled data.\n",
        "question": {
            "statement": "What is a major challenge in training a 3D human keypoint detector from point clouds?",
            "options": [
                "capturing large amounts of human point clouds is difficult",
                "there is limited availability of point cloud data",
                "annotating 3D keypoints is expensive, subjective, error-prone, and difficult for long-tail cases",
                "human point clouds are not suitable for keypoint detection"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Exploring and Exploiting Uncertainty for Incomplete Multi-View Classification\nMengyao Xie‡, Zongbo Han‡, Changqing Zhang∗, Yichen Bai, Qinghua Hu\nCollege of Intelligence and Computing, Tianjin University\nzhangchangqing@tju.edu.cn\nAbstract\nClassifying incomplete multi-view data is inevitable since\narbitrary view missing widely exists in real-world applica-\ntions. Although great progress has been achieved, existing\nincomplete multi-view methods are still difficult to obtain a\ntrustworthy prediction due to the relatively high uncertainty\nnature of missing views. First, the missing view is of high\nuncertainty, and thus it is not reasonable to provide a single\ndeterministic imputation. Second, the quality of the imputed\ndata itself is of high uncertainty. To explore and exploit the\nuncertainty, we propose an Uncertainty-induced Incomplete\nMulti-View Data Classification (UIMC) model to classify\nthe incomplete multi-view data under a stable and reliable\nframework. We construct a distribution and sample multiple\ntimes to characterize the uncertainty of missing views, and\nadaptively utilize them according to the sampling quality.\nAccordingly, the proposed method realizes more perceivable\nimputation and controllable fusion. Specifically, we model\neach missing data with a distribution conditioning on the\navailable views and thus introducing uncertainty. Then an\nevidence-based fusion strategy is employed to guarantee the\ntrustworthy integration of the imputed views. Extensive ex-\nperiments are conducted on multiple benchmark data sets\nand our method establishes a state-of-the-art performance\nin terms of both performance and trustworthiness.\n",
        "question": {
            "statement": "What is a major challenge in classifying incomplete multi-view data?",
            "options": [
                "The lack of domain expertise",
                "The large amount of available data",
                "The high uncertainty nature of missing views",
                "The complexity of the classification algorithm"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "NeuMap: Neural Coordinate Mapping by\nAuto-Transdecoder for Camera Localization\nShitao Tang\nSicong Tang\nAndrea Tagliasacchi\nPing Tan\nYasutaka Furukawa\nSimon Fraser University\n{shitao tang, sta105, andrea.tagliasacchi, pingtan, furukawa}@sfu.ca\nScene specific\nScene agnostic\nAachen Night Performance\nTransformer\nQuery\nCodes\nCNN\nSparse Coords.\nQuery\nDense Coords.\nCNN\nCoordinate \nRegression\nTransformer\nQuery\nDatabase\nCNN\nCNN\nFeature Matches\nAccuracy\nData size (GB)\nFeature \nMatching\nNeuMap \n(Ours)\n0\n.25 .05 0.1 0.2 0.4 0.8 1.6 3.2 6.4 12.8\nESAC\n0\n20\n40\n60\n80\n100\nDSAC++\nQP+SIFT\nActive Search\nPixelLoc\nNeuMap\nCascaded\nSqueezer\nHLoc\nFigure 1. The paper presents neural coordinate mapping (NeuMap) for camera localization. NeuMap encodes a scene into a set of codes\nand uses a scene-agnostic transformer to decode the coordinates of key-points in a query image. The right compares the localization\naccuracy and the data size for Aachen Night benchmark. The accuracy is averaged over three translation/rotation error thresholds (0.25m,\n2◦), (0.5m, 5◦), or (5m, 10◦). NeuMap and Squeezer control representation sizes and are illustrated by curves with multiple data points.\nAbstract\nThis paper presents an end-to-end neural mapping\nmethod for camera localization, encoding a whole scene\ninto a grid of latent codes, with which a Transformer-\nbased auto-decoder regresses 3D coordinates of query pix-\nels. State-of-the-art camera localization methods require\neach scene to be stored as a 3D point cloud with per-point\nfeatures, which takes several gigabytes of storage per scene.\nWhile compression is possible, the performance drops sig-\nnificantly at high compression rates. NeuMap achieves ex-\ntremely high compression rates with minimal performance\ndrop by using 1) learnable latent codes to store scene in-\nformation and 2) a scene-agnostic Transformer-based auto-\ndecoder to infer coordinates for a query pixel. The scene-\nagnostic network design also learns robust matching priors\nby training with large-scale data, and further allows us to\njust optimize the codes quickly for a new scene while fixing\nthe network weights. Extensive evaluations with five bench-\nmarks show that NeuMap outperforms all the other coor-\ndinate regression methods significantly and reaches similar\nperformance as the feature matching methods while hav-\ning a much smaller scene representation size. For example,\nNeuMap achieves 39.1% accuracy in Aachen night bench-\nmark with only 6MB of data, while other compelling meth-\nods require 100MB or a few gigabytes and fail completely\nunder high compression settings. The codes are available\nat https://github.com/Tangshitao/NeuMap.\n",
        "question": {
            "statement": "What is the primary advantage of using learnable latent codes to store scene information in camera localization?",
            "options": [
                "High compression rates with minimal performance drop",
                "Improved feature extraction",
                "Faster inference times",
                "Robust matching priors"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Bias in Pruned Vision Models: In-Depth Analysis and Countermeasures\nEugenia Ioﬁnova\nIST Austria\nAlexandra Peste\nIST Austria\nDan Alistarh\nIST Austria & Neural Magic\nAbstract\nPruning—that is, setting a signiﬁcant subset of the pa-\nrameters of a neural network to zero—is one of the most\npopular methods of model compression. Yet, several recent\nworks have raised the issue that pruning may induce or ex-\nacerbate bias in the output of the compressed model. De-\nspite existing evidence for this phenomenon, the relation-\nship between neural network pruning and induced bias is\nnot well-understood. In this work, we systematically inves-\ntigate and characterize this phenomenon in Convolutional\nNeural Networks for computer vision. First, we show that\nit is in fact possible to obtain highly-sparse models, e.g.\nwith less than 10% remaining weights, which do not de-\ncrease in accuracy nor substantially increase in bias when\ncompared to dense models.\nAt the same time, we also\nﬁnd that, at higher sparsities, pruned models exhibit higher\nuncertainty in their outputs, as well as increased correla-\ntions, which we directly link to increased bias. We pro-\npose easy-to-use criteria which, based only on the uncom-\npressed model, establish whether bias will increase with\npruning, and identify the samples most susceptible to bi-\nased predictions post-compression. Our code can be found\nat https://github.com/IST-DASLab/pruned-\nvision-model-bias.\n",
        "question": {
            "statement": "What is a potential drawback of pruning in neural networks?",
            "options": [
                "Enhanced interpretability",
                "Increased bias in the output",
                "Improved computational efficiency",
                "Reduced overfitting"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Probing neural representations of scene perception in a hippocampally\ndependent task using artificial neural networks\nMarkus Frey1,2\nChristian F. Doeller1,2\nCaswell Barry3\n1Kavli Institute for Systems Neuroscience, NTNU, Norway\n2Max-Planck-Insitute for Human Cognitive and Brain Sciences, Germany\n3Cell & Developmental Biology, UCL, United Kingdom\nAbstract\nDeep artificial neural networks (DNNs) trained through\nbackpropagation provide effective models of the mam-\nmalian visual system, accurately capturing the hierarchy of\nneural responses through primary visual cortex to inferior\ntemporal cortex (IT) [41,43]. However, the ability of these\nnetworks to explain representations in higher cortical areas\nis relatively lacking and considerably less well researched.\nFor example, DNNs have been less successful as a model\nof the egocentric to allocentric transformation embodied by\ncircuits in retrosplenial and posterior parietal cortex. We\ndescribe a novel scene perception benchmark inspired by a\nhippocampal dependent task, designed to probe the ability\nof DNNs to transform scenes viewed from different egocen-\ntric perspectives. Using a network architecture inspired by\nthe connectivity between temporal lobe structures and the\nhippocampus, we demonstrate that DNNs trained using a\ntriplet loss can learn this task. Moreover, by enforcing a\nfactorized latent space, we can split information propaga-\ntion into ”what” and ”where” pathways, which we use to\nreconstruct the input. This allows us to beat the state-of-\nthe-art for unsupervised object segmentation on the CATER\nand MOVi-A,B,C benchmarks.\n",
        "question": {
            "statement": "What type of neural networks have been successful in modeling the mammalian visual system?",
            "options": [
                "Deep artificial neural networks",
                "Convolutional neural networks",
                "Shallow artificial neural networks",
                "Recurrent neural networks"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "9",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "HRDFuse: Monocular 360◦Depth Estimation by Collaboratively Learning\nHolistic-with-Regional Depth Distributions\nHao Ai1\nZidong Cao1\nYan-Pei Cao2\nYing Shan2\nLin Wang1,3*\n1AI Thrust, HKUST(GZ)\n2ARC Lab, Tencent PCG\n3Dept. of CSE, HKUST\nhai033@connect.hkust-gz.edu.cn, caozidong1996@gmail.com\ncaoyanpei@gmail.com, yingsshan@tencent.com, linwang@ust.hk\nAbstract\nDepth estimation from a monocular 360◦image is a bur-\ngeoning problem owing to its holistic sensing of a scene.\nRecently, some methods, e.g., OmniFusion, have applied\nthe tangent projection (TP) to represent a 360◦image and\npredicted depth values via patch-wise regressions, which\nare merged to get a depth map with equirectangular pro-\njection (ERP) format. However, these methods suffer from\n1) non-trivial process of merging plenty of patches; 2) cap-\nturing less holistic-with-regional contextual information by\ndirectly regressing the depth value of each pixel. In this\npaper, we propose a novel framework, HRDFuse, that sub-\ntly combines the potential of convolutional neural networks\n(CNNs) and transformers by collaboratively learning the\nholistic contextual information from the ERP and the re-\ngional structural information from the TP. Firstly, we pro-\npose a spatial feature alignment (SFA) module that learns\nfeature similarities between the TP and ERP to aggregate\nthe TP features into a complete ERP feature map in a pixel-\nwise manner. Secondly, we propose a collaborative depth\ndistribution classification (CDDC) module that learns the\nholistic-with-regional histograms capturing the ERP and\nTP depth distributions. As such, the final depth values can\nbe predicted as a linear combination of histogram bin cen-\nters. Lastly, we adaptively combine the depth predictions\nfrom ERP and TP to obtain the final depth map. Extensive\nexperiments show that our method predicts more smooth\nand accurate depth results while achieving favorably bet-\nter results than the SOTA methods.\nMultimedia Material\nFor videos, code, demo and more information, you can\nvisit https://VLIS2022.github.io/HRDFuse/\n",
        "question": {
            "statement": "What is the main limitation of previous monocular 360° depth estimation methods?",
            "options": [
                "High computational requirements",
                "Capturing less holistic-with-regional contextual information",
                "Lack of training data",
                "Difficulty in handling 360° images"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Differentiable Shadow Mapping for Efficient Inverse Graphics\nMarkus Worchel\nMarc Alexa\nTU Berlin\nAbstract\nWe show how shadows can be efficiently generated in\ndifferentiable rendering of triangle meshes. Our central ob-\nservation is that pre-filtered shadow mapping, a technique\nfor approximating shadows based on rendering from the\nperspective of a light, can be combined with existing dif-\nferentiable rasterizers to yield differentiable visibility infor-\nmation. We demonstrate at several inverse graphics prob-\nlems that differentiable shadow maps are orders of mag-\nnitude faster than differentiable light transport simulation\nwith similar accuracy – while differentiable rasterization\nwithout shadows often fails to converge.\n",
        "question": {
            "statement": "What is the main advantage of using differentiable shadow mapping in inverse graphics problems?",
            "options": [
                "Better handling of complex geometries",
                "Increased memory usage",
                "Faster computation time",
                "Improved visual realism"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "3",
                "0",
                "10",
                "2"
            ]
        },
        "difference": 7,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Center Focusing Network for Real-Time LiDAR Panoptic Segmentation\nXiaoyan Li1,2\nGang Zhang3\nBoyue Wang1,2\nYongli Hu1,2\nBaocai Yin1,2\n1Beijing Municipal Key Lab of Multimedia and Intelligent Software Technology\n2Beijing Institute of Artificial Intelligence, Faculty of Information Technology,\nBeijing University of Technology, Beijing 100124, China\n3Mogo Auto Intelligence and Telemetics Information Technology Co. Ltd.\n{xiaoyan.li,wby,huyongli,ybc}@bjut.edu.cn\nzhanggang11021136@gmail.com\nAbstract\nLiDAR panoptic segmentation facilitates an autonomous\nvehicle to comprehensively understand the surrounding ob-\njects and scenes and is required to run in real time. The\nrecent proposal-free methods accelerate the algorithm, but\ntheir effectiveness and efficiency are still limited owing to\nthe difficulty of modeling non-existent instance centers and\nthe costly center-based clustering modules. To achieve ac-\ncurate and real-time LiDAR panoptic segmentation, a novel\ncenter focusing network (CFNet) is introduced. Specifically,\nthe center focusing feature encoding (CFFE) is proposed to\nexplicitly understand the relationships between the origi-\nnal LiDAR points and virtual instance centers by shifting\nthe LiDAR points and filling in the center points. More-\nover, to leverage the redundantly detected centers, a fast\ncenter deduplication module (CDM) is proposed to select\nonly one center for each instance. Experiments on the Se-\nmanticKITTI and nuScenes panoptic segmentation bench-\nmarks demonstrate that our CFNet outperforms all existing\nmethods by a large margin and is 1.6 times faster than the\nmost efficient method.\n",
        "question": {
            "statement": "What is a major challenge in current LiDAR panoptic segmentation methods?",
            "options": [
                "Segmenting objects from different angles",
                "Modeling non-existent instance centers",
                "Integrating with other sensor modalities",
                "Processing large amounts of data"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "MethaneMapper: Spectral Absorption aware Hyperspectral Transformer for\nMethane Detection\nSatish Kumar\nsatishkumar@ucsb.edu\nIvan Arevalo\nifa@ucsb.edu\nASM Iftekhar\niftekhar@ucsb.edu\nB S Manjunath\nmanj@ucsb.edu\nDepartment of Electrical and Computer Engineering\nUniversity of California Santa Barbara\nAbstract\nMethane (CH4) is the chief contributor to global cli-\nmate change.\nRecent Airborne Visible-Infrared Imaging\nSpectrometer-Next Generation (AVIRIS-NG) has been very\nuseful in quantitative mapping of methane emissions. Ex-\nisting methods for analyzing this data are sensitive to local\nterrain conditions, often require manual inspection from do-\nmain experts, prone to significant error and hence are not\nscalable. To address these challenges, we propose a novel\nend-to-end spectral absorption wavelength aware trans-\nformer network, MethaneMapper, to detect and quantify the\nemissions. MethaneMapper introduces two novel modules\nthat help to locate the most relevant methane plume regions\nin the spectral domain and uses them to localize these ac-\ncurately. Thorough evaluation shows that MethaneMapper\nachieves 0.63 mAP in detection and reduces the model size\n(by 5×) compared to the current state of the art. In ad-\ndition, we also introduce a large-scale dataset of methane\nplume segmentation mask for over 1200 AVIRIS-NG flight\nlines from 2015-2022. It contains over 4000 methane plume\nsites. Our dataset will provide researchers the opportunity\nto develop and advance new methods for tackling this chal-\nlenging green-house gas detection problem with significant\nbroader social impact. Dataset and source code link1.\n",
        "question": {
            "statement": "What is the primary goal of the MethaneMapper system?",
            "options": [
                "To detect and quantify methane emissions",
                "To create a large-scale dataset of methane plume segmentation masks",
                "To develop a new hyperspectral transformer",
                "To analyze local terrain conditions"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Making Vision Transformers Efficient from A Token Sparsification View\nShuning Chang1* Pichao Wang2†‡\nMing Lin2‡\nFan Wang2\nDavid Junhao Zhang1\nRong Jin2\nMike Zheng Shou1†\n1Show Lab, National University of Singapore\n2Alibaba Group\nAbstract\nThe quadratic computational complexity to the number\nof tokens limits the practical applications of Vision Trans-\nformers (ViTs). Several works propose to prune redundant\ntokens to achieve efficient ViTs. However, these methods\ngenerally suffer from (i) dramatic accuracy drops, (ii) ap-\nplication difficulty in the local vision transformer, and (iii)\nnon-general-purpose networks for downstream tasks.\nIn\nthis work, we propose a novel Semantic Token ViT (STViT),\nfor efficient global and local vision transformers, which can\nalso be revised to serve as backbone for downstream tasks.\nThe semantic tokens represent cluster centers, and they are\ninitialized by pooling image tokens in space and recovered\nby attention, which can adaptively represent global or local\nsemantic information. Due to the cluster properties, a few\nsemantic tokens can attain the same effect as vast image to-\nkens, for both global and local vision transformers. For in-\nstance, only 16 semantic tokens on DeiT-(Tiny,Small,Base)\ncan achieve the same accuracy with more than 100% in-\nference speed improvement and nearly 60% FLOPs reduc-\ntion; on Swin-(Tiny,Small,Base), we can employ 16 seman-\ntic tokens in each window to further speed it up by around\n20% with slight accuracy increase. Besides great success\nin image classification, we also extend our method to video\nrecognition. In addition, we design a STViT-R(ecovery) net-\nwork to restore the detailed spatial information based on\nthe STViT, making it work for downstream tasks, which is\npowerless for previous token sparsification methods. Ex-\nperiments demonstrate that our method can achieve com-\npetitive results compared to the original networks in object\ndetection and instance segmentation, with over 30% FLOPs\nreduction for backbone.\n",
        "question": {
            "statement": "What is a major limitation of Vision Transformers (ViTs) that several works have attempted to address?",
            "options": [
                "quadratic computational complexity",
                "limited training data",
                "poor interpretability",
                "large model size"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Semantic-Conditional Diffusion Networks for Image Captioning*\nJianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jianlin Feng, Hongyang Chao, Tao Mei\nSchool of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China\nHiDream.ai Inc.\n{jianjieluo.sysu,yehaoli.sysu,panyw.ustc,tingyao.ustc}@gmail.com;\n{fengjlin,isschhy}@mail.sysu.edu.cn;tmei@hidream.ai\nAbstract\nRecent advances on text-to-image generation have wit-\nnessed the rise of diffusion models which act as power-\nful generative models.\nNevertheless, it is not trivial to\nexploit such latent variable models to capture the depen-\ndency among discrete words and meanwhile pursue com-\nplex visual-language alignment in image captioning. In this\npaper, we break the deeply rooted conventions in learning\nTransformer-based encoder-decoder, and propose a new\ndiffusion model based paradigm tailored for image cap-\ntioning, namely Semantic-Conditional Diffusion Networks\n(SCD-Net).\nTechnically, for each input image, we first\nsearch the semantically relevant sentences via cross-modal\nretrieval model to convey the comprehensive semantic in-\nformation. The rich semantics are further regarded as se-\nmantic prior to trigger the learning of Diffusion Trans-\nformer, which produces the output sentence in a diffusion\nprocess. In SCD-Net, multiple Diffusion Transformer struc-\ntures are stacked to progressively strengthen the output\nsentence with better visional-language alignment and lin-\nguistical coherence in a cascaded manner. Furthermore,\nto stabilize the diffusion process, a new self-critical se-\nquence training strategy is designed to guide the learn-\ning of SCD-Net with the knowledge of a standard autore-\ngressive Transformer model.\nExtensive experiments on\nCOCO dataset demonstrate the promising potential of us-\ning diffusion models in the challenging image captioning\ntask. Source code is available at https://github.\ncom/YehLi/xmodaler/tree/master/configs/\nimage_caption/scdnet.\n",
        "question": {
            "statement": "What is the main advantage of using diffusion models in image captioning tasks?",
            "options": [
                "They require less training data",
                "They are faster than traditional encoder-decoder models",
                "They can capture complex dependencies between discrete words and visual elements",
                "They are only suitable for generating short captions"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "8",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "ScaleFL: Resource-Adaptive Federated Learning with Heterogeneous Clients\nFatih Ilhan\nGeorgia Institute of Technology\nAtlanta, GA\nfilhan@gatech.edu\nGong Su\nIBM Research\nYorktown Heights, NY\ngongsu@us.ibm.com\nLing Liu\nGeorgia Institute of Technology\nAtlanta, GA\nling.liu@cc.gatech.edu\nAbstract\nFederated learning (FL) is an attractive distributed\nlearning paradigm supporting real-time continuous learn-\ning and client privacy by default. In most FL approaches,\nall edge clients are assumed to have sufﬁcient computation\ncapabilities to participate in the learning of a deep neural\nnetwork (DNN) model. However, in real-life applications,\nsome clients may have severely limited resources and can\nonly train a much smaller local model. This paper presents\nScaleFL, a novel FL approach with two distinctive mecha-\nnisms to handle resource heterogeneity and provide an equi-\ntable FL framework for all clients. First, ScaleFL adaptively\nscales down the DNN model along width and depth dimen-\nsions by leveraging early exits to ﬁnd the best-ﬁt models for\nresource-aware local training on distributed clients. In this\nway, ScaleFL provides an efﬁcient balance of preserving\nbasic and complex features in local model splits with vari-\nous sizes for joint training while enabling fast inference for\nmodel deployment. Second, ScaleFL utilizes self-distillation\namong exit predictions during training to improve aggre-\ngation through knowledge transfer among subnetworks. We\nconduct extensive experiments on benchmark CV (CIFAR-\n10/100, ImageNet) and NLP datasets (SST-2, AgNews). We\ndemonstrate that ScaleFL outperforms existing representa-\ntive heterogeneous FL approaches in terms of global/local\nmodel performance and provides inference efﬁciency, with\nup to 2x latency and 4x model size reduction with negligible\nperformance drop below 2%.\n",
        "question": {
            "statement": "What is a key challenge in federated learning approaches that ScaleFL aims to address?",
            "options": [
                "Resource heterogeneity among edge clients",
                " Limited data availability at each client",
                "Insufficient communication bandwidth between clients",
                "Inability to use deep neural network models"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "MotionDiffuser: Controllable Multi-Agent Motion Prediction using Diffusion\nChiyu “Max” Jiang∗\nAndre Cornman∗\nCheolho Park\nBenjamin Sapp\nYin Zhou\nDragomir Anguelov\n∗equal contribution\nWaymo LLC\nFigure 1. MotionDiffuser is a learned representation for the distribution of multi-agent trajectories based on diffusion models. During\ninference, samples from the predicted joint future distribution are ﬁrst drawn i.i.d. from a random normal distribution (leftmost column),\nand gradually denoised using a learned denoiser into the ﬁnal predictions (rightmost column). Diffusion allows us to learn a diverse,\nmultimodal distribution over joint outputs (top right). Furthermore, guidance in the form of a differentiable cost function can be applied at\ninference time to obtain results satisfying additional priors and constraints (bottom right).\nAbstract\nWe present MotionDiffuser, a diffusion based represen-\ntation for the joint distribution of future trajectories over\nmultiple agents. Such representation has several key ad-\nvantages: ﬁrst, our model learns a highly multimodal dis-\ntribution that captures diverse future outcomes. Second, the\nsimple predictor design requires only a single L2 loss train-\ning objective, and does not depend on trajectory anchors.\nThird, our model is capable of learning the joint distribu-\ntion for the motion of multiple agents in a permutation-\ninvariant manner. Furthermore, we utilize a compressed\ntrajectory representation via PCA, which improves model\nperformance and allows for efﬁcient computation of the\nexact sample log probability.\nSubsequently, we propose\na general constrained sampling framework that enables\ncontrolled trajectory sampling based on differentiable cost\nfunctions. This strategy enables a host of applications such\nas enforcing rules and physical priors, or creating tai-\nlored simulation scenarios. MotionDiffuser can be com-\nbined with existing backbone architectures to achieve top\nmotion forecasting results. We obtain state-of-the-art re-\nsults for multi-agent motion prediction on the Waymo Open\nMotion Dataset.\n",
        "question": {
            "statement": "What advantage does the MotionDiffuser model have in terms of its training objective?",
            "options": [
                "It relies on hand-crafted features",
                "It requires a complex combination of multiple losses",
                "It only requires a single L2 loss training objective",
                "It needs a large amount of annotated data"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Devil’s on the Edges: Selective Quad Attention for Scene Graph Generation\nDeunsol Jung\nSanghyun Kim\nWon Hwa Kim\nMinsu Cho\nPohang University of Science and Technology (POSTECH), South Korea\nhttp://cvlab.postech.ac.kr/research/SQUAT\nAbstract\nScene graph generation aims to construct a semantic\ngraph structure from an image such that its nodes and edges\nrespectively represent objects and their relationships. One\nof the major challenges for the task lies in the presence\nof distracting objects and relationships in images; contex-\ntual reasoning is strongly distracted by irrelevant objects or\nbackgrounds and, more importantly, a vast number of irrel-\nevant candidate relations. To tackle the issue, we propose\nthe Selective Quad Attention Network (SQUAT) that learns\nto select relevant object pairs and disambiguate them via di-\nverse contextual interactions. SQUAT consists of two main\ncomponents: edge selection and quad attention. The edge\nselection module selects relevant object pairs, i.e., edges in\nthe scene graph, which helps contextual reasoning, and the\nquad attention module then updates the edge features us-\ning both edge-to-node and edge-to-edge cross-attentions to\ncapture contextual information between objects and object\npairs.\nExperiments demonstrate the strong performance\nand robustness of SQUAT, achieving the state of the art on\nthe Visual Genome and Open Images v6 benchmarks.\n",
        "question": {
            "statement": "What is a major challenge in scene graph generation?",
            "options": [
                "Limited computing resources",
                "Distracting objects and relationships in images",
                "Insufficient training data",
                "Complexity of scene graphs"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "On the Difficulty of Unpaired Infrared-to-Visible Video Translation:\nFine-Grained Content-Rich Patches Transfer\nZhenjie Yu1 \nShuang Li1 \f Yirui Shen1 \nChi Harold Liu1 \nShuigen Wang2 \n1Beijing Institute of Technology \n2Yantai IRay Technologies Lt. Co.\n{zjyu, shuangli, yiruishen, chiliu}@bit.edu.cn \nshuigen.wang@iraytek.com\nAbstract\nExplicit visible videos can provide sufficient visual in-\nformation and facilitate vision applications. Unfortunately,\nthe image sensors of visible cameras are sensitive to light\nconditions like darkness or overexposure. To make up for\nthis, recently, infrared sensors capable of stable imaging\nhave received increasing attention in autonomous driving\nand monitoring.\nHowever, most prosperous vision mod-\nels are still trained on massive clear visible data, facing\nhuge visual gaps when deploying to infrared imaging sce-\nnarios. In such cases, transferring the infrared video to a\ndistinct visible one with fine-grained semantic patterns is\na worthwhile endeavor. Previous works improve the out-\nputs by equally optimizing each patch on the translated vis-\nible results, which is unfair for enhancing the details on\ncontent-rich patches due to the long-tail effect of pixel dis-\ntribution. Here we propose a novel CPTrans framework\nto tackle the challenge via balancing gradients of different\npatches, achieving the fine-grained Content-rich Patches\nTransferring. Specifically, the content-aware optimization\nmodule encourages model optimization along gradients of\ntarget patches, ensuring the improvement of visual details.\nAdditionally, the content-aware temporal normalization\nmodule enforces the generator to be robust to the motions of\ntarget patches. Moreover, we extend the existing dataset In-\nfraredCity to more challenging adverse weather conditions\n(rain and snow), dubbed as InfraredCity-Adverse1. Exten-\nsive experiments show that the proposed CPTrans achieves\nstate-of-the-art performance under diverse scenes while re-\nquiring less training time than competitive methods.\n",
        "question": {
            "statement": "What is a limitation of using visible cameras in certain applications?",
            "options": [
                "Cost",
                "Size",
                "Power consumption",
                "Light sensitivity"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Text-guided Unsupervised Latent Transformation for Multi-attribute\nImage Manipulation\nXiwen Wei1, Zhen Xu1, Cheng Liu2, Si Wu1,3∗, Zhiwen Yu1, and Hau San Wong4\n1School of Computer Science and Engineering, South China University of Technology\n2Department of Computer Science, Shantou University\n3Peng Cheng Laboratory\n4Department of Computer Science, City University of Hong Kong\n{202021044777, csxuzhen}@mail.scut.edu.cn, cliu@stu.edu.cn, {cswusi, zhwyu}@scut.edu.cn,\ncshswong@cityu.edu.hk\nAbstract\nGreat progress has been made in StyleGAN-based im-\nage editing. To associate with preset attributes, most ex-\nisting approaches focus on supervised learning for seman-\ntically meaningful latent space traversal directions, and\neach manipulation step is typically determined for an in-\ndividual attribute. To address this limitation, we propose a\nText-guided Unsupervised StyleGAN Latent Transformation\n(TUSLT) model, which adaptively infers a single transfor-\nmation step in the latent space of StyleGAN to simultane-\nously manipulate multiple attributes on a given input image.\nSpeciﬁcally, we adopt a two-stage architecture for a latent\nmapping network to break down the transformation process\ninto two manageable steps. Our network ﬁrst learns a di-\nverse set of semantic directions tailored to an input image,\nand later nonlinearly fuses the ones associated with the tar-\nget attributes to infer a residual vector. The resulting tightly\ninterlinked two-stage architecture delivers the ﬂexibility to\nhandle diverse attribute combinations. By leveraging the\ncross-modal text-image representation of CLIP, we can per-\nform pseudo annotations based on the semantic similarity\nbetween preset attribute text descriptions and training im-\nages, and further jointly train an auxiliary attribute clas-\nsiﬁer with the latent mapping network to provide semantic\nguidance. We perform extensive experiments to demonstrate\nthat the adopted strategies contribute to the superior perfor-\nmance of TUSLT.\n",
        "question": {
            "statement": "What is the primary goal of the proposed Text-guided Unsupervised StyleGAN Latent Transformation (TUSLT) model?",
            "options": [
                "To focus on supervised learning for semantically meaningful latent space traversal directions",
                "To determine each manipulation step individually for an individual attribute",
                "To eliminate the need for cross-modal text-image representations",
                "To simultaneously manipulate multiple attributes on a given input image"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP\nRunnan Chen1,2\nYouquan Liu2,3\nLingdong Kong2,4\nXinge Zhu5\nYuexin Ma6\nYikang Li2\nYuenan Hou2,†\nYu Qiao2\nWenping Wang7,†\n1The University of Hong Kong\n2Shanghai AI Laboratory\n3Hochschule Bremerhaven\n4National University of Singapore\n5The Chinese University of Hong Kong\n6ShanghaiTech University\n7Texas A&M University\nAbstract\nContrastive\nLanguage-Image\nPre-training\n(CLIP)\nachieves promising results in 2D zero-shot and few-shot\nlearning. Despite the impressive performance in 2D, apply-\ning CLIP to help the learning in 3D scene understanding\nhas yet to be explored. In this paper, we make the first\nattempt to investigate how CLIP knowledge benefits 3D\nscene understanding. We propose CLIP2Scene, a simple\nyet effective framework that transfers CLIP knowledge\nfrom 2D image-text pre-trained models to a 3D point cloud\nnetwork. We show that the pre-trained 3D network yields\nimpressive performance on various downstream tasks,\ni.e., annotation-free and fine-tuning with labelled data for\nsemantic segmentation. Specifically, built upon CLIP, we\ndesign a Semantic-driven Cross-modal Contrastive Learning\nframework that pre-trains a 3D network via semantic and\nspatial-temporal consistency regularization. For the former,\nwe first leverage CLIP’s text semantics to select the positive\nand negative point samples and then employ the contrastive\nloss to train the 3D network. In terms of the latter, we\nforce the consistency between the temporally coherent point\ncloud features and their corresponding image features. We\nconduct experiments on SemanticKITTI, nuScenes, and\nScanNet. For the first time, our pre-trained network achieves\nannotation-free 3D semantic segmentation with 20.8% and\n25.08% mIoU on nuScenes and ScanNet, respectively. When\nfine-tuned with 1% or 100% labelled data, our method\nsignificantly outperforms other self-supervised methods,\nwith improvements of 8% and 1% mIoU, respectively.\nFurthermore, we demonstrate the generalizability for\nhandling cross-domain datasets. Code is publicly available1.\n",
        "question": {
            "statement": "What is the primary goal of the proposed CLIP2Scene framework?",
            "options": [
                "To enable real-time object detection in 3D scenes",
                "To transfer knowledge from 2D image-text pre-trained models to 3D point cloud networks",
                "To develop a new language model for 3D scene understanding",
                "To improve the efficiency of existing 3D point cloud networks"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "2",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Spring: A High-Resolution High-Detail Dataset and Benchmark\nfor Scene Flow, Optical Flow and Stereo\nLukas Mehl\nJenny Schmalfuss\nAzin Jahedi\nYaroslava Nalivayko\nAndr´\nes Bruhn\nInstitute for Visualization and Interactive Systems, University of Stuttgart\nfirstname.lastname@vis.uni-stuttgart.de\nAbstract\nWhile recent methods for motion and stereo estimation\nrecover an unprecedented amount of details, such highly\ndetailed structures are neither adequately reflected in the\ndata of existing benchmarks nor their evaluation methodol-\nogy. Hence, we introduce Spring – a large, high-resolution,\nhigh-detail, computer-generated benchmark for scene flow,\noptical flow, and stereo. Based on rendered scenes from\nthe open-source Blender movie “Spring”, it provides photo-\nrealistic HD datasets with state-of-the-art visual effects and\nground truth training data. Furthermore, we provide a web-\nsite to upload, analyze and compare results. Using a novel\nevaluation methodology based on a super-resolved UHD\nground truth, our Spring benchmark can assess the quality\nof fine structures and provides further detailed performance\nstatistics on different image regions. Regarding the num-\nber of ground truth frames, Spring is 60× larger than the\nonly scene flow benchmark, KITTI 2015, and 15× larger\nthan the well-established MPI Sintel optical flow bench-\nmark. Initial results for recent methods on our benchmark\nshow that estimating fine details is indeed challenging, as\ntheir accuracy leaves significant room for improvement.\nThe Spring benchmark and the corresponding datasets are\navailable at http://spring-benchmark.org.\n",
        "question": {
            "statement": "What is a major limitation of existing benchmarks for motion and stereo estimation?",
            "options": [
                "They require specialized hardware",
                "They are too computationally expensive",
                "They are limited to indoor scenes",
                "They lack highly detailed structures"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Diffusion Video Autoencoders: Toward Temporally Consistent\nFace Video Editing via Disentangled Video Encoding\nGyeongman Kim1\nHajin Shim1\nHyunsu Kim2\nYunjey Choi2\nJunho Kim2\nEunho Yang1,3\n1Korea Advanced Institute of Science and Technology (KAIST), South Korea\n2NAVER AI Lab\n3AITRICS, South Korea\n{gmkim, shimazing, eunhoy}@kaist.ac.kr\n{hyunsu1125.kim, yunjey.choi, jhkim.ai}@navercorp.com\nOriginal\nBaseline\nOurs\nFigure 1. Face video editing. Our editing method shows improvement compared to the baseline [35] in terms of temporal consis-\ntency (left, “eyeglasses”) and robustness to the unusual case such as the hand-occluded face (right, “beard”).\nAbstract\nInspired by the impressive performance of recent face\nimage editing methods, several studies have been naturally\nproposed to extend these methods to the face video editing\ntask. One of the main challenges here is temporal consis-\ntency among edited frames, which is still unresolved. To this\nend, we propose a novel face video editing framework based\non diffusion autoencoders that can successfully extract the\ndecomposed features - for the ﬁrst time as a face video edit-\ning model - of identity and motion from a given video. This\nmodeling allows us to edit the video by simply manipulat-\ning the temporally invariant feature to the desired direction\nfor the consistency. Another unique strength of our model\nis that, since our model is based on diffusion models, it can\nsatisfy both reconstruction and edit capabilities at the same\ntime, and is robust to corner cases in wild face videos (e.g.\noccluded faces) unlike the existing GAN-based methods.1\n",
        "question": {
            "statement": "What is a major challenge in face video editing tasks?",
            "options": [
                "temporal consistency among edited frames",
                "rendering realistic facial expressions",
                "distinguishing between different identities in a video",
                "extracting facial features from individual frames"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "CUDA: Convolution-based Unlearnable Datasets\nVinu Sankar Sadasivan\nUniversity of Maryland\nvinu@umd.edu\nMahdi Soltanolkotabi\nUniversity of Southern California\nsoltanol@usc.edu\nSoheil Feizi\nUniversity of Maryland\nsfeizi@cs.umd.edu\nAbstract\nLarge-scale training of modern deep learning models\nheavily relies on publicly available data on the web. This\npotentially unauthorized usage of online data leads to con-\ncerns regarding data privacy. Recent works aim to make\nunlearnable data for deep learning models by adding small,\nspecially designed noises to tackle this issue.\nHowever,\nthese methods are vulnerable to adversarial training (AT)\nand/or are computationally heavy. In this work, we pro-\npose a novel, model-free, Convolution-based Unlearnable\nDAtaset (CUDA) generation technique. CUDA is generated\nusing controlled class-wise convolutions with filters that are\nrandomly generated via a private key. CUDA encourages\nthe network to learn the relation between filters and labels\nrather than informative features for classifying the clean\ndata.\nWe develop some theoretical analysis demonstrat-\ning that CUDA can successfully poison Gaussian mixture\ndata by reducing the clean data performance of the optimal\nBayes classifier. We also empirically demonstrate the effec-\ntiveness of CUDA with various datasets (CIFAR-10, CIFAR-\n100, ImageNet-100, and Tiny-ImageNet), and architectures\n(ResNet-18, VGG-16, Wide ResNet-34-10, DenseNet-121,\nDeIT, EfficientNetV2-S, and MobileNetV2).\nOur experi-\nments show that CUDA is robust to various data augmenta-\ntions and training approaches such as smoothing, AT with\ndifferent budgets, transfer learning, and fine-tuning. For\ninstance, training a ResNet-18 on ImageNet-100 CUDA\nachieves only 8.96%, 40.08%, and 20.58% clean test accu-\nracies with empirical risk minimization (ERM), L∞AT, and\nL2 AT, respectively. Here, ERM on the clean training data\nachieves a clean test accuracy of 80.66%. CUDA exhibits\nunlearnability effect with ERM even when only a fraction\nof the training dataset is perturbed. Furthermore, we also\nshow that CUDA is robust to adaptive defenses designed\nspecifically to break it.\n",
        "question": {
            "statement": "What is the main concern addressed by recent research in generating unlearnable datasets for deep learning models?",
            "options": [
                "Computational efficiency of deep learning models",
                "Data privacy due to unauthorized usage of online data",
                "Robustness of deep learning models to noisy data",
                "Interpretability of deep learning models"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "EXCALIBUR\nEncouraging and Evaluating Embodied Exploration\nHao Zhuk, Raghav Kapoork, So Yeon Mink,\nWinson Hann, Jiatai Lik, Kaiwen Gengk,\nGraham Neubigk, Yonatan Biskk, Aniruddha Kembhavin, Luca Weihsn\nkCarnegie Mellon University, nAllen Institute for Artiﬁcial Intelligence\nzhuhao@cmu.edu\nPhase I: Exploration\nPhase III: Reentering\nPhase II\nPhase IV\nQuestions Available Since Phase II\nQ1: How many eggs are there in the glass bowl\nin the fridge?\nQ2: What is the sculpture on the bedroom table \nmade of?\nQ3: What is the color of the scrub in the larger \nbathroom?\nQ4: Is there a pot lighter than the laptop in the \nliving room?\nA1: 3 \nA1: 1 \nA2: Plastic \nA2: Metal \nA3: Blue \nA3: Green and \nyellow \nA4: Yes \nA4: Yes \n1\n1\n2\n2\n3\n4\n4\n3\n1\n2\n3\n4\n1\n2\n3\n4\nFigure 1. Episode in EXCALIBUR played by a human annotator. An episode is divided into four sequential phases: in Phase I, the agent\nexplores the house for 2,500 steps (each action takes a step); in Phase II the agent needs to answer 20 questions (5 shown) about the\nexplored environment; in Phase III the agent is given a second chance to reenter the house, now with knowledge of the questions; in Phase\nIV the agent answers the questions again. Performance is evaluated with the answer accuracy in Phases II&IV and the time spent in Phase\nIII. The observation space is egocentric (see left and right panels). The action space includes navigation and manipulation actions (Fig. 2).\nAbstract\nExperience precedes understanding. Humans constantly\nexplore and learn about their environment out of curiosity,\ngather information, and update their models of the world.\nOn the other hand, machines are either trained to learn pas-\nsively from static and ﬁxed datasets, or taught to complete\nspeciﬁc goal-conditioned tasks. To encourage the develop-\nment of exploratory interactive agents, we present the EX-\nCALIBUR benchmark. EXCALIBUR allows agents to ex-\nplore their environment for long durations and then query\ntheir understanding of the physical world via inquiries like:\n“is the small heavy red bowl made from glass?” or “is\nthere a silver spoon heavier than the egg?”. This design\nencourages agents to perform free-form home exploration\nwithout myopia induced by goal conditioning. Once the\nagents have answered a series of questions, they can renter\nthe scene to reﬁne their knowledge, update their beliefs,\nand improve their performance on the questions. Our ex-\nperiments demonstrate the challenges posed by this dataset\nfor the present-day state-of-the-art embodied systems and\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\nExcept for this watermark, it is identical to the accepted version;\nthe final published version of the proceedings is available on IEEE Xplore.\n14931\nthe headroom afforded to develop new innovative methods.\nFinally, we present a virtual reality interface that enables\nhumans to seamlessly interact within the simulated world\nand use it to gather human performance measures. EXCAL-\nIBUR affords unique challenges in comparison to present-\nday benchmarks and represents the next frontier for embod-\nied AI research.\n",
        "question": {
            "statement": "What is the primary purpose of the EXCALIBUR benchmark?",
            "options": [
                "To evaluate the performance of embodied systems on specific goal-conditioned tasks",
                "To gather human performance measures in virtual reality environments",
                "To train machines to learn passively from static and fixed datasets",
                "To encourage the development of exploratory interactive agents"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "RONO: Robust Discriminative Learning with Noisy Labels\nfor 2D-3D Cross-Modal Retrieval\nYanglin Feng1\nHongyuan Zhu2\nDezhong Peng1,3,4\nXi Peng1\nPeng Hu1*\n1College of Computer Science, Sichuan University\n2Institute for Infocomm Research (I2R), A*STAR, Singapore\n3Sichuan Zhiqian Technology Co., Ltd\n4Chengdu Ruibei Yingte Information Technology Co., Ltd\nAbstract\nRecently, with the advent of Metaverse and AI Gener-\nated Content, cross-modal retrieval becomes popular with\na burst of 2D and 3D data.\nHowever, this problem is\nchallenging given the heterogeneous structure and semantic\ndiscrepancies. Moreover, imperfect annotations are ubiq-\nuitous given the ambiguous 2D and 3D content, thus in-\nevitably producing noisy labels to degrade the learning per-\nformance. To tackle the problem, this paper proposes a ro-\nbust 2D-3D retrieval framework (RONO) to robustly learn\nfrom noisy multimodal data. Specifically, one novel Robust\nDiscriminative Center Learning mechanism (RDCL) is pro-\nposed in RONO to adaptively distinguish clean and noisy\nsamples for respectively providing them with positive and\nnegative optimization directions, thus mitigating the nega-\ntive impact of noisy labels. Besides, we present a Shared\nSpace Consistency Learning mechanism (SSCL) to capture\nthe intrinsic information inside the noisy data by minimizing\nthe cross-modal and semantic discrepancy between com-\nmon space and label space simultaneously. Comprehen-\nsive mathematical analyses are given to theoretically prove\nthe noise tolerance of the proposed method. Furthermore,\nwe conduct extensive experiments on four 3D-model mul-\ntimodal datasets to verify the effectiveness of our method\nby comparing it with 15 state-of-the-art methods. Code is\navailable at https://github.com/penghu-cs/RONO.\n",
        "question": {
            "statement": "What is a common challenge in 2D-3D cross-modal retrieval due to the nature of the data?",
            "options": [
                "Limited availability of 3D models",
                "Insufficient training data",
                "Noisy labels",
                "High computational cost"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Seeing With Sound:\nLong-range Acoustic Beamforming for Multimodal Scene Understanding\nPraneeth Chakravarthula1\nJim Aldon D’Souza2\nEthan Tseng1\nJoe Bartusek1\nFelix Heide1,2\n1Princeton University\n2Algolux\nAbstract\nMobile robots, including autonomous vehicles rely heav-\nily on sensors that use electromagnetic radiation like lidars,\nradars and cameras for perception. While effective in most\nscenarios, these sensors can be unreliable in unfavorable\nenvironmental conditions, including low-light scenarios and\nadverse weather, and they can only detect obstacles within\ntheir direct line-of-sight. Audible sound from other road\nusers propagates as acoustic waves that carry information\neven in challenging scenarios. However, their low spatial\nresolution and lack of directional information have made\nthem an overlooked sensing modality. In this work, we intro-\nduce long-range acoustic beamforming of sound produced\nby road users in-the-wild as a complementary sensing modal-\nity to traditional electromagnetic radiation-based sensors.\nTo validate our approach and encourage further work in the\nfield, we also introduce the first-ever multimodal long-range\nacoustic beamforming dataset. We propose a neural aper-\nture expansion method for beamforming and demonstrate\nits effectiveness for multimodal automotive object detection\nwhen coupled with RGB images in challenging automotive\nscenarios, where camera-only approaches fail or are unable\nto provide ultra-fast acoustic sensing sampling rates. Data\nand code can be found here1.\n",
        "question": {
            "statement": "What limitation of traditional sensors used in mobile robots and autonomous vehicles can be addressed by using audible sound waves?",
            "options": [
                "Their high cost and complexity",
                "Their limited range and accuracy in ideal environmental conditions",
                "Their reliance on machine learning algorithms",
                "Their inability to detect obstacles outside their direct line-of-sight"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Natural Language-Assisted Sign Language Recognition\nRonglai Zuo1\nFangyun Wei2†\nBrian Mak1\n1The Hong Kong University of Science and Technology\n2Microsoft Research Asia\n{rzuo,mak}@cse.ust.hk\nfawe@microsoft.com\nAbstract\nSign languages are visual languages which convey in-\nformation by signers’ handshape, facial expression, body\nmovement, and so forth.\nDue to the inherent restric-\ntion of combinations of these visual ingredients, there ex-\nist a significant number of visually indistinguishable signs\n(VISigns) in sign languages, which limits the recognition\ncapacity of vision neural networks. To mitigate the problem,\nwe propose the Natural Language-Assisted Sign Language\nRecognition (NLA-SLR) framework, which exploits seman-\ntic information contained in glosses (sign labels). First,\nfor VISigns with similar semantic meanings, we propose\nlanguage-aware label smoothing by generating soft labels\nfor each training sign whose smoothing weights are com-\nputed from the normalized semantic similarities among the\nglosses to ease training. Second, for VISigns with distinct\nsemantic meanings, we present an inter-modality mixup\ntechnique which blends vision and gloss features to further\nmaximize the separability of different signs under the super-\nvision of blended labels. Besides, we also introduce a novel\nbackbone, video-keypoint network, which not only models\nboth RGB videos and human body keypoints but also de-\nrives knowledge from sign videos of different temporal re-\nceptive fields. Empirically, our method achieves state-of-\nthe-art performance on three widely-adopted benchmarks:\nMSASL, WLASL, and NMFs-CSL. Codes are available at\nhttps://github.com/FangyunWei/SLRT.\n",
        "question": {
            "statement": "What is a major challenge in recognizing sign languages using computer vision approaches?",
            "options": [
                "difficulty in capturing facial expressions",
                "visually indistinguishable signs",
                "limited availability of sign language datasets",
                "insufficient computing power for processing sign language videos"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition\nXinyu Gong2*†, Sreyas Mohan1∗, Naina Dhingra1, Jean-Charles Bazin1, Yilei Li1,\nZhangyang Wang2, Rakesh Ranjan1\n1Meta Reality Labs, 2The University of Texas at Austin\nAbstract\nIn this paper, we study a novel problem in egocentric\naction recognition, which we term as “Multimodal Gener-\nalization” (MMG). MMG aims to study how systems can\ngeneralize when data from certain modalities is limited or\neven completely missing. We thoroughly investigate MMG\nin the context of standard supervised action recognition and\nthe more challenging few-shot setting for learning new ac-\ntion categories. MMG consists of two novel scenarios, de-\nsigned to support security, and efficiency considerations in\nreal-world applications: (1) missing modality generaliza-\ntion where some modalities that were present during the\ntrain time are missing during the inference time, and (2)\ncross-modal zero-shot generalization, where the modali-\nties present during the inference time and the training time\nare disjoint.\nTo enable this investigation, we construct\na new dataset MMG-Ego4D containing data points with\nvideo, audio, and inertial motion sensor (IMU) modali-\nties. Our dataset is derived from Ego4D [27] dataset, but\nprocessed and thoroughly re-annotated by human experts\nto facilitate research in the MMG problem. We evaluate\na diverse array of models on MMG-Ego4D and propose\nnew methods with improved generalization ability. In par-\nticular, we introduce a new fusion module with modality\ndropout training, contrastive-based alignment training, and\na novel cross-modal prototypical loss for better few-shot\nperformance. We hope this study will serve as a bench-\nmark and guide future research in multimodal generaliza-\ntion problems. The benchmark and code are available at\nhttps://github.com/facebookresearch/MMG Ego4D\n",
        "question": {
            "statement": "What is the main goal of Multimodal Generalization (MMG) in egocentric action recognition?",
            "options": [
                "To improve the accuracy of action recognition models in ideal conditions",
                "To study how systems can generalize when data from certain modalities is limited or even completely missing",
                "To reduce the amount of data required for training action recognition models",
                "To develop new sensors for collecting data in egocentric action recognition"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "Visual Dependency Transformers:\nDependency Tree Emerges from Reversed Attention\nMingyu Ding13*\nYikang Shen2\nLijie Fan3\nZhenfang Chen2\nZitian Chen4\nPing Luo1\nJosh Tenenbaum3\nChuang Gan24\n1The University of Hong Kong\n2MIT-IBM Watson AI Lab\n3MIT\n4UMass Amherst\nAbstract\nHumans possess a versatile mechanism for extracting\nstructured representations of our visual world. When look-\ning at an image, we can decompose the scene into entities\nand their parts as well as obtain the dependencies between\nthem. To mimic such capability, we propose Visual Depen-\ndency Transformers (DependencyViT) 1 that can induce vi-\nsual dependencies without any labels. We achieve that with\na novel neural operator called reversed attention that can\nnaturally capture long-range visual dependencies between\nimage patches. Specifically, we formulate it as a depen-\ndency graph where a child token in reversed attention is\ntrained to attend to its parent tokens and send information\nfollowing a normalized probability distribution rather than\ngathering information in conventional self-attention. With\nsuch a design, hierarchies naturally emerge from reversed\nattention layers, and a dependency tree is progressively in-\nduced from leaf nodes to the root node unsupervisedly.\nDependencyViT offers several appealing benefits. (i) En-\ntities and their parts in an image are represented by dif-\nferent subtrees, enabling part partitioning from dependen-\ncies; (ii) Dynamic visual pooling is made possible. The\nleaf nodes which rarely send messages can be pruned with-\nout hindering the model performance, based on which we\npropose the lightweight DependencyViT-Lite to reduce the\ncomputational and memory footprints; (iii) DependencyViT\nworks well on both self- and weakly-supervised pretraining\nparadigms on ImageNet, and demonstrates its effectiveness\non 8 datasets and 5 tasks, such as unsupervised part and\nsaliency segmentation, recognition, and detection.\n",
        "question": {
            "statement": "What is the primary benefit of using DependencyViT's reversed attention mechanism?",
            "options": [
                "It enables the model to capture long-range visual dependencies between image patches.",
                "It allows the model to focus solely on local features.",
                "It relies heavily on labeled data for training.",
                "It reduces the computational footprint of the model without affecting performance."
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "3"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Improving Image Recognition by Retrieving from Web-Scale Image-Text Data\nAhmet Iscen\nAlireza Fathi\nCordelia Schmid\nGoogle Research\nAbstract\nRetrieval augmented models are becoming increasingly\npopular for computer vision tasks after their recent success\nin NLP problems. The goal is to enhance the recognition ca-\npabilities of the model by retrieving similar examples for the\nvisual input from an external memory set. In this work, we\nintroduce an attention-based memory module, which learns\nthe importance of each retrieved example from the mem-\nory. Compared to existing approaches, our method removes\nthe influence of the irrelevant retrieved examples, and re-\ntains those that are beneficial to the input query. We also\nthoroughly study various ways of constructing the memory\ndataset. Our experiments show the benefit of using a massive-\nscale memory dataset of 1B image-text pairs, and demon-\nstrate the performance of different memory representations.\nWe evaluate our method in three different classification tasks,\nnamely long-tailed recognition, learning with noisy labels,\nand fine-grained classification, and show that it achieves\nstate-of-the-art accuracies in ImageNet-LT, Places-LT and\nWebvision datasets.\n",
        "question": {
            "statement": "What is the primary purpose of retrieval augmented models in computer vision?",
            "options": [
                "To reduce the size of the training dataset",
                "To eliminate the need for labeled data",
                "To increase the computational complexity of the model",
                "To enhance the recognition capabilities of the model by retrieving relevant examples"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Achieving a Better Stability-Plasticity Trade-off via Auxiliary Networks in\nContinual Learning\nSanghwan Kim\nLorenzo Noci\nAntonio Orvieto\nThomas Hofmann\nETH Z¨\nurich\nZ¨\nurich, Switzerland\n{sanghwan.kim, lorenzo.noci, antonio.orvieto, thomas.hofmann}@inf.ethz.ch\nAbstract\nIn contrast to the natural capabilities of humans to\nlearn new tasks in a sequential fashion, neural networks\nare known to suffer from catastrophic forgetting, where the\nmodel’s performances on old tasks drop dramatically after\nbeing optimized for a new task. Since then, the continual\nlearning (CL) community has proposed several solutions\naiming to equip the neural network with the ability to learn\nthe current task (plasticity) while still achieving high accu-\nracy on the previous tasks (stability). Despite remarkable\nimprovements, the plasticity-stability trade-off is still far\nfrom being solved and its underlying mechanism is poorly\nunderstood. In this work, we propose Auxiliary Network\nContinual Learning (ANCL), a novel method that applies\nan additional auxiliary network which promotes plasticity\nto the continually learned model which mainly focuses on\nstability. More concretely, the proposed framework mate-\nrializes in a regularizer that naturally interpolates between\nplasticity and stability, surpassing strong baselines on task\nincremental and class incremental scenarios.\nThrough\nextensive analyses on ANCL solutions, we identify some\nessential principles beneath the stability-plasticity trade-\noff. The code implementation of our work is available at\nhttps://github.com/kim-sanghwan/ANCL.\n",
        "question": {
            "statement": "What is a major limitation of neural networks when learning new tasks in a sequential fashion?",
            "options": [
                "overfitting",
                "information bottleneck",
                "bias-variance trade-off",
                "catastrophic forgetting"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Activating More Pixels in Image Super-Resolution Transformer\nXiangyu Chen1,2,3 Xintao Wang4 Jiantao Zhou1 Yu Qiao2,3 Chao Dong2,3†\n1State Key Laboratory of Internet of Things for Smart City, University of Macau\n2Shenzhen Key Lab of Computer Vision and Pattern Recognition,\nShenzhen Institute of Advanced Technology, Chinese Academy of Sciences\n3Shanghai Artificial Intelligence Laboratory 4ARC Lab, Tencent PCG\n{chxy95, xintao.alpha}@gmail.com jtzhou@um.edu.mo {yu.qiao, chao.dong}@siat.ac.cn\nAbstract\nTransformer-based methods have shown impressive per-\nformance in low-level vision tasks, such as image super-\nresolution. However, we find that these networks can only\nutilize a limited spatial range of input information through\nattribution analysis.\nThis implies that the potential of\nTransformer is still not fully exploited in existing networks.\nIn order to activate more input pixels for better recon-\nstruction, we propose a novel Hybrid Attention Transformer\n(HAT). It combines both channel attention and window-\nbased self-attention schemes, thus making use of their com-\nplementary advantages of being able to utilize global statis-\ntics and strong local fitting capability. Moreover, to better\naggregate the cross-window information, we introduce an\noverlapping cross-attention module to enhance the interac-\ntion between neighboring window features. In the train-\ning stage, we additionally adopt a same-task pre-training\nstrategy to exploit the potential of the model for further im-\nprovement. Extensive experiments show the effectiveness of\nthe proposed modules, and we further scale up the model to\ndemonstrate that the performance of this task can be greatly\nimproved. Our overall method significantly outperforms the\nstate-of-the-art methods by more than 1dB.\n",
        "question": {
            "statement": "What is a limitation of transformer-based methods in image super-resolution tasks?",
            "options": [
                "They require large amounts of training data",
                "They are only effective for high-resolution images",
                "They are computationally expensive",
                "They can only utilize a limited spatial range of input information"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "InstantAvatar: Learning Avatars from Monocular Video in 60 Seconds\nTianjian Jiang1*, Xu Chen1,2*, Jie Song†1, Otmar Hilliges1\n1 ETH Z¨\nurich\n2 Max Planck Institute for Intelligent Systems, T¨\nubingen\n1 min \nTraining\n15 FPS\nRendering\nFigure 1. InstantAvatar: we propose a system that can reconstruct animatable high-fidelity human avatars from a monocular video within\n60 seconds, providing poses and masks, and can animate and render the model at 15 FPS at 540 × 540 resolution. To achieve this we\nintegrate accelerated neural radiance fields, originally designed for rigid scenes, with a fast correspondence search module for articulation.\nAn efficient empty-space skipping strategy further speeds up training and inference, enabling near-instant avatar learning.\nAbstract\nIn this paper, we take one step further towards real-world\napplicability of monocular neural avatar reconstruction by\ncontributing InstantAvatar, a system that can reconstruct\nhuman avatars from a monocular video within seconds, and\nthese avatars can be animated and rendered at an inter-\nactive rate. To achieve this efficiency we propose a care-\nfully designed and engineered system, that leverages emerg-\ning acceleration structures for neural fields, in combination\nwith an efficient empty-space skipping strategy for dynamic\nscenes. We also contribute an efficient implementation that\nwe will make available for research purposes. Compared\nto existing methods, InstantAvatar converges 130× faster\nand can be trained in minutes instead of hours. It achieves\ncomparable or even better reconstruction quality and novel\npose synthesis results. When given the same time budget,\nour method significantly outperforms SoTA methods. In-\nstantAvatar can yield acceptable visual quality in as little\nas 10 seconds training time. For code and more demo re-\nsults, please refer to https://ait.ethz.ch/InstantAvatar.\n*Equal contribution.\n†Corresponding author\n",
        "question": {
            "statement": "What is the primary advantage of the InstantAvatar system compared to existing methods?",
            "options": [
                "It converges 130 times faster",
                "It requires less computational power",
                "It produces higher-quality avatars",
                "It only works with stereo videos"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "3",
                "0"
            ]
        },
        "difference": 7,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Adaptive Human Matting for Dynamic Videos\nChung-Ching Lin, Jiang Wang, Kun Luo, Kevin Lin, Linjie Li, Lijuan Wang, Zicheng Liu\nMicrosoft\n{chungching.lin,jiangwang,kun.luo,keli,linjli,lijuanw,zliu}@microsoft.com\nFigure 1. Our method predicts reliable alpha mattes on dynamic videos without requiring trimaps or pre-captured backgrounds.\nAbstract\nThe most recent efforts in video matting have focused\non eliminating trimap dependency since trimap annotations\nare expensive and trimap-based methods are less adapt-\nable for real-time applications. Despite the latest tripmap-\nfree methods showing promising results, their performance\noften degrades when dealing with highly diverse and un-\nstructured videos. We address this limitation by introduc-\ning Adaptive Matting for Dynamic Videos, termed AdaM,\nwhich is a framework designed for simultaneously differen-\ntiating foregrounds from backgrounds and capturing alpha\nmatte details of human subjects in the foreground. Two in-\nterconnected network designs are employed to achieve this\ngoal: (1) an encoder-decoder network that produces alpha\nmattes and intermediate masks which are used to guide the\ntransformer in adaptively decoding foregrounds and back-\ngrounds, and (2) a transformer network in which long- and\nshort-term attention combine to retain spatial and tempo-\nral contexts, facilitating the decoding of foreground de-\ntails. We benchmark and study our methods on recently\nintroduced datasets, showing that our model notably im-\nproves matting realism and temporal coherence in complex\nreal-world videos and achieves new best-in-class general-\nizability.\nFurther details and examples are available at\nhttps://github.com/microsoft/AdaM.\n",
        "question": {
            "statement": "What is a major limitation of recent video matting methods?",
            "options": [
                "They often degrade in performance when dealing with highly diverse and unstructured videos",
                "They are limited to static images",
                "They need manual annotation of every frame",
                "They require a lot of computational power"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "MetaMix: Towards Corruption-Robust Continual Learning with\nTemporally Self-Adaptive Data Transformation\nZhenyi Wang1\nLi Shen 2,*\nDonglin Zhan 3\nQiuling Suo 1\nYanjun Zhu 1\nTiehang Duan 4\nMingchen Gao 1\n1State University of New York at Buffalo, USA; 2JD Explore Academy, China\n3 Columbia University, USA; 4 Meta, USA\n{zhenyiwa, qiulings, yjzhu, mgao8}@buffalo.edu; dz2478@columbia.edu\n{mathshenli, tiehang.duan}@gmail.com\nAbstract\nContinual Learning (CL) has achieved rapid progress in\nrecent years. However, it is still largely unknown how to\ndetermine whether a CL model is trustworthy and how to\nfoster its trustworthiness. This work focuses on evaluating\nand improving the robustness to corruptions of existing CL\nmodels. Our empirical evaluation results show that existing\nstate-of-the-art (SOTA) CL models are particularly vulnera-\nble to various data corruptions during testing. To make them\ntrustworthy and robust to corruptions deployed in safety-\ncritical scenarios, we propose a meta-learning framework of\nself-adaptive data augmentation to tackle the corruption ro-\nbustness in CL. The proposed framework, MetaMix, learns to\naugment and mix data, automatically transforming the new\ntask data or memory data. It directly optimizes the general-\nization performance against data corruptions during train-\ning. To evaluate the corruption robustness of our proposed\napproach, we construct several CL corruption datasets with\ndifferent levels of severity. We perform comprehensive exper-\niments on both task- and class-continual learning. Extensive\nexperiments demonstrate the effectiveness of our proposed\nmethod compared to SOTA baselines.\n",
        "question": {
            "statement": "What is a major concern for Continual Learning models that researchers are trying to address?",
            "options": [
                "Their vulnerability to data corruptions",
                "Their high computational requirements",
                "Their lack of interpretability",
                "Their ability to learn from large amounts of data"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Ham2Pose: Animating Sign Language Notation into Pose Sequences\nRotem Shalev Arkushin\nReichman University\nrotemroo@gmail.com\nAmit Moryossef\nBar-Ilan University\namitmoryossef@gmail.com\nOhad Fried\nReichman University\nofried@runi.ac.il\nhttps://rotem-shalev.github.io/ham-to-pose\nAbstract\nTranslating spoken languages into Sign languages is\nnecessary for open communication between the hearing and\nhearing-impaired communities. To achieve this goal, we\npropose the first method for animating a text written in\nHamNoSys, a lexical Sign language notation, into signed\npose sequences. As HamNoSys is universal by design, our\nproposed method offers a generic solution invariant to the\ntarget Sign language. Our method gradually generates pose\npredictions using transformer encoders that create mean-\ningful representations of the text and poses while consider-\ning their spatial and temporal information. We use weak su-\npervision for the training process and show that our method\nsucceeds in learning from partial and inaccurate data. Ad-\nditionally, we offer a new distance measurement that con-\nsiders missing keypoints, to measure the distance between\npose sequences using DTW-MJE. We validate its correct-\nness using AUTSL, a large-scale Sign language dataset,\nshow that it measures the distance between pose sequences\nmore accurately than existing measurements, and use it to\nassess the quality of our generated pose sequences. Code\nfor the data pre-processing, the model, and the distance\nmeasurement is publicly released for future research.\n",
        "question": {
            "statement": "What is the primary goal of translating spoken languages into sign languages?",
            "options": [
                "To create a standardized system for sign language notation",
                "To develop a new form of artistic expression",
                "To improve language learning for the hearing population",
                "To enable open communication between the hearing and hearing-impaired communities"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Progressive Open Space Expansion for Open-Set Model Attribution\nTianyun Yang1,2, Danding Wang1,2*\n, Fan Tang1,2, Xinying Zhao1,2, Juan Cao1,2, Sheng Tang1,3\n1 Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China\n2University of Chinese Academy of Sciences, Beijing, China\n3Research Institute of Intelligent Computing, Zhejiang Lab, Hangzhou, China\n{yangtianyun19z,wangdanding,tangfan,zhaoxinying21s,caojuan,ts}@ict.ac.cn\nAbstract\nDespite the remarkable progress in generative technol-\nogy, the Janus-faced issues of intellectual property protec-\ntion and malicious content supervision have arisen. Efforts\nhave been paid to manage synthetic images by attributing\nthem to a set of potential source models.\nHowever, the\nclosed-set classification setting limits the application in\nreal-world scenarios for handling contents generated by\narbitrary models. In this study, we focus on a challenging\ntask, namely Open-Set Model Attribution (OSMA), to simul-\ntaneously attribute images to known models and identify\nthose from unknown ones.\nCompared to existing open-\nset recognition (OSR) tasks focusing on semantic novelty,\nOSMA is more challenging as the distinction between\nimages from known and unknown models may only lie in\nvisually imperceptible traces. To this end, we propose a\nProgressive Open Space Expansion (POSE) solution, which\nsimulates open-set samples that maintain the same se-\nmantics as closed-set samples but embedded with different\nimperceptible traces. Guided by a diversity constraint, the\nopen space is simulated progressively by a set of lightweight\naugmentation models. We consider three real-world sce-\nnarios and construct an OSMA benchmark dataset, includ-\ning unknown models trained with different random seeds,\narchitectures, and datasets from known ones.\nExtensive\nexperiments on the dataset demonstrate POSE is superior\nto both existing model attribution methods and off-the-shelf\nOSR methods. Github: https://github.com/ICTMCG/POSE\n",
        "question": {
            "statement": "What is the main challenge addressed by Open-Set Model Attribution (OSMA) compared to traditional closed-set classification settings?",
            "options": [
                "handling contents generated by arbitrary models",
                "recognizing semantic novelty",
                "identifying images with specific objects",
                "attributing images to known models"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "2"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "UniHCP: A Unified Model for Human-Centric Perceptions\nYuanzheng Ci1*\n, Yizhou Wang2,3*\n, Meilin Chen2, Shixiang Tang1, Lei Bai3†\n, Feng Zhu4, Rui Zhao4,5,\nFengwei Yu4, Donglian Qi2, Wanli Ouyang3\n1The University of Sydney, 2Zhejiang University, 3Shanghai AI Laboratory, 4SenseTime Research,\n5Qing Yuan Research Institute, Shanghai Jiao Tong University, Shanghai, China\nyuanzheng.ci@sydney.edu.au, yizhouwang@zju.edu.cn, bailei@pjlab.org.cn\nAbstract\nHuman-centric perceptions (e.g., pose estimation, hu-\nman parsing, pedestrian detection, person re-identification,\netc.) play a key role in industrial applications of visual mod-\nels. While specific human-centric tasks have their own rel-\nevant semantic aspect to focus on, they also share the same\nunderlying semantic structure of the human body. However,\nfew works have attempted to exploit such homogeneity and\ndesign a general-propose model for human-centric tasks.\nIn this work, we revisit a broad range of human-centric\ntasks and unify them in a minimalist manner. We propose\nUniHCP, a Unified Model for Human-Centric Perceptions,\nwhich unifies a wide range of human-centric tasks in a sim-\nplified end-to-end manner with the plain vision transformer\narchitecture. With large-scale joint training on 33 human-\ncentric datasets, UniHCP can outperform strong baselines\non several in-domain and downstream tasks by direct eval-\nuation. When adapted to a specific task, UniHCP achieves\nnew SOTAs on a wide range of human-centric tasks, e.g.,\n69.8 mIoU on CIHP for human parsing, 86.18 mA on PA-\n100K for attribute prediction, 90.3 mAP on Market1501 for\nReID, and 85.8 JI on CrowdHuman for pedestrian detec-\ntion, performing better than specialized models tailored for\neach task. The code and pretrained model are available at\nhttps://github.com/OpenGVLab/UniHCP.\n",
        "question": {
            "statement": "What is a common characteristic among various human-centric perception tasks, such as pose estimation, human parsing, and pedestrian detection?",
            "options": [
                "They require different types of sensors to operate",
                "They are typically performed in isolation from each other",
                "They do not involve computer vision techniques",
                "They share the same underlying semantic structure of the human body"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "0",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Learning Instance-Level Representation for Large-Scale Multi-Modal\nPretraining in E-commerce\nYang Jin1,2, Yongzhi Li2, Zehuan Yuan2, Yadong Mu1*\n1Peking University\n2ByteDance Inc.\njiny@stu.pku.edu.cn, liyongzhi.ailab@bytedance.com,\nyuanzehuan@bytedance.com, myd@pku.edu.cn\nAbstract\nThis paper aims to establish a generic multi-modal\nfoundation model that has the scalable capability to mas-\nsive downstream applications in E-commerce.\nRecently,\nlarge-scale vision-language pretraining approaches have\nachieved remarkable advances in the general domain.\nHowever, due to the significant differences between natu-\nral and product images, directly applying these frameworks\nfor modeling image-level representations to E-commerce\nwill be inevitably sub-optimal. To this end, we propose an\ninstance-centric multi-modal pretraining paradigm called\nECLIP in this work. In detail, we craft a decoder archi-\ntecture that introduces a set of learnable instance queries\nto explicitly aggregate instance-level semantics. Moreover,\nto enable the model to focus on the desired product in-\nstance without reliance on expensive manual annotations,\ntwo specially configured pretext tasks are further proposed.\nPretrained on the 100 million E-commerce-related data,\nECLIP successfully extracts more generic, semantic-rich,\nand robust representations. Extensive experimental results\nshow that, without further fine-tuning, ECLIP surpasses\nexisting methods by a large margin on a broad range of\ndownstream tasks, demonstrating the strong transferability\nto real-world E-commerce applications.\n",
        "question": {
            "statement": "What is a major limitation of directly applying large-scale vision-language pretraining approaches to E-commerce?",
            "options": [
                "The difference between natural and product images",
                "Inadequate model architecture",
                "Lack of annotated data",
                "Insufficient computational resources"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "2",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "TBP-Former: Learning Temporal Bird’s-Eye-View Pyramid for\nJoint Perception and Prediction in Vision-Centric Autonomous Driving\nShaoheng Fang1*\nZi Wang1*\nYiqi Zhong2\nJunhao Ge1\nSiheng Chen1,3†\n1Cooperative Medianet Innovation Center, Shanghai Jiao Tong University\n2Department of Computer Science, University of Southern California\n3Shanghai AI Laboratory\n1{shfang, w4ngz1, cancaries, sihengc}@sjtu.edu.cn\n2{yiqizhon}@usc.edu\nAbstract\nVision-centric joint perception and prediction (PnP) has\nbecome an emerging trend in autonomous driving research.\nIt predicts the future states of the traffic participants in\nthe surrounding environment from raw RGB images. How-\never, it is still a critical challenge to synchronize features\nobtained at multiple camera views and timestamps due to\ninevitable geometric distortions and further exploit those\nspatial-temporal features. To address this issue, we pro-\npose a temporal bird’s-eye-view pyramid transformer (TBP-\nFormer) for vision-centric PnP, which includes two novel\ndesigns. First, a pose-synchronized BEV encoder is pro-\nposed to map raw image inputs with any camera pose at\nany time to a shared and synchronized BEV space for bet-\nter spatial-temporal synchronization.\nSecond, a spatial-\ntemporal pyramid transformer is introduced to compre-\nhensively extract multi-scale BEV features and predict fu-\nture BEV states with the support of spatial priors.\nEx-\ntensive experiments on nuScenes dataset show that our\nproposed framework overall outperforms all state-of-the-\nart vision-based prediction methods.\nCode is available\nat: https://github.com/MediaBrain-SJTU/TBP-Former\n",
        "question": {
            "statement": "What is a major challenge in vision-centric joint perception and prediction for autonomous driving?",
            "options": [
                "Processing data from other sensors like lidar or radar",
                "Predicting future states of traffic participants",
                "Detecting objects in raw RGB images",
                "Synchronizing features obtained at multiple camera views and timestamps"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "3",
                "0",
                "10"
            ]
        },
        "difference": 7,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Learning to Zoom and Unzoom\nChittesh Thavamani1\nMengtian Li†1\nFrancesco Ferroni‡2\nDeva Ramanan1\n1Carnegie Mellon University\n2Argo AI\ntchittesh@gmail.org\nmengtial@alumni.cmu.edu\nfferroni@nvidia.com\ndeva@cs.cmu.edu\nAbstract\nMany perception systems in mobile computing, au-\ntonomous navigation, and AR/VR face strict compute con-\nstraints that are particularly challenging for high-resolution\ninput images. Previous works propose nonuniform downsam-\nplers that \"learn to zoom\" on salient image regions, reducing\ncompute while retaining task-relevant image information.\nHowever, for tasks with spatial labels (such as 2D/3D ob-\nject detection and semantic segmentation), such distortions\nmay harm performance. In this work (LZU), we \"learn to\nzoom\" in on the input image, compute spatial features, and\nthen \"unzoom\" to revert any deformations. To enable ef-\nﬁcient and differentiable unzooming, we approximate the\nzooming warp with a piecewise bilinear mapping that is\ninvertible. LZU can be applied to any task with 2D spa-\ntial input and any model with 2D spatial features, and we\ndemonstrate this versatility by evaluating on a variety of\ntasks and datasets: object detection on Argoverse-HD, se-\nmantic segmentation on Cityscapes, and monocular 3D ob-\nject detection on nuScenes. Interestingly, we observe boosts\nin performance even when high-resolution sensor data is\nunavailable, implying that LZU can be used to \"learn to up-\nsample\" as well. Code and additional visuals are available\nat https://tchittesh.github.io/lzu/.\n",
        "question": {
            "statement": "What is a key challenge faced by perception systems in mobile computing, autonomous navigation, and AR/VR?",
            "options": [
                "Compute constraints due to high-resolution input images",
                "Difficulty in collecting ground truth labels",
                "Limited availability of training data",
                "Insufficient memory for storing models"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval\nSiteng Huang1,3*\n, Biao Gong2, Yulin Pan2, Jianwen Jiang2, Yiliang Lv2, Yuyuan Li3, Donglin Wang1†\n1Machine Intelligence Lab (MiLAB), AI Division, School of Engineering, Westlake University\n2Alibaba Group 3Zhejiang University\n{huangsiteng, wangdonglin}@westlake.edu.cn, y2li@zju.edu.cn,\na.biao.gong@gmail.com, {yanwen.pyl, jianwen.jjw, yiliang.lyl}@alibaba-inc.com\nAbstract\nMany recent studies leverage the pre-trained CLIP for\ntext-video cross-modal retrieval by tuning the backbone\nwith additional heavy modules, which not only brings\nhuge computational burdens with much more parameters,\nbut also leads to the knowledge forgetting from upstream\nmodels. In this work, we propose the VoP: Text-Video Co-\noperative Prompt Tuning for efficient tuning on the text-\nvideo retrieval task. The proposed VoP is an end-to-end\nframework with both video & text prompts introducing,\nwhich can be regarded as a powerful baseline with only\n0.1% trainable parameters. Further, based on the spatio-\ntemporal characteristics of videos, we develop three novel\nvideo prompt mechanisms to improve the performance with\ndifferent scales of trainable parameters. The basic idea of\nthe VoP enhancement is to model the frame position, frame\ncontext, and layer function with specific trainable prompts,\nrespectively. Extensive experiments show that compared to\nfull fine-tuning, the enhanced VoP achieves a 1.4% average\nR@1 gain across five text-video retrieval benchmarks with\n6× less parameter overhead. The code will be available at\nhttps://github.com/bighuang624/VoP.\n",
        "question": {
            "statement": "What is a major drawback of using pre-trained CLIP models for text-video cross-modal retrieval?",
            "options": [
                "requires minimal training data",
                "improves performance on all benchmarks",
                "leads to knowledge forgetting from upstream models",
                "increases computational efficiency"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "AdamsFormer for Spatial Action Localization in the Future\nHyung-gun Chi† 2 Kwonjoon Lee1 Nakul Agarwal1 Yi Xu1,3 Karthik Ramani2 Chiho Choi† 4\n1Honda Research Institute USA\n2Purdue University\n3Northeastern University\n4Samsung Semiconductor US\n{hgchi, ramani}@purdue.edu\n{kwonjoon lee, nakul agarwal}@honda-ri.com\nxu.yi@northeastern.edu\nchiho1.choi@samsung.com\nAbstract\nPredicting future action locations is vital for applica-\ntions like human-robot collaboration. While some computer\nvision tasks have made progress in predicting human ac-\ntions, accurately localizing these actions in future frames\nremains an area with room for improvement.\nWe intro-\nduce a new task called spatial action localization in the\nfuture (SALF), which aims to predict action locations in\nboth observed and future frames. SALF is challenging be-\ncause it requires understanding the underlying physics of\nvideo observations to predict future action locations accu-\nrately. To address SALF, we use the concept of NeuralODE,\nwhich models the latent dynamics of sequential data by\nsolving ordinary differential equations (ODE) with neural\nnetworks. We propose a novel architecture, AdamsFormer,\nwhich extends observed frame features to future time hori-\nzons by modeling continuous temporal dynamics through\nODE solving. Specifically, we employ the Adams method,\na multi-step approach that efficiently uses information from\nprevious steps without discarding it. Our extensive experi-\nments on UCF101-24 and JHMDB-21 datasets demonstrate\nthat our proposed model outperforms existing long-range\ntemporal modeling methods by a significant margin in terms\nof frame-mAP.\n",
        "question": {
            "statement": "What type of equations are used in the NeuralODE concept to model the latent dynamics of sequential data?",
            "options": [
                "algebraic equations",
                "ordinary differential equations",
                "partial differential equations",
                "integral equations"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "OpenScene: 3D Scene Understanding with Open Vocabularies\nSongyou Peng1,2,3\nKyle Genova1\nChiyu “Max” Jiang4\nAndrea Tagliasacchi1,5\nMarc Pollefeys2\nThomas Funkhouser1\n1 Google Research\n2 ETH Zurich\n3 MPI for Intelligent Systems, T¨\nubingen\n4 Waymo LLC\n5 Simon Fraser University\npengsongyou.github.io/openscene\nZero-shot Semantic Segmentation\n“fan” - Object\n“soft” - Property\n“sit” - Affordance\n“kitchen” – Room Type\n“metal” - Material\nInput 3D Point Cloud\n“work” - Activity\nZero-shot Semantic Segmentation\nFigure 1. Open-vocabulary 3D Scene Understanding. We propose OpenScene, a zero-shot approach to 3D scene understanding that\nco-embeds dense 3D point features with image pixels and text. The examples above show a 3D scene with surface points colored by how\nwell they match a user-specified query string – yellow is highest, green is middle, blue is low. Because its features are language-based,\nOpenScene answers a wide variety of example queries, like “soft”, “kitchen”, or “work”, without labeled 3D data.\nAbstract\nTraditional 3D scene understanding approaches rely on\nlabeled 3D datasets to train a model for a single task with\nsupervision.\nWe propose OpenScene, an alternative ap-\nproach where a model predicts dense features for 3D scene\npoints that are co-embedded with text and image pixels in\nCLIP feature space. This zero-shot approach enables task-\nagnostic training and open-vocabulary queries. For exam-\nple, to perform SOTA zero-shot 3D semantic segmentation\nit first infers CLIP features for every 3D point and later\nclassifies them based on similarities to embeddings of ar-\nbitrary class labels. More interestingly, it enables a suite\nof open-vocabulary scene understanding applications that\nhave never been done before. For example, it allows a user\nto enter an arbitrary text query and then see a heat map\nindicating which parts of a scene match. Our approach is\neffective at identifying objects, materials, affordances, ac-\ntivities, and room types in complex 3D scenes, all using a\nsingle model trained without any labeled 3D data.\n",
        "question": {
            "statement": "What is the key advantage of the OpenScene approach to 3D scene understanding?",
            "options": [
                "It requires large amounts of labeled 3D data to train",
                "It relies heavily on manual feature engineering",
                "It can only be used for a specific type of 3D scene",
                "It can perform various tasks without requiring labeled 3D data"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "PACO: Parts and Attributes of Common Objects\nVignesh Ramanathan∗1\nAnmol Kalia∗1\nVladan Petrovic∗1\nYi Wen1\nBaixue Zheng1\nBaishan Guo1\nRui Wang1\nAaron Marquez1\nRama Kovvuri1\nAbhishek Kadian1\nAmir Mousavi2†\nYiwen Song1\nAbhimanyu Dubey1\nDhruv Mahajan1\n1Meta AI\n2Simon Fraser University\nFigure 1. (left) PACO includes objects with object masks, object attributes, part masks, and part attributes. (right) Object instance queries\ncomposed of object and part attributes are shown with corresponding positive images in green and negative images in red.\nAbstract\nObject models are gradually progressing from predict-\ning just category labels to providing detailed descriptions\nof object instances.\nThis motivates the need for large\ndatasets which go beyond traditional object masks and pro-\nvide richer annotations such as part masks and attributes.\nHence, we introduce PACO: Parts and Attributes of Com-\nmon Objects. It spans 75 object categories, 456 object-\npart categories and 55 attributes across image (LVIS) and\nvideo (Ego4D) datasets. We provide 641K part masks an-\nnotated across 260K object boxes, with roughly half of\nthem exhaustively annotated with attributes as well.\nWe\ndesign evaluation metrics and provide benchmark results\nfor three tasks on the dataset: part mask segmentation, ob-\nject and part attribute prediction and zero-shot instance de-\ntection.\nDataset, models, and code are open-sourced at\nhttps://github.com/facebookresearch/paco.\n∗Equal contribution\n† Work done during internship at Meta AI\n",
        "question": {
            "statement": "What is the main motivation behind creating large datasets with richer annotations such as part masks and attributes?",
            "options": [
                "To enable object models to provide more detailed descriptions of object instances",
                "To reduce the amount of data required for object detection",
                "To focus solely on predicting category labels",
                "To eliminate the need for object masks"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "0",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "ViPLO: Vision Transformer based Pose-Conditioned Self-Loop Graph for\nHuman-Object Interaction Detection\nJeeseung Park1\nJin-Woo Park1,2\nJong-Seok Lee2\n1mAy-I Inc., Seoul, Korea,\n2Yonsei University, Korea\n{jspark, jin}@may-i.io\njong-seok.lee@yonsei.ac.kr\nAbstract\nHuman-Object Interaction (HOI) detection, which lo-\ncalizes and infers relationships between human and ob-\njects, plays an important role in scene understanding. Al-\nthough two-stage HOI detectors have advantages of high\nefficiency in training and inference, they suffer from lower\nperformance than one-stage methods due to the old back-\nbone networks and the lack of considerations for the\nHOI perception process of humans in the interaction clas-\nsifiers.\nIn this paper, we propose Vision Transformer\nbased Pose-Conditioned Self-Loop Graph (ViPLO) to re-\nsolve these problems. First, we propose a novel feature ex-\ntraction method suitable for the Vision Transformer back-\nbone, called masking with overlapped area (MOA) module.\nThe MOA module utilizes the overlapped area between each\npatch and the given region in the attention function, which\naddresses the quantization problem when using the Vision\nTransformer backbone. In addition, we design a graph with\na pose-conditioned self-loop structure, which updates the\nhuman node encoding with local features of human joints.\nThis allows the classifier to focus on specific human joints\nto effectively identify the type of interaction, which is mo-\ntivated by the human perception process for HOI. As a re-\nsult, ViPLO achieves the state-of-the-art results on two pub-\nlic benchmarks, especially obtaining a +2.07 mAP perfor-\nmance gain on the HICO-DET dataset.\n",
        "question": {
            "statement": "What is a limitation of two-stage Human-Object Interaction (HOI) detectors?",
            "options": [
                "They require more data for training",
                "They are unable to detect interactions between multiple objects",
                "They are less efficient in terms of computational resources",
                "They suffer from lower performance than one-stage methods"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning\nChia-Wen Kuo\nGeorgia Tech\nalbert.cwkuo@gatech.edu\nZsolt Kira\nGeorgia Tech\nzkira@gatech.edu\nAbstract\nA great deal of progress has been made in image caption-\ning, driven by research into how to encode the image using\npre-trained models. This includes visual encodings (e.g. im-\nage grid features or detected objects) and more recently\ntextual encodings (e.g. image tags or text descriptions of\nimage regions). As more advanced encodings are available\nand incorporated, it is natural to ask: how to efficiently and\neffectively leverage the heterogeneous set of encodings? In\nthis paper, we propose to regard the encodings as augmented\nviews of the input image. The image captioning model en-\ncodes each view independently with a shared encoder ef-\nficiently, and a contrastive loss is incorporated across the\nencoded views in a novel way to improve their representation\nquality and the model’s data efficiency. Our proposed hier-\narchical decoder then adaptively weighs the encoded views\naccording to their effectiveness for caption generation by\nfirst aggregating within each view at the token level, and then\nacross views at the view level. We demonstrate significant\nperformance improvements of +5.6% CIDEr on MS-COCO\nand +12.9% CIDEr on Flickr30k compared to state of the\narts, and conduct rigorous analyses to demonstrate the im-\nportance of each part of our design.\n",
        "question": {
            "statement": "What approach is used to leverage a heterogeneous set of encodings in image captioning?",
            "options": [
                "combining all encodings into a single vector",
                "using a separate encoder for each encoding type",
                "ranking encodings based on their complexity",
                "regarding the encodings as augmented views of the input image"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "End-to-End Vectorized HD-map Construction with Piecewise B´\nezier Curve\nLimeng Qiao\nWenjie Ding\nXi Qiu*\nChi Zhang\nMEGVII Technology\n{qiaolimeng, dingwenjie, qiuxi, zhangchi}@megvii.com\nAbstract\nVectorized high-definition map (HD-map) construction,\nwhich focuses on the perception of centimeter-level environ-\nmental information, has attracted significant research inter-\nest in the autonomous driving community. Most existing ap-\nproaches first obtain rasterized map with the segmentation-\nbased pipeline and then conduct heavy post-processing for\ndownstream-friendly vectorization. In this paper, by delving\ninto parameterization-based methods, we pioneer a concise\nand elegant scheme that adopts unified piecewise B´\nezier\ncurve. In order to vectorize changeful map elements end-\nto-end, we elaborate a simple yet effective architecture,\nnamed Piecewise B´\nezier HD-map Network (BeMapNet),\nwhich is formulated as a direct set prediction paradigm and\npostprocessing-free. Concretely, we first introduce a novel\nIPM-PE Align module to inject 3D geometry prior into BEV\nfeatures through common position encoding in Transformer.\nThen a well-designed Piecewise B´\nezier Head is proposed to\noutput the details of each map element, including the coor-\ndinate of control points and the segment number of curves.\nIn addition, based on the progressively restoration of B´\nezier\ncurve, we also present an efficient Point-Curve-Region Loss\nfor supervising more robust and precise HD-map modeling.\nExtensive comparisons show that our method is remarkably\nsuperior to other existing SOTAs by 18.0 mAP at least 1.\n",
        "question": {
            "statement": "What type of curve is used in the proposed method for constructing high-definition maps?",
            "options": [
                "NURBS curve",
                "Spline curve",
                "Piecewise Bezier curve",
                "B-spline curve"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Masked representation learning for domain generalized stereo matching\nZhibo Rao1,2B, Bangshu Xiong1, Mingyi He2, Yuchao Dai2, Renjie He2, Zhelun Shen3, Xing Li2B\n1Nanchang Hangkong University, Nanchang, China\n2Northwestern Polytechnical University, Xi’an, China\n3Baidu Research, Beijing, China\nraoxi36@foxmail.com, xiongbs@126.com, {myhe, daiyuchao, davidhrj}@nwpu.edu.cn,\n1901213310@pku.edu.cn, lixing36@foxmail.com\nAbstract\nRecently, many deep stereo matching methods have be-\ngun to focus on cross-domain performance, achieving im-\npressive achievements.\nHowever, these methods did not\ndeal with the significant volatility of generalization perfor-\nmance among different training epochs. Inspired by masked\nrepresentation learning and multi-task learning, this pa-\nper designs a simple and effective masked representation\nfor domain generalized stereo matching. First, we feed the\nmasked left and complete right images as input into the\nmodels. Then, we add a lightweight and simple decoder\nfollowing the feature extraction module to recover the orig-\ninal left image. Finally, we train the models with two tasks\n(stereo matching and image reconstruction) as a pseudo-\nmulti-task learning framework, promoting models to learn\nstructure information and to improve generalization per-\nformance. We implement our method on two well-known\narchitectures (CFNet and LacGwcNet) to demonstrate its\neffectiveness. Experimental results on multi-datasets show\nthat: (1) our method can be easily plugged into the cur-\nrent various stereo matching models to improve generaliza-\ntion performance; (2) our method can reduce the signifi-\ncant volatility of generalization performance among differ-\nent training epochs; (3) we find that the current methods\nprefer to choose the best results among different training\nepochs as generalization performance, but it is impossible\nto select the best performance by ground truth in practice.\n",
        "question": {
            "statement": "What is the main goal of the proposed method in the context of stereo matching?",
            "options": [
                "Reducing the computational cost of stereo matching algorithms",
                "Increasing the accuracy of stereo matching on a specific dataset",
                "Enhancing the robustness of stereo matching to noise and outliers",
                "Improving generalization performance across different domains"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "2",
                "2",
                "10"
            ]
        },
        "difference": 8,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Look, Radiate, and Learn:\nSelf-Supervised Localisation via Radio-Visual Correspondence\nMohammed Alloulah*\nMaximilian Arnold†\nNokia Bell Labs\nAbstract\nNext generation cellular networks will implement ra-\ndio sensing functions alongside customary communications,\nthereby enabling unprecedented worldwide sensing coverage\noutdoors. Deep learning has revolutionised computer vision\nbut has had limited application to radio perception tasks,\nin part due to lack of systematic datasets and benchmarks\ndedicated to the study of the performance and promise of\nradio sensing. To address this gap, we present MaxRay:\na synthetic radio-visual dataset and benchmark that facil-\nitate precise target localisation in radio. We further pro-\npose to learn to localise targets in radio without supervision\nby extracting self-coordinates from radio-visual correspon-\ndence. We use such self-supervised coordinates to train a\nradio localiser network. We characterise our performance\nagainst a number of state-of-the-art baselines. Our results\nindicate that accurate radio target localisation can be au-\ntomatically learned from paired radio-visual data without\nlabels, which is important for empirical data. This opens\nthe door for vast data scalability and may prove key to re-\nalising the promise of robust radio sensing atop a unified\ncommunication-perception cellular infrastructure. Dataset\nwill be hosted on IEEE DataPort.\n",
        "question": {
            "statement": "What is a major limitation of applying deep learning to radio perception tasks?",
            "options": [
                "lack of systematic datasets and benchmarks",
                "limited availability of radio signals",
                "insufficient computing power",
                "high cost of radio equipment"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "ObjectMatch: Robust Registration using Canonical Object Correspondences\nCan G¨\numeli\nAngela Dai\nMatthias Nießner\nTechnical University of Munich\nFigure 1. Modern camera pose estimation relies on feature matching between overlapping frames - in this work, we present ObjectMatch to\nfind correspondences between frames with little or no overlap by predicting semantic mappings through canonical object correspondences.\nThe images above share no direct overlap, yet our method establishes indirect correspondences, thus enabling a successful registration.\nAbstract\nWe present ObjectMatch1, a semantic and object-centric\ncamera pose estimator for RGB-D SLAM pipelines. Mod-\nern camera pose estimators rely on direct correspondences\nof overlapping regions between frames; however, they can-\nnot align camera frames with little or no overlap. In this\nwork, we propose to leverage indirect correspondences ob-\ntained via semantic object identification.\nFor instance,\nwhen an object is seen from the front in one frame and from\nthe back in another frame, we can provide additional pose\nconstraints through canonical object correspondences. We\nfirst propose a neural network to predict such correspon-\ndences on a per-pixel level, which we then combine in our\nenergy formulation with state-of-the-art keypoint matching\nsolved with a joint Gauss-Newton optimization. In a pair-\nwise setting, our method improves registration recall of\nstate-of-the-art feature matching, including from 24% to\n45% in pairs with 10% or less inter-frame overlap. In regis-\ntering RGB-D sequences, our method outperforms cutting-\nedge SLAM baselines in challenging, low-frame-rate sce-\nnarios, achieving more than 35% reduction in trajectory er-\nror in multiple scenes.\n1https://cangumeli.github.io/ObjectMatch/\n",
        "question": {
            "statement": "What is the main limitation of modern camera pose estimators?",
            "options": [
                "They require high-quality images",
                "They are limited to indoor environments",
                "They rely on manual annotations",
                "They cannot align camera frames with little or no overlap"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Generating Holistic 3D Human Motion from Speech\nHongwei Yi1∗\nHualin Liang2∗\nYifei Liu2∗\nQiong Cao3†\nYandong Wen1\nTimo Bolkart1\nDacheng Tao3\nMichael J. Black1†\n1Max Planck Institute for Intelligent Systems, T¨\nubingen, Germany\n2South China University of Technology\n3JD Explore Academy\n{hongwei.yi, yandong.wen, timo.bolkart, black}@tuebingen.mpg.de\n{hualinliang3, yifei9697, mathqiong2012, dacheng.tao}@gmail.com\n... are things that everyone can do, to combat the kidney shortage. A truly amazing people can donate one of  ...\nFigure 1. Speech-to-motion translation example. Given a speech signal as input, our approach generates realistic, coherent, and diverse\nholistic body motions; that is, the body motion together with facial expressions and hand gestures. From top to bottom: the input audio, the\ncorresponding transcript, video frames, and the generated motions. Note that the audio is the only input to our approach, while the transcript\nand video frames are just shown for reference.\nAbstract\nThis work addresses the problem of generating 3D holistic\nbody motions from human speech. Given a speech record-\ning, we synthesize sequences of 3D body poses, hand ges-\ntures, and facial expressions that are realistic and diverse.\nTo achieve this, we first build a high-quality dataset of 3D\nholistic body meshes with synchronous speech. We then\ndefine a novel speech-to-motion generation framework in\nwhich the face, body, and hands are modeled separately.\nThe separated modeling stems from the fact that face artic-\nulation strongly correlates with human speech, while body\nposes and hand gestures are less correlated. Specifically,\nwe employ an autoencoder for face motions, and a composi-\ntional vector-quantized variational autoencoder (VQ-VAE)\nfor the body and hand motions. The compositional VQ-\nVAE is key to generating diverse results. Additionally, we\npropose a cross-conditional autoregressive model that gener-\nates body poses and hand gestures, leading to coherent and\n*Equal Contribution.\n†Joint Corresponding Authors.\nrealistic motions. Extensive experiments and user studies\ndemonstrate that our proposed approach achieves state-of-\nthe-art performance both qualitatively and quantitatively.\nOur dataset and code are released for research purposes at\nhttps://talkshow.is.tue.mpg.de/.\n",
        "question": {
            "statement": "What is the main goal of the approach presented in the paper?",
            "options": [
                "Recognize emotions from speech signals",
                "Classify speech recordings into different categories",
                "Synthesize speech from 3D human motions",
                "Generate realistic and diverse 3D human motions from speech"
            ],
            "answer": "d"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "0",
                "0",
                "10"
            ]
        },
        "difference": 10,
        "gpt_answer": "d",
        "cause": ""
    },
    {
        "context": "Spatial-Frequency Mutual Learning for Face Super-Resolution\nChenyang Wang, Junjun Jiang*, Zhiwei Zhong, Xianming Liu\nSchool of Computer Science and Technology, Harbin Institute of Technology, Harbin, China\n{wangchy02,jiangjunjun,zhwzhong,csxm}@hit.edu.cn\nAbstract\nFace super-resolution (FSR) aims to reconstruct high-\nresolution (HR) face images from the low-resolution (LR)\nones. With the advent of deep learning, the FSR technique\nhas achieved significant breakthroughs. However, existing\nFSR methods either have a fixed receptive field or fail to\nmaintain facial structure, limiting the FSR performance. To\ncircumvent this problem, Fourier transform is introduced,\nwhich can capture global facial structure information and\nachieve image-size receptive field. Relying on the Fourier\ntransform, we devise a spatial-frequency mutual network\n(SFMNet) for FSR, which is the first FSR method to ex-\nplore the correlations between spatial and frequency do-\nmains as far as we know. To be specific, our SFMNet is\na two-branch network equipped with a spatial branch and\na frequency branch. Benefiting from the property of Fourier\ntransform, the frequency branch can achieve image-size re-\nceptive field and capture global dependency while the spa-\ntial branch can extract local dependency. Considering that\nthese dependencies are complementary and both favorable\nfor FSR, we further develop a frequency-spatial interac-\ntion block (FSIB) which mutually amalgamates the com-\nplementary spatial and frequency information to enhance\nthe capability of the model. Quantitative and qualitative\nexperimental results show that the proposed method out-\nperforms state-of-the-art FSR methods in recovering face\nimages. The implementation and model will be released at\nhttps://github.com/wcy-cs/SFMNet.\n",
        "question": {
            "statement": "What advantage does the Fourier transform offer in face super-resolution tasks?",
            "options": [
                "it is limited to local facial features extraction",
                "it can capture global facial structure information and achieve image-size receptive field",
                "it can only process high-resolution images",
                "it requires manual labeling of facial structures"
            ],
            "answer": "b"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "0",
                "10",
                "0",
                "0"
            ]
        },
        "difference": 10,
        "gpt_answer": "b",
        "cause": ""
    },
    {
        "context": "RangeViT: Towards Vision Transformers\nfor 3D Semantic Segmentation in Autonomous Driving\nAngelika Ando1,2,*, Spyros Gidaris1, Andrei Bursuc1, Gilles Puy1, Alexandre Boulch1, Renaud Marlet1,3\n1Valeo.ai, Paris, France\n2Centre for Robotics, Mines Paris, Universit´\ne PSL, Paris, France\n3LIGM, Ecole des Ponts, Univ. Gustave Eiffel, CNRS, Marne-la-Vall´\nee, France\nAbstract\nCasting semantic segmentation of outdoor LiDAR point\nclouds as a 2D problem, e.g., via range projection, is an\neffective and popular approach.\nThese projection-based\nmethods usually benefit from fast computations and, when\ncombined with techniques which use other point cloud\nrepresentations, achieve state-of-the-art results.\nToday,\nprojection-based methods leverage 2D CNNs but recent ad-\nvances in computer vision show that vision transformers\n(ViTs) have achieved state-of-the-art results in many image-\nbased benchmarks. In this work, we question if projection-\nbased methods for 3D semantic segmentation can benefit\nfrom these latest improvements on ViTs. We answer posi-\ntively but only after combining them with three key ingre-\ndients: (a) ViTs are notoriously hard to train and require\na lot of training data to learn powerful representations.\nBy preserving the same backbone architecture as for RGB\nimages, we can exploit the knowledge from long training\non large image collections that are much cheaper to ac-\nquire and annotate than point clouds. We reach our best\nresults with pre-trained ViTs on large image datasets. (b)\nWe compensate ViTs’ lack of inductive bias by substituting\na tailored convolutional stem for the classical linear em-\nbedding layer. (c) We refine pixel-wise predictions with a\nconvolutional decoder and a skip connection from the con-\nvolutional stem to combine low-level but fine-grained fea-\ntures of the the convolutional stem with the high-level but\ncoarse predictions of the ViT encoder. With these ingre-\ndients, we show that our method, called RangeViT, out-\nperforms existing projection-based methods on nuScenes\nand SemanticKITTI. The code is available at https://\ngithub.com/valeoai/rangevit.\n",
        "question": {
            "statement": "What is a major challenge in training Vision Transformers (ViTs) for 3D semantic segmentation?",
            "options": [
                "They are incompatible with convolutional neural networks",
                "They are limited to processing 2D images only",
                "They require a lot of training data to learn powerful representations.",
                "They are inherently biased towards certain object classes"
            ],
            "answer": "c"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "2",
                "0",
                "10",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "c",
        "cause": ""
    },
    {
        "context": "Building Rearticulable Models for Arbitrary 3D Objects from 4D Point Clouds\nShaowei Liu1\nSaurabh Gupta1*\nShenlong Wang1∗\n1University of Illinois Urbana-Champaign\nhttps://stevenlsw.github.io/reart/\nInput: 4D Point Cloud of Arbitrary Object\nOutput: Articulated Model\nApplication: Re-Articulation\nPart\nSkeleton\nScrew Params\nGuidance\nRetargeted Model\n<latexit sha1_base64=\"4mSRiAOC1HPb\nUsbyd7QN48TyFA=\">AB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0mkqMeiF48t2FpoQ\n9lsN+3azSbsToQS+gu8eFDEqz/Jm/GbZuDtj4YeLw3w8y8IJHCoOt+O4W19Y3NreJ\n2aWd3b/+gfHjUNnGqGW+xWMa6E1DpVC8hQIl7ySa0yiQ/CEY3878hyeujYjVPU4S7k\nd0qEQoGEUrNbFfrhVdw6ySrycVCBHo1/+6g1ilkZcIZPUmK7nJuhnVKNgk9LvdTwh\nLIxHfKupYpG3PjZ/NApObPKgISxtqWQzNXfExmNjJlEge2MKI7MsjcT/O6KYbXfiZU\nkiJXbLEoTCXBmMy+JgOhOUM5sYQyLeythI2opgxtNiUbgrf8ipX1S9y2qtWavUb/\nI4inACp3AOHlxBHe6gAS1gwOEZXuHNeXRenHfnY9FacPKZY/gD5/MH4xeNAQ=</lat\nexit>t\nFigure 1. Given a short point cloud sequence of arbitrary articulated object (left), our method outputs an animatable 3D model (middle),\nwhich can be retargeted to novel poses with only a few sparse point correspondences (right).\nAbstract\nWe build rearticulable models for arbitrary everyday\nman-made objects containing an arbitrary number of parts\nthat are connected together in arbitrary ways via 1 degree-\nof-freedom joints. Given point cloud videos of such every-\nday objects, our method identifies the distinct object parts,\nwhat parts are connected to what other parts, and the prop-\nerties of the joints connecting each part pair. We do this\nby jointly optimizing the part segmentation, transformation,\nand kinematics using a novel energy minimization frame-\nwork. Our inferred animatable models, enables retargeting\nto novel poses with sparse point correspondences guidance.\nWe test our method on a new articulating robot dataset,\nand the Sapiens dataset with common daily objects. Ex-\nperiments show that our method outperforms two leading\nprior works on various metrics.\n",
        "question": {
            "statement": "What is the primary application of the rearticulable models built from 4D point clouds?",
            "options": [
                "Re-Articulation",
                "Scene Understanding",
                "3D Reconstruction",
                "Object Recognition"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "1",
                "0",
                "0"
            ]
        },
        "difference": 9,
        "gpt_answer": "a",
        "cause": ""
    },
    {
        "context": "Exploring the Effect of Primitives for\nCompositional Generalization in Vision-and-Language\nChuanhao Li1, Zhen Li1, Chenchen Jing3*\n, Yunde Jia2,1, Yuwei Wu2,1∗\n1Beijing Key Laboratory of Intelligent Information Technology,\nSchool of Computer Science & Technology, Beijing Institute of Technology, China\n2Guangdong Laboratory of Machine Perception and Intelligent Computing,\nShenzhen MSU-BIT University, China\n3School of Computer Science, Zhejiang University, Hangzhou, China\n{lichuanhao,li.zhen,jiayunde,wuyuwei}@bit.edu.cn\njingchenchen@zju.edu.cn\nAbstract\nCompositionality is one of the fundamental properties of\nhuman cognition (Fodor & Pylyshyn, 1988). Compositional\ngeneralization is critical to simulate the compositional ca-\npability of humans, and has received much attention in the\nvision-and-language (V&L) community. It is essential to\nunderstand the effect of the primitives, including words, im-\nage regions, and video frames, to improve the compositional\ngeneralization capability. In this paper, we explore the ef-\nfect of primitives for compositional generalization in V&L.\nSpecifically, we present a self-supervised learning based\nframework that equips existing V&L methods with two char-\nacteristics: semantic equivariance and semantic invari-\nance. With the two characteristics, the methods understand\nprimitives by perceiving the effect of primitive changes on\nsample semantics and ground-truth. Experimental results\non two tasks: temporal video grounding and visual question\nanswering, demonstrate the effectiveness of our framework.\n",
        "question": {
            "statement": "What is the primary goal of compositional generalization in vision-and-language research?",
            "options": [
                "To enable models to generalize to new combinations of concepts",
                "To develop more efficient natural language processing algorithms",
                "To increase the robustness of object detection systems",
                "To improve the accuracy of image classification models"
            ],
            "answer": "a"
        },
        "validation": {
            "format": true,
            "grammar": true,
            "language": true,
            "statement": true,
            "relevance": "10",
            "options": [
                "10",
                "2",
                "0",
                "0"
            ]
        },
        "difference": 8,
        "gpt_answer": "a",
        "cause": ""
    }
]