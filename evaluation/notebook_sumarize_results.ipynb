{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook tem como entrada os resultados das avaliações dos diferentes sistemas de rag e cria visualizações dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_answers(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[(df['system_answer'].isna() == False) & (df['question'].isna() == False)]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../results/ml-short-questions'\n",
    "df_no_rag = pd.read_csv(directory+'/df_rag_False_use_text_False_use_questions_False_use_summary_False_use_expansion_False.csv')\n",
    "df_all_rag = pd.read_csv(directory+'/df_rag_True_use_text_True_use_questions_True_use_summary_True_use_expansion_False.csv')\n",
    "df_use_text = pd.read_csv(directory+'/df_rag_True_use_text_True_use_questions_False_use_summary_False_use_expansion_False.csv')\n",
    "df_use_questions = pd.read_csv(directory+'/df_rag_True_use_text_False_use_questions_True_use_summary_False_use_expansion_False.csv')\n",
    "df_use_summary = pd.read_csv(directory+'/df_rag_True_use_text_False_use_questions_False_use_summary_True_use_expansion_False.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status No Rag:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   question        200 non-null    object \n",
      " 1   correct_answer  200 non-null    object \n",
      " 2   wrong_answer1   200 non-null    object \n",
      " 3   wrong_answer2   200 non-null    object \n",
      " 4   wrong_answer3   200 non-null    object \n",
      " 5   text            200 non-null    object \n",
      " 6   init_line       200 non-null    float64\n",
      " 7   source          200 non-null    object \n",
      " 8   system_answer   200 non-null    object \n",
      " 9   rag_context     0 non-null      float64\n",
      "dtypes: float64(2), object(8)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "Status All Rag:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   question        200 non-null    object \n",
      " 1   correct_answer  200 non-null    object \n",
      " 2   wrong_answer1   200 non-null    object \n",
      " 3   wrong_answer2   200 non-null    object \n",
      " 4   wrong_answer3   200 non-null    object \n",
      " 5   text            200 non-null    object \n",
      " 6   init_line       200 non-null    float64\n",
      " 7   source          200 non-null    object \n",
      " 8   system_answer   200 non-null    object \n",
      " 9   rag_context     200 non-null    object \n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "Status Use Text:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   question        200 non-null    object \n",
      " 1   correct_answer  200 non-null    object \n",
      " 2   wrong_answer1   200 non-null    object \n",
      " 3   wrong_answer2   200 non-null    object \n",
      " 4   wrong_answer3   200 non-null    object \n",
      " 5   text            200 non-null    object \n",
      " 6   init_line       200 non-null    float64\n",
      " 7   source          200 non-null    object \n",
      " 8   system_answer   200 non-null    object \n",
      " 9   rag_context     200 non-null    object \n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "Status Use Questions:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   question        200 non-null    object \n",
      " 1   correct_answer  200 non-null    object \n",
      " 2   wrong_answer1   200 non-null    object \n",
      " 3   wrong_answer2   200 non-null    object \n",
      " 4   wrong_answer3   200 non-null    object \n",
      " 5   text            200 non-null    object \n",
      " 6   init_line       200 non-null    float64\n",
      " 7   source          200 non-null    object \n",
      " 8   system_answer   200 non-null    object \n",
      " 9   rag_context     200 non-null    object \n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 15.8+ KB\n",
      "None\n",
      "Status Use Summary:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 10 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   question        200 non-null    object \n",
      " 1   correct_answer  200 non-null    object \n",
      " 2   wrong_answer1   200 non-null    object \n",
      " 3   wrong_answer2   200 non-null    object \n",
      " 4   wrong_answer3   200 non-null    object \n",
      " 5   text            200 non-null    object \n",
      " 6   init_line       200 non-null    float64\n",
      " 7   source          200 non-null    object \n",
      " 8   system_answer   200 non-null    object \n",
      " 9   rag_context     200 non-null    object \n",
      "dtypes: float64(1), object(9)\n",
      "memory usage: 15.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#See status of files\n",
    "def print_status(df):\n",
    "    print(df.info())\n",
    "\n",
    "print('Status No Rag:')\n",
    "print_status(df_no_rag)\n",
    "\n",
    "print('Status All Rag:')\n",
    "print_status(df_all_rag)\n",
    "\n",
    "print('Status Use Text:')\n",
    "print_status(df_use_text)\n",
    "\n",
    "print('Status Use Questions:')\n",
    "print_status(df_use_questions)\n",
    "\n",
    "print('Status Use Summary:')\n",
    "print_status(df_use_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(dataframe):\n",
    "    print('BERT Score:')\n",
    "    print(dataframe['bert_score'].mean())\n",
    "    print('Rouge-L Recall:')\n",
    "    print(dataframe['rouge_score'].mean())\n",
    "    #print('LLM Score:')\n",
    "    #llm_scores = dataframe[~dataframe['llm_evaluation'].isna()]\n",
    "    #print(llm_scores['llm_evaluation'].count())\n",
    "    #print(llm_scores['llm_evaluation'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scores resultados sem RAG:\n",
      "BERT Score:\n",
      "0.566990291262136\n",
      "Rouge-L Recall:\n",
      "0.41356439449322346\n",
      "\n",
      "\n",
      "Scores resultados com todos os métodos de RAG:\n",
      "BERT Score:\n",
      "0.5650485436893203\n",
      "Rouge-L Recall:\n",
      "0.3513248636116201\n",
      "\n",
      "\n",
      "Scores resultados apenas chunks originais:\n",
      "BERT Score:\n",
      "0.5514563106796116\n",
      "Rouge-L Recall:\n",
      "0.3708999093084202\n",
      "\n",
      "\n",
      "Scores resultados apenas perguntas:\n",
      "BERT Score:\n",
      "0.5009708737864078\n",
      "Rouge-L Recall:\n",
      "0.3026392181844516\n",
      "\n",
      "\n",
      "Scores resultados apenas resumos:\n",
      "BERT Score:\n",
      "0.566990291262136\n",
      "Rouge-L Recall:\n",
      "0.36624119377063413\n"
     ]
    }
   ],
   "source": [
    "#Get bert, rouge-l and llm accuracy for each dataframe\n",
    "print('\\nScores resultados sem RAG:')\n",
    "print_scores(df_no_rag)\n",
    "\n",
    "print('\\n\\nScores resultados com todos os métodos de RAG:')\n",
    "print_scores(df_all_rag)\n",
    "\n",
    "print('\\n\\nScores resultados apenas chunks originais:')\n",
    "print_scores(df_use_text)\n",
    "\n",
    "print('\\n\\nScores resultados apenas perguntas:')\n",
    "print_scores(df_use_questions)\n",
    "\n",
    "print('\\n\\nScores resultados apenas resumos:')\n",
    "print_scores(df_use_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness All RAG:  0.7417537073311128\n",
      "Faithfulness only chunks:  0.8173017790193076\n",
      "Faithfulness only questions:  0.746419506097627\n",
      "Faithfulness only summary:  0.7661345772545259\n"
     ]
    }
   ],
   "source": [
    "#Avaliação de faithfulness\n",
    "print('Faithfulness All RAG: ', df_all_rag['faithfulness_context_eval_score'].mean())\n",
    "print('Faithfulness only chunks: ', df_use_text['faithfulness_context_eval_score'].mean())\n",
    "print('Faithfulness only questions: ', df_use_questions['faithfulness_context_eval_score'].mean())\n",
    "print('Faithfulness only summary: ', df_use_summary['faithfulness_context_eval_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Faithfulness No RAG: 0.7751072114769821\n",
      "Faithfulness All RAG:  0.7503564937343296\n",
      "Faithfulness only chunks:  0.7670616667211856\n",
      "Faithfulness only questions:  0.7557852390511903\n",
      "Faithfulness only summary:  0.7506626175559395\n"
     ]
    }
   ],
   "source": [
    "#Avaliação de faithfulness se comparando com o texto original que gerou a pergunta\n",
    "print('Faithfulness No RAG:', df_no_rag['faithfulness_text_eval_score'].mean())\n",
    "print('Faithfulness All RAG: ', df_all_rag['faithfulness_text_eval_score'].mean())\n",
    "print('Faithfulness only chunks: ', df_use_text['faithfulness_text_eval_score'].mean())\n",
    "print('Faithfulness only questions: ', df_use_questions['faithfulness_text_eval_score'].mean())\n",
    "print('Faithfulness only summary: ', df_use_summary['faithfulness_text_eval_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer Relevance No RAG: 0.8926544745918363\n",
      "Answer Relevance All RAG:  0.8819961777665246\n",
      "Answer Relevance only chunks:  0.8822765964634565\n",
      "Answer Relevance only questions:  0.884750719336297\n",
      "Answer Relevance only summary:  0.8779062658548356\n"
     ]
    }
   ],
   "source": [
    "#Avaliação de answer relevnace\n",
    "print('Answer Relevance No RAG:', df_no_rag['answer_relevance_score'].mean())\n",
    "print('Answer Relevance All RAG: ', df_all_rag['answer_relevance_score'].mean())\n",
    "print('Answer Relevance only chunks: ', df_use_text['answer_relevance_score'].mean())\n",
    "print('Answer Relevance only questions: ', df_use_questions['answer_relevance_score'].mean())\n",
    "print('Answer Relevance only summary: ', df_use_summary['answer_relevance_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Relevance All RAG:  0.9973045822102425\n",
      "Context Relevance only chunks:  0.9946091644204852\n",
      "Context Relevance only questions:  1.0\n",
      "Context Relevance only summary:  0.9973045822102425\n"
     ]
    }
   ],
   "source": [
    "#Avaliação de context relevnace\n",
    "print('Context Relevance All RAG: ', df_all_rag['context_relevance_score'].mean())\n",
    "print('Context Relevance only chunks: ', df_use_text['context_relevance_score'].mean())\n",
    "print('Context Relevance only questions: ', df_use_questions['context_relevance_score'].mean())\n",
    "print('Context Relevance only summary: ', df_use_summary['context_relevance_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity No RAG:  5.0885907389083\n",
      "Perplexity All RAG:  3.6821232571434783\n"
     ]
    }
   ],
   "source": [
    "#Avaliação Perplexity\n",
    "print('Perplexity No RAG: ', df_no_rag['perplexity'].mean())\n",
    "print('Perplexity All RAG: ', df_all_rag['perplexity'].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
